{
  
    
        "post0": {
            "title": "Multicollinearity in Linear Regression",
            "content": "import pandas as pd . Using Advertising Dataset, link :- . https://www.kaggle.com/bumba5341/advertisingcsv . import statsmodels.api as sm df_adv = pd.read_csv(&#39;Advertising.csv&#39;, index_col=0) df_adv.head() . TV radio newspaper sales . 1 230.1 | 37.8 | 69.2 | 22.1 | . 2 44.5 | 39.3 | 45.1 | 10.4 | . 3 17.2 | 45.9 | 69.3 | 9.3 | . 4 151.5 | 41.3 | 58.5 | 18.5 | . 5 180.8 | 10.8 | 58.4 | 12.9 | . X = df_adv[[&#39;TV&#39;,&#39;radio&#39;,&#39;newspaper&#39;]] y = df_adv[&#39;sales&#39;] print(X,y) . TV radio newspaper 1 230.1 37.8 69.2 2 44.5 39.3 45.1 3 17.2 45.9 69.3 4 151.5 41.3 58.5 5 180.8 10.8 58.4 .. ... ... ... 196 38.2 3.7 13.8 197 94.2 4.9 8.1 198 177.0 9.3 6.4 199 283.6 42.0 66.2 200 232.1 8.6 8.7 [200 rows x 3 columns] 1 22.1 2 10.4 3 9.3 4 18.5 5 12.9 ... 196 7.6 197 9.7 198 12.8 199 25.5 200 13.4 Name: sales, Length: 200, dtype: float64 . fit a Oridinirary least square model with intercept on TV and Radio . X = sm.add_constant(X) . X . const TV radio newspaper . 1 1.0 | 230.1 | 37.8 | 69.2 | . 2 1.0 | 44.5 | 39.3 | 45.1 | . 3 1.0 | 17.2 | 45.9 | 69.3 | . 4 1.0 | 151.5 | 41.3 | 58.5 | . 5 1.0 | 180.8 | 10.8 | 58.4 | . ... ... | ... | ... | ... | . 196 1.0 | 38.2 | 3.7 | 13.8 | . 197 1.0 | 94.2 | 4.9 | 8.1 | . 198 1.0 | 177.0 | 9.3 | 6.4 | . 199 1.0 | 283.6 | 42.0 | 66.2 | . 200 1.0 | 232.1 | 8.6 | 8.7 | . 200 rows × 4 columns . model = sm.OLS(y, X).fit() . model.summary() # const indicates B0 value . OLS Regression Results Dep. Variable: sales | R-squared: 0.897 | . Model: OLS | Adj. R-squared: 0.896 | . Method: Least Squares | F-statistic: 570.3 | . Date: Tue, 27 Apr 2021 | Prob (F-statistic): 1.58e-96 | . Time: 19:40:31 | Log-Likelihood: -386.18 | . No. Observations: 200 | AIC: 780.4 | . Df Residuals: 196 | BIC: 793.6 | . Df Model: 3 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . const 2.9389 | 0.312 | 9.422 | 0.000 | 2.324 | 3.554 | . TV 0.0458 | 0.001 | 32.809 | 0.000 | 0.043 | 0.049 | . radio 0.1885 | 0.009 | 21.893 | 0.000 | 0.172 | 0.206 | . newspaper -0.0010 | 0.006 | -0.177 | 0.860 | -0.013 | 0.011 | . Omnibus: 60.414 | Durbin-Watson: 2.084 | . Prob(Omnibus): 0.000 | Jarque-Bera (JB): 151.241 | . Skew: -1.327 | Prob(JB): 1.44e-33 | . Kurtosis: 6.332 | Cond. No. 454. | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. import matplotlib.pyplot as plt import seaborn as sns X.iloc[:,1:].corr() . TV radio newspaper . TV 1.000000 | 0.054809 | 0.056648 | . radio 0.054809 | 1.000000 | 0.354104 | . newspaper 0.056648 | 0.354104 | 1.000000 | . plt.imshow(X,cmap=&#39;autumn&#39;) plt.show() . sns.heatmap(X,linewidth = 0.5 , cmap = &#39;coolwarm&#39;) plt.show() . Using Salary DataSet, link :- . https://github.com/mr-siddy/Machine-Learning/blob/master/Linear%20Regression/Salary_Data.csv . df_salary = pd.read_csv(&#39;Salary_Data.csv&#39;) df_salary.head() . YearsExperience Age Salary . 0 1.1 | 21.0 | 39343 | . 1 1.3 | 21.5 | 46205 | . 2 1.5 | 21.7 | 37731 | . 3 2.0 | 22.0 | 43525 | . 4 2.2 | 22.2 | 39891 | . X = df_salary[[&#39;YearsExperience&#39;,&#39;Age&#39;]] y = df_salary[&#39;Salary&#39;] . fit OLS model on y and X . X = sm.add_constant(X) model = sm.OLS(y,X).fit() . model.summary() # here observe R2, const, stderr and P&gt;|t| --&gt; high correlation . OLS Regression Results Dep. Variable: Salary | R-squared: 0.960 | . Model: OLS | Adj. R-squared: 0.957 | . Method: Least Squares | F-statistic: 323.9 | . Date: Tue, 27 Apr 2021 | Prob (F-statistic): 1.35e-19 | . Time: 19:58:08 | Log-Likelihood: -300.35 | . No. Observations: 30 | AIC: 606.7 | . Df Residuals: 27 | BIC: 610.9 | . Df Model: 2 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . const -6661.9872 | 2.28e+04 | -0.292 | 0.773 | -5.35e+04 | 4.02e+04 | . YearsExperience 6153.3533 | 2337.092 | 2.633 | 0.014 | 1358.037 | 1.09e+04 | . Age 1836.0136 | 1285.034 | 1.429 | 0.165 | -800.659 | 4472.686 | . Omnibus: 2.695 | Durbin-Watson: 1.711 | . Prob(Omnibus): 0.260 | Jarque-Bera (JB): 1.975 | . Skew: 0.456 | Prob(JB): 0.372 | . Kurtosis: 2.135 | Cond. No. 626. | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. X.iloc[:,1:].corr() . YearsExperience Age . YearsExperience 1.000000 | 0.987258 | . Age 0.987258 | 1.000000 | . sns.heatmap(X, cmap=&#39;summer&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x21443500808&gt; . How to Resolve . we check the P value and drop the feature which has higher p value . drop_age = X.drop(&#39;Age&#39;, axis=1) . model = sm.OLS(y,drop_age).fit() model.summary() . OLS Regression Results Dep. Variable: Salary | R-squared: 0.957 | . Model: OLS | Adj. R-squared: 0.955 | . Method: Least Squares | F-statistic: 622.5 | . Date: Tue, 27 Apr 2021 | Prob (F-statistic): 1.14e-20 | . Time: 20:07:01 | Log-Likelihood: -301.44 | . No. Observations: 30 | AIC: 606.9 | . Df Residuals: 28 | BIC: 609.7 | . Df Model: 1 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . const 2.579e+04 | 2273.053 | 11.347 | 0.000 | 2.11e+04 | 3.04e+04 | . YearsExperience 9449.9623 | 378.755 | 24.950 | 0.000 | 8674.119 | 1.02e+04 | . Omnibus: 2.140 | Durbin-Watson: 1.648 | . Prob(Omnibus): 0.343 | Jarque-Bera (JB): 1.569 | . Skew: 0.363 | Prob(JB): 0.456 | . Kurtosis: 2.147 | Cond. No. 13.2 | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "url": "https://mr-siddy.github.io/The-Student-Blog/2021/04/27/Multico-LR.html",
            "relUrl": "/2021/04/27/Multico-LR.html",
            "date": " • Apr 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Multiple Linear Regrassion using 50_Startups dataset",
            "content": "dataset link :- https://www.kaggle.com/farhanmd29/50-startups . Import Libraries and Creating Dataframe . import numpy as np import matplotlib.pyplot as plt import pandas as pd . dataset = pd.read_csv(&quot;50_Startups.csv&quot;,&quot;,&quot;) dataset.head() . R&amp;D Spend Administration Marketing Spend State Profit . 0 165349.20 | 136897.80 | 471784.10 | New York | 192261.83 | . 1 162597.70 | 151377.59 | 443898.53 | California | 191792.06 | . 2 153441.51 | 101145.55 | 407934.54 | Florida | 191050.39 | . 3 144372.41 | 118671.85 | 383199.62 | New York | 182901.99 | . 4 142107.34 | 91391.77 | 366168.42 | Florida | 166187.94 | . X = dataset.iloc[:, :-1] y = dataset.iloc[:,4] . print(X.head(),&quot; n&quot;) print(y.head()) . R&amp;D Spend Administration Marketing Spend State 0 165349.20 136897.80 471784.10 New York 1 162597.70 151377.59 443898.53 California 2 153441.51 101145.55 407934.54 Florida 3 144372.41 118671.85 383199.62 New York 4 142107.34 91391.77 366168.42 Florida 0 192261.83 1 191792.06 2 191050.39 3 182901.99 4 166187.94 Name: Profit, dtype: float64 . Data Preprocessing . states=pd.get_dummies(X[&#39;State&#39;],drop_first=True) #get_dummies helps to create dummy variables wrt no of categorial fratures # drop_first = True helps us to create dummy variable trap . X = X.drop(&#39;State&#39;,axis=1) . X . R&amp;D Spend Administration Marketing Spend . 0 165349.20 | 136897.80 | 471784.10 | . 1 162597.70 | 151377.59 | 443898.53 | . 2 153441.51 | 101145.55 | 407934.54 | . 3 144372.41 | 118671.85 | 383199.62 | . 4 142107.34 | 91391.77 | 366168.42 | . 5 131876.90 | 99814.71 | 362861.36 | . 6 134615.46 | 147198.87 | 127716.82 | . 7 130298.13 | 145530.06 | 323876.68 | . 8 120542.52 | 148718.95 | 311613.29 | . 9 123334.88 | 108679.17 | 304981.62 | . 10 101913.08 | 110594.11 | 229160.95 | . 11 100671.96 | 91790.61 | 249744.55 | . 12 93863.75 | 127320.38 | 249839.44 | . 13 91992.39 | 135495.07 | 252664.93 | . 14 119943.24 | 156547.42 | 256512.92 | . 15 114523.61 | 122616.84 | 261776.23 | . 16 78013.11 | 121597.55 | 264346.06 | . 17 94657.16 | 145077.58 | 282574.31 | . 18 91749.16 | 114175.79 | 294919.57 | . 19 86419.70 | 153514.11 | 0.00 | . 20 76253.86 | 113867.30 | 298664.47 | . 21 78389.47 | 153773.43 | 299737.29 | . 22 73994.56 | 122782.75 | 303319.26 | . 23 67532.53 | 105751.03 | 304768.73 | . 24 77044.01 | 99281.34 | 140574.81 | . 25 64664.71 | 139553.16 | 137962.62 | . 26 75328.87 | 144135.98 | 134050.07 | . 27 72107.60 | 127864.55 | 353183.81 | . 28 66051.52 | 182645.56 | 118148.20 | . 29 65605.48 | 153032.06 | 107138.38 | . 30 61994.48 | 115641.28 | 91131.24 | . 31 61136.38 | 152701.92 | 88218.23 | . 32 63408.86 | 129219.61 | 46085.25 | . 33 55493.95 | 103057.49 | 214634.81 | . 34 46426.07 | 157693.92 | 210797.67 | . 35 46014.02 | 85047.44 | 205517.64 | . 36 28663.76 | 127056.21 | 201126.82 | . 37 44069.95 | 51283.14 | 197029.42 | . 38 20229.59 | 65947.93 | 185265.10 | . 39 38558.51 | 82982.09 | 174999.30 | . 40 28754.33 | 118546.05 | 172795.67 | . 41 27892.92 | 84710.77 | 164470.71 | . 42 23640.93 | 96189.63 | 148001.11 | . 43 15505.73 | 127382.30 | 35534.17 | . 44 22177.74 | 154806.14 | 28334.72 | . 45 1000.23 | 124153.04 | 1903.93 | . 46 1315.46 | 115816.21 | 297114.46 | . 47 0.00 | 135426.92 | 0.00 | . 48 542.05 | 51743.15 | 0.00 | . 49 0.00 | 116983.80 | 45173.06 | . print(states.head()) . Florida New York 0 0 1 1 0 0 2 1 0 3 0 1 4 1 0 . X=pd.concat([X,states],axis=1) . print(X.head()) # Now we will apply y=b0+b1x1+b2x2+....... . R&amp;D Spend Administration Marketing Spend Florida New York 0 165349.20 136897.80 471784.10 0 1 1 162597.70 151377.59 443898.53 0 0 2 153441.51 101145.55 407934.54 1 0 3 144372.41 118671.85 383199.62 0 1 4 142107.34 91391.77 366168.42 1 0 . Train_test_split . from sklearn.model_selection import train_test_split . X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=0) . y_test . 28 103282.38 11 144259.40 10 146121.95 41 77798.83 2 191050.39 27 105008.31 38 81229.06 31 97483.56 22 110352.25 4 166187.94 Name: Profit, dtype: float64 . Applying Linear Regression . from sklearn.linear_model import LinearRegression . regressor = LinearRegression() regressor.fit(X_train, y_train) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . y_pred = regressor.predict(X_test) . y_pred . array([103015.20159796, 132582.27760816, 132447.73845174, 71976.09851258, 178537.48221055, 116161.24230165, 67851.69209676, 98791.73374687, 113969.43533012, 167921.0656955 ]) . from sklearn.metrics import r2_score # r2 = 1-(sum_of_residual/sum_of_mean) also model is good if r2 --&gt; 1 score= r2_score(y_test,y_pred) . score . 0.9347068473282423 .",
            "url": "https://mr-siddy.github.io/The-Student-Blog/2021/04/26/multiple-linear-regression.html",
            "relUrl": "/2021/04/26/multiple-linear-regression.html",
            "date": " • Apr 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Simple, Ridge and Lasso Linear Regression",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model import seaborn as sns . from sklearn.datasets import load_boston . df = load_boston() . df . {&#39;data&#39;: array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02, 4.9800e+00], [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02, 9.1400e+00], [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02, 4.0300e+00], ..., [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02, 5.6400e+00], [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02, 6.4800e+00], [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02, 7.8800e+00]]), &#39;target&#39;: array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. , 18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6, 15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2, 13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7, 21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9, 35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5, 19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. , 20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2, 23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8, 33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4, 21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. , 20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6, 23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4, 15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4, 17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7, 25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4, 23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. , 32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3, 34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4, 20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. , 26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3, 31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1, 22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6, 42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. , 36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4, 32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. , 20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1, 20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2, 22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1, 21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6, 19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7, 32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1, 18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8, 16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8, 13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3, 8.8, 7.2, 10.5, 7.4, 10.2, 11.5, 15.1, 23.2, 9.7, 13.8, 12.7, 13.1, 12.5, 8.5, 5. , 6.3, 5.6, 7.2, 12.1, 8.3, 8.5, 5. , 11.9, 27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3, 7. , 7.2, 7.5, 10.4, 8.8, 8.4, 16.7, 14.2, 20.8, 13.4, 11.7, 8.3, 10.2, 10.9, 11. , 9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4, 9.6, 8.7, 8.4, 12.8, 10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4, 15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7, 19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2, 29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8, 20.6, 21.2, 19.1, 20.6, 15.2, 7. , 8.1, 13.6, 20.1, 21.8, 24.5, 23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]), &#39;feature_names&#39;: array([&#39;CRIM&#39;, &#39;ZN&#39;, &#39;INDUS&#39;, &#39;CHAS&#39;, &#39;NOX&#39;, &#39;RM&#39;, &#39;AGE&#39;, &#39;DIS&#39;, &#39;RAD&#39;, &#39;TAX&#39;, &#39;PTRATIO&#39;, &#39;B&#39;, &#39;LSTAT&#39;], dtype=&#39;&lt;U7&#39;), &#39;DESCR&#39;: &#34;.. _boston_dataset: n nBoston house prices dataset n n n**Data Set Characteristics:** n n :Number of Instances: 506 n n :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. n n :Attribute Information (in order): n - CRIM per capita crime rate by town n - ZN proportion of residential land zoned for lots over 25,000 sq.ft. n - INDUS proportion of non-retail business acres per town n - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) n - NOX nitric oxides concentration (parts per 10 million) n - RM average number of rooms per dwelling n - AGE proportion of owner-occupied units built prior to 1940 n - DIS weighted distances to five Boston employment centres n - RAD index of accessibility to radial highways n - TAX full-value property-tax rate per $10,000 n - PTRATIO pupil-teacher ratio by town n - B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town n - LSTAT % lower status of the population n - MEDV Median value of owner-occupied homes in $1000&#39;s n n :Missing Attribute Values: None n n :Creator: Harrison, D. and Rubinfeld, D.L. n nThis is a copy of UCI ML housing dataset. nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/ n n nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. n nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. &#39;Hedonic nprices and the demand for clean air&#39;, J. Environ. Economics &amp; Management, nvol.5, 81-102, 1978. Used in Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics n...&#39;, Wiley, 1980. N.B. Various transformations are used in the table on npages 244-261 of the latter. n nThe Boston house-price data has been used in many machine learning papers that address regression nproblems. n n.. topic:: References n n - Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&#39;, Wiley, 1980. 244-261. n - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. n&#34;, &#39;filename&#39;: &#39;C: Users mrsid anaconda3 lib site-packages sklearn datasets data boston_house_prices.csv&#39;} . dataset= pd.DataFrame(df.data) print(dataset.head()) . 0 1 2 3 4 5 6 7 8 9 10 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 11 12 0 396.90 4.98 1 396.90 9.14 2 392.83 4.03 3 394.63 2.94 4 396.90 5.33 . dataset.head() . 0 1 2 3 4 5 6 7 8 9 10 11 12 . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | . dataset.columns=df.feature_names . dataset.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | . df.target.shape . (506,) . dataset[&quot;price&quot;]=df.target . dataset.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT price . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . X=dataset.iloc[:,:-1] # independent fratures y=dataset.iloc[:,-1] # dependent frature . Linear Regression . from sklearn.model_selection import cross_val_score from sklearn.linear_model import LinearRegression lin_regressor = LinearRegression() mse = cross_val_score(lin_regressor,X,y,scoring=&#39;neg_mean_squared_error&#39;,cv=5) #cv=crossvalidation mean_mse = np.mean(mse) print(mean_mse) # this value should be nearer to zero . -37.13180746769922 . Ridge Regression . from sklearn.linear_model import Ridge from sklearn.model_selection import GridSearchCV ridge=Ridge() parameters = {&#39;alpha&#39;:[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]} ridge_regressor=GridSearchCV(ridge,parameters,scoring = &#39;neg_mean_squared_error&#39;,cv=5) ridge_regressor.fit(X,y) . GridSearchCV(cv=5, error_score=nan, estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver=&#39;auto&#39;, tol=0.001), iid=&#39;deprecated&#39;, n_jobs=None, param_grid={&#39;alpha&#39;: [1e-15, 1e-10, 1e-08, 0.001, 0.01, 1, 5, 10, 20, 30, 35, 40, 45, 50, 55, 100]}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=&#39;neg_mean_squared_error&#39;, verbose=0) . print(ridge_regressor.best_params_) print(ridge_regressor.best_score_) . {&#39;alpha&#39;: 100} -29.905701947540365 . Lasso Regression . from sklearn.linear_model import Lasso from sklearn.model_selection import GridSearchCV lasso=Lasso() parameters = {&#39;alpha&#39;:[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]} Lasso_regressor=GridSearchCV(lasso,parameters,scoring = &#39;neg_mean_squared_error&#39;,cv=5) Lasso_regressor.fit(X,y) print(Lasso_regressor.best_params_) print(Lasso_regressor.best_score_) . {&#39;alpha&#39;: 1} -35.531580220694856 . C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4430.746729651311, tolerance: 3.9191485420792076 positive) C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4397.459304778431, tolerance: 3.3071316790123455 positive) C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3796.653037433508, tolerance: 2.813643886419753 positive) C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2564.292735790545, tolerance: 3.3071762123456794 positive) C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4294.252997826028, tolerance: 3.4809104444444445 positive) . from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0) . prediction_lasso=Lasso_regressor.predict(X_test) prediction_ridge=ridge_regressor.predict(X_test) . Plotting of results . import seaborn as sns sns.distplot(y_test-prediction_lasso) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x20e4370d588&gt; . sns.distplot(y_test-prediction_ridge) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x20e437ebf48&gt; . sns.distplot(y_test-mean_mse) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x20e438e2e48&gt; .",
            "url": "https://mr-siddy.github.io/The-Student-Blog/2021/04/26/Regression-on-Boston-Housing-Dataset.html",
            "relUrl": "/2021/04/26/Regression-on-Boston-Housing-Dataset.html",
            "date": " • Apr 26, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "EDA with Python and Applying Logistic Regression",
            "content": "Import Libraries . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline . The Data . train = pd.read_csv(&#39;titanic_train.csv&#39;) . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . Missing Data . train.isnull() # if True indicates a null value # but it is not a good way as data set can be vast . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 False | False | False | False | False | False | False | False | False | False | True | False | . 1 False | False | False | False | False | False | False | False | False | False | False | False | . 2 False | False | False | False | False | False | False | False | False | False | True | False | . 3 False | False | False | False | False | False | False | False | False | False | False | False | . 4 False | False | False | False | False | False | False | False | False | False | True | False | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 False | False | False | False | False | False | False | False | False | False | True | False | . 887 False | False | False | False | False | False | False | False | False | False | False | False | . 888 False | False | False | False | False | True | False | False | False | False | True | False | . 889 False | False | False | False | False | False | False | False | False | False | False | False | . 890 False | False | False | False | False | False | False | False | False | False | True | False | . 891 rows × 12 columns . sns.heatmap(train.isnull(),yticklabels = False, cbar =False, cmap=&#39;viridis&#39;) # so most of the null values are present in age and cabin . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a91cd84c8&gt; . sns.set_style(&#39;whitegrid&#39;) sns.countplot(x=&#39;Survived&#39;,data=train) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a91d397c8&gt; . sns.set_style(&#39;whitegrid&#39;) sns.countplot(x=&#39;Survived&#39;,hue=&#39;Sex&#39;,data=train, palette=&#39;RdBu_r&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92b67d88&gt; . sns.set_style(&#39;whitegrid&#39;) sns.countplot(x=&#39;Survived&#39;,hue=&#39;Pclass&#39;,data=train, palette=&#39;rainbow&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92c2ee88&gt; . sns.distplot(train[&#39;Age&#39;].dropna(), kde=False, color=&#39;darkred&#39;, bins=40) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92ca32c8&gt; . train[&#39;Age&#39;].hist(bins=30, color=&#39;blue&#39;,alpha=0.3) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92daeb88&gt; . sns.countplot(x=&#39;SibSp&#39;, data=train) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92e6f2c8&gt; . train[&#39;Fare&#39;].hist(bins=30,color=&#39;green&#39;,alpha=0.4) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92ed4e48&gt; . Data Cleaning . plt.figure(figsize=(12,7)) sns.boxplot(x=&#39;Pclass&#39;,y=&#39;Age&#39;,data=train,palette=&#39;winter&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92c2e5c8&gt; . we can see the wealthrer passengers in the higher classes tend to older, which makes sense, we&#39;ll use these average age values to impute based on pcalss for age . def impute_age(cols): Age = cols[0] Pclass = cols[1] if pd.isnull(Age): if Pclass == 1: return 37 elif Pclass == 2: return 29 else: return 24 else: return Age . train[&#39;Age&#39;] = train [[&#39;Age&#39;,&#39;Pclass&#39;]].apply(impute_age,axis=1 ) # now check heatmap again . sns.heatmap(train.isnull(),yticklabels = False, cbar =False, cmap=&#39;viridis&#39;) # so most of the null values are present in age and cabin . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a931ec508&gt; . we have to apply a lot of feature engineering to handle Cabin coz of a lot of Nan values hence we&#39;ll drop it for now . train.drop(&#39;Cabin&#39;,axis=1,inplace=True) . sns.heatmap(train.isnull(),yticklabels = False, cbar =False, cmap=&#39;viridis&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a93031ac8&gt; . train.dropna(inplace=True) . sns.heatmap(train.isnull(),yticklabels = False, cbar =False, cmap=&#39;viridis&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a930b6148&gt; . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | S | . Converting Categorical Features . we&#39;ll need to convert categorical features to dummy variables using pandas, otherwise our machine learning algorithm wont be able to directly take in those features as inputs . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 889 entries, 0 to 890 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 889 non-null int64 1 Survived 889 non-null int64 2 Pclass 889 non-null int64 3 Name 889 non-null object 4 Sex 889 non-null object 5 Age 889 non-null float64 6 SibSp 889 non-null int64 7 Parch 889 non-null int64 8 Ticket 889 non-null object 9 Fare 889 non-null float64 10 Embarked 889 non-null object dtypes: float64(2), int64(5), object(4) memory usage: 83.3+ KB . pd.get_dummies(train[&#39;Embarked&#39;],drop_first=True).head() . Q S . 0 0 | 1 | . 1 0 | 0 | . 2 0 | 1 | . 3 0 | 1 | . 4 0 | 1 | . sex = pd.get_dummies(train[&#39;Sex&#39;],drop_first=True) embark = pd.get_dummies(train[&#39;Embarked&#39;],drop_first=True) . train.drop([&#39;Sex&#39;,&#39;Embarked&#39;,&#39;Name&#39;,&#39;Ticket&#39;],axis=1,inplace=True) . train.head() . PassengerId Survived Pclass Age SibSp Parch Fare . 0 1 | 0 | 3 | 22.0 | 1 | 0 | 7.2500 | . 1 2 | 1 | 1 | 38.0 | 1 | 0 | 71.2833 | . 2 3 | 1 | 3 | 26.0 | 0 | 0 | 7.9250 | . 3 4 | 1 | 1 | 35.0 | 1 | 0 | 53.1000 | . 4 5 | 0 | 3 | 35.0 | 0 | 0 | 8.0500 | . train = pd.concat([train,sex,embark],axis=1) . train.head() . PassengerId Survived Pclass Age SibSp Parch Fare male Q S . 0 1 | 0 | 3 | 22.0 | 1 | 0 | 7.2500 | 1 | 0 | 1 | . 1 2 | 1 | 1 | 38.0 | 1 | 0 | 71.2833 | 0 | 0 | 0 | . 2 3 | 1 | 3 | 26.0 | 0 | 0 | 7.9250 | 0 | 0 | 1 | . 3 4 | 1 | 1 | 35.0 | 1 | 0 | 53.1000 | 0 | 0 | 1 | . 4 5 | 0 | 3 | 35.0 | 0 | 0 | 8.0500 | 1 | 0 | 1 | . Building a Logistic Regression Model . Train Test Split . train.drop(&#39;Survived&#39;,axis=1).head() . PassengerId Pclass Age SibSp Parch Fare male Q S . 0 1 | 3 | 22.0 | 1 | 0 | 7.2500 | 1 | 0 | 1 | . 1 2 | 1 | 38.0 | 1 | 0 | 71.2833 | 0 | 0 | 0 | . 2 3 | 3 | 26.0 | 0 | 0 | 7.9250 | 0 | 0 | 1 | . 3 4 | 1 | 35.0 | 1 | 0 | 53.1000 | 0 | 0 | 1 | . 4 5 | 3 | 35.0 | 0 | 0 | 8.0500 | 1 | 0 | 1 | . train[&#39;Survived&#39;].head() . 0 0 1 1 2 1 3 1 4 0 Name: Survived, dtype: int64 . from sklearn.model_selection import train_test_split . X_train,X_test,y_train,y_test = train_test_split(train.drop(&#39;Survived&#39;,axis=1),train[&#39;Survived&#39;],test_size=0.30,random_state=101) . Training and Predicting . from sklearn.linear_model import LogisticRegression . logmodel = LogisticRegression() logmodel.fit(X_train,y_train) . C: Users mrsid anaconda3 lib site-packages sklearn linear_model _logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . predictions = logmodel.predict(X_test) . from sklearn.metrics import confusion_matrix . accuracy = confusion_matrix(y_test,predictions) . accuracy . array([[149, 14], [ 39, 65]], dtype=int64) . from sklearn.metrics import accuracy_score . accuracy = accuracy_score(y_test,predictions) accuracy . 0.8014981273408239 . predictions . array([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1], dtype=int64) .",
            "url": "https://mr-siddy.github.io/The-Student-Blog/2021/04/25/EDA.html",
            "relUrl": "/2021/04/25/EDA.html",
            "date": " • Apr 25, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Exploratory Data Analysis on Iris Dataset",
            "content": "dataset link :- https://archive.ics.uci.edu/ml/datasets/Iris . Import all libraries . import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt . Creating DataFrame . df = pd.read_csv(&quot;https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv&quot;) . df.head() . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . df.shape . (150, 5) . Univariate Analysis . df.loc[df[&#39;species&#39;]==&#39;setosa&#39;] . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . 5 5.4 | 3.9 | 1.7 | 0.4 | setosa | . 6 4.6 | 3.4 | 1.4 | 0.3 | setosa | . 7 5.0 | 3.4 | 1.5 | 0.2 | setosa | . 8 4.4 | 2.9 | 1.4 | 0.2 | setosa | . 9 4.9 | 3.1 | 1.5 | 0.1 | setosa | . 10 5.4 | 3.7 | 1.5 | 0.2 | setosa | . 11 4.8 | 3.4 | 1.6 | 0.2 | setosa | . 12 4.8 | 3.0 | 1.4 | 0.1 | setosa | . 13 4.3 | 3.0 | 1.1 | 0.1 | setosa | . 14 5.8 | 4.0 | 1.2 | 0.2 | setosa | . 15 5.7 | 4.4 | 1.5 | 0.4 | setosa | . 16 5.4 | 3.9 | 1.3 | 0.4 | setosa | . 17 5.1 | 3.5 | 1.4 | 0.3 | setosa | . 18 5.7 | 3.8 | 1.7 | 0.3 | setosa | . 19 5.1 | 3.8 | 1.5 | 0.3 | setosa | . 20 5.4 | 3.4 | 1.7 | 0.2 | setosa | . 21 5.1 | 3.7 | 1.5 | 0.4 | setosa | . 22 4.6 | 3.6 | 1.0 | 0.2 | setosa | . 23 5.1 | 3.3 | 1.7 | 0.5 | setosa | . 24 4.8 | 3.4 | 1.9 | 0.2 | setosa | . 25 5.0 | 3.0 | 1.6 | 0.2 | setosa | . 26 5.0 | 3.4 | 1.6 | 0.4 | setosa | . 27 5.2 | 3.5 | 1.5 | 0.2 | setosa | . 28 5.2 | 3.4 | 1.4 | 0.2 | setosa | . 29 4.7 | 3.2 | 1.6 | 0.2 | setosa | . 30 4.8 | 3.1 | 1.6 | 0.2 | setosa | . 31 5.4 | 3.4 | 1.5 | 0.4 | setosa | . 32 5.2 | 4.1 | 1.5 | 0.1 | setosa | . 33 5.5 | 4.2 | 1.4 | 0.2 | setosa | . 34 4.9 | 3.1 | 1.5 | 0.1 | setosa | . 35 5.0 | 3.2 | 1.2 | 0.2 | setosa | . 36 5.5 | 3.5 | 1.3 | 0.2 | setosa | . 37 4.9 | 3.1 | 1.5 | 0.1 | setosa | . 38 4.4 | 3.0 | 1.3 | 0.2 | setosa | . 39 5.1 | 3.4 | 1.5 | 0.2 | setosa | . 40 5.0 | 3.5 | 1.3 | 0.3 | setosa | . 41 4.5 | 2.3 | 1.3 | 0.3 | setosa | . 42 4.4 | 3.2 | 1.3 | 0.2 | setosa | . 43 5.0 | 3.5 | 1.6 | 0.6 | setosa | . 44 5.1 | 3.8 | 1.9 | 0.4 | setosa | . 45 4.8 | 3.0 | 1.4 | 0.3 | setosa | . 46 5.1 | 3.8 | 1.6 | 0.2 | setosa | . 47 4.6 | 3.2 | 1.4 | 0.2 | setosa | . 48 5.3 | 3.7 | 1.5 | 0.2 | setosa | . 49 5.0 | 3.3 | 1.4 | 0.2 | setosa | . df_setosa=df.loc[df[&#39;species&#39;]==&#39;setosa&#39;] df_virginica=df.loc[df[&#39;species&#39;]==&#39;virginica&#39;] df_versicolor=df.loc[df[&#39;species&#39;]==&#39;versicolor&#39;] . plt.plot(df_setosa[&#39;sepal_length&#39;],np.zeros_like(df_setosa[&#39;sepal_length&#39;]),&#39;o&#39;) plt.plot(df_virginica[&#39;sepal_length&#39;],np.zeros_like(df_virginica[&#39;sepal_length&#39;]),&#39;o&#39;) plt.plot(df_versicolor[&#39;sepal_length&#39;],np.zeros_like(df_versicolor[&#39;sepal_length&#39;]),&#39;o&#39;) plt.xlabel(&quot;sepal_length&quot;) plt.show() . Bivariate Analysis . sns.FacetGrid(df,hue=&#39;species&#39;,size=5).map(plt.scatter,&quot;petal_length&quot;,&quot;sepal_width&quot;).add_legend(); plt.show() . Multivariate Analysis . sns.pairplot(df,hue=&#39;species&#39;,size=3) . C: Users mrsid anaconda3 lib site-packages seaborn axisgrid.py:2079: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) . &lt;seaborn.axisgrid.PairGrid at 0x13024488808&gt; .",
            "url": "https://mr-siddy.github.io/The-Student-Blog/2021/04/24/EDA-IRIS.html",
            "relUrl": "/2021/04/24/EDA-IRIS.html",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Seaborn || sns",
            "content": "Distribution plots . distplot, joinplot, pairplot . import seaborn as sns import numpy as np . df = sns.load_dataset(&quot;tips&quot;) . df.head() # tip is dependent feature # others are independent features . total_bill tip sex smoker day time size . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | . df.dtypes . total_bill float64 tip float64 sex category smoker category day category time category size int64 dtype: object . Correlation with Heatmap . df.corr() # correlarion can only be found out if values are floating point or integers # corr values range b/w -1 to +1 . total_bill tip size . total_bill 1.000000 | 0.675734 | 0.598315 | . tip 0.675734 | 1.000000 | 0.489299 | . size 0.598315 | 0.489299 | 1.000000 | . Observations :- 1) +ve corr -&gt;&gt; Total bill inc then tip will also inc . sns.heatmap(df.corr()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d218d27108&gt; . JointPlot . Univariate analysis . sns.jointplot(x=&#39;tip&#39;,y=&#39;total_bill&#39;,data=df,kind=&#39;hex&#39;) # hex=hexagonal shape . &lt;seaborn.axisgrid.JointGrid at 0x2d219541988&gt; . sns.jointplot(x=&#39;tip&#39;,y=&#39;total_bill&#39;,data=df,kind=&#39;reg&#39;) # reg gives probablity density line(on graph) and regression line (inside plot) . &lt;seaborn.axisgrid.JointGrid at 0x2d2197b6d88&gt; . Pair plot . same data row is matched with another variable&#39;s value . sns.pairplot(df, hue=&#39;sex&#39;) . &lt;seaborn.axisgrid.PairGrid at 0x2d219f05bc8&gt; . Dist Plot . sns.distplot(df[&#39;tip&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21a51bcc8&gt; . sns.distplot(df[&#39;tip&#39;],kde =False,bins=10) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21a657dc8&gt; . Categorical Plots . Count Plot . sns.countplot(&#39;sex&#39;,data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21a6d2688&gt; . Bar Plot . sns.barplot(x=&#39;total_bill&#39;,y=&#39;sex&#39;,data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21b8f2788&gt; . Box Plot . sns.boxplot(&#39;sex&#39;, &#39;total_bill&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21b950348&gt; . sns.boxplot(x=&#39;day&#39;, y=&#39;total_bill&#39;, data=df, palette=&#39;rainbow&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21b9b9a48&gt; . sns.boxplot(data=df, orient=&#39;v&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21baf1708&gt; . sns.boxplot(x=&#39;total_bill&#39;, y=&#39;day&#39;, hue=&#39;smoker&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21bb73288&gt; . Violin Plot . sns.violinplot(x=&quot;total_bill&quot;,y=&#39;day&#39;,data=df, palette=&#39;rainbow&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21bc73488&gt; .",
            "url": "https://mr-siddy.github.io/The-Student-Blog/2021/04/23/Seaborn.html",
            "relUrl": "/2021/04/23/Seaborn.html",
            "date": " • Apr 23, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Matplotlib || plt",
            "content": "import matplotlib.pyplot as plt %matplotlib inline . import numpy as np . x = np.arange(0,10) y=np.arange(11,21) . a=np.arange(40,50) b=np.arange(50,60) . Scatter plot . plt.scatter(x,y,c=&#39;g&#39;) # c= color plt.xlabel(&#39;X axis&#39;) plt.ylabel(&#39;Y axis&#39;) plt.title(&#39;Graph in 2D&#39;) plt.savefig(&#39;g1.png&#39;) plt.show() . plt plot . plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x20b5b3bef48&gt;] . y=x*x plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x20b5b4228c8&gt;] . plt.plot(x,y,&#39;r&#39;) . [&lt;matplotlib.lines.Line2D at 0x20b5b447d08&gt;] . plt.plot(x,y,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x20b5b4ef748&gt;] . plt.plot(x,y,&#39;r*-&#39;) . [&lt;matplotlib.lines.Line2D at 0x20b5b562fc8&gt;] . Subplots . plt.subplot(2,2,1) # 2 rows 2 cols 1 position plt.plot(x,y,&#39;r&#39;) plt.subplot(2,2,2) plt.plot(x,y,&#39;g&#39;) plt.subplot(2,2,3) plt.plot(x,y,&#39;b&#39;) . [&lt;matplotlib.lines.Line2D at 0x20b5b61f088&gt;] . . np.pi . 3.141592653589793 . x = np.arange(0,4*np.pi,0.1) y=np.sin(x) plt.title(&quot;sine wave form&quot;) plt.plot(x,y) plt.show() . x=np.arange(0,5*np.pi,0.1) y_sin = np.sin(x) y_cos = np.cos(x) plt.subplot(2,1,1) plt.plot(x,y_sin,&#39;r--&#39;) plt.title(&quot;sine graph&quot;) plt.subplot(2,1,2) plt.plot(x,y_cos,&#39;g--&#39;) plt.title(&quot;cosine graph&quot;) plt.show() . Bar plot . x= [2,8,10] y = [11,16,18] x2 = [3,9,11] y2 = [4,7,9] plt.bar(x,y) plt.bar(x2,y2,color =&#39;g&#39;) plt.title(&#39;Bar graph&#39;) plt.ylabel( &#39;Yaxis&#39;) plt.xlabel( &#39;Xaxis&#39;) plt.show() . Histograms . a = np.array([1,2,3,4,5,5,6,67,7,8,8,9]) # y axis == bins - desity or count plt.hist(a) plt.title(&#39;histogram&#39;) plt.show() . Box plot . data = [np.random.normal(0,std,100) for std in range(1,4)] # selecting a normal distribution b/w low=0, to std, step=100 # rectangular box plot plt.boxplot(data, vert=True, patch_artist= True) . {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x20b5bb46688&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb46f08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb55d08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb55e88&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb6b188&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb6ba08&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x20b5bb4b8c8&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb4bf48&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb5bdc8&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb5bec8&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb6bb88&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb71a08&gt;], &#39;boxes&#39;: [&lt;matplotlib.patches.PathPatch at 0x20b5bb46048&gt;, &lt;matplotlib.patches.PathPatch at 0x20b5bb50fc8&gt;, &lt;matplotlib.patches.PathPatch at 0x20b5bb65ac8&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x20b5bb4bfc8&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb5fd08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb71b88&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x20b5bb50f08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb5fe08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb76a08&gt;], &#39;means&#39;: []} . data . [array([-0.7784494 , -0.30130908, 0.54002525, -0.51800759, 0.01819769, -0.83990426, -0.28781469, 0.04318482, 1.23528389, 2.1785494 , -2.0737086 , 1.0928547 , -0.0187436 , 1.26047616, -0.22879622, 0.7987299 , -1.32200805, 1.5095032 , -0.90634209, -0.88452427, 0.21450132, -0.33105648, -0.89893418, 0.2640048 , 0.18846496, -0.13365763, -0.56769452, 1.70685974, 2.50448167, -0.71739823, -2.15135456, -0.79866835, 0.01126657, 0.03509671, 0.70977944, -0.48825295, -0.51388798, 0.03850738, -0.11959896, -1.44425172, -0.48869629, 1.99891486, -0.79457436, 0.82734671, -0.21331385, -1.01447424, -1.62881497, 1.55287689, -0.76185124, -1.33031956, -0.24552639, 0.07408732, -2.05106282, 1.08293709, -0.39720809, -0.37170031, -0.78308727, 0.94345425, -1.61168896, 0.75191668, -0.19178661, 0.35292808, -0.32761845, -0.12057788, -0.90665516, 0.61673275, 0.3552815 , -0.75085115, 0.95438335, -0.4752099 , -1.22754795, 0.90739187, 0.98549253, 1.17860435, -0.47033725, -1.11863367, -2.1007785 , -1.28848407, -0.97587155, -1.50746364, 0.15689869, -1.29434923, 0.95408283, 0.38562582, 1.09328084, -0.83567472, 1.46300781, 0.21707649, 1.04889211, 0.13129867, 0.78442675, 0.21995366, 1.63712729, 1.50326651, 0.28453443, -0.2031552 , -0.28490282, 1.33678566, 2.37008989, 0.79503051]), array([ 1.23556973, -0.02072204, -1.12229404, -2.96722053, -1.30085601, -2.60421508, 1.24700109, -0.31148209, -2.52475577, -3.79873713, -0.5184776 , -1.40388223, -0.76082764, 1.21536502, -0.98142646, 0.43235375, 2.01282379, -0.21453285, 3.61200475, 1.8287454 , -2.37699005, -4.43876649, -1.5534308 , 0.19087839, 0.63776082, -3.89796591, -0.77253082, 0.15942456, 1.50682854, -2.13153439, -0.03070496, -0.87138476, -3.60486968, -3.73673651, 1.36459964, -0.57526159, 1.74855 , -1.59916748, -2.53317411, 0.34688596, -0.39179164, 3.50326963, -2.16398775, 1.6853139 , 0.93583756, -3.19704488, 2.29302575, 0.1907704 , 1.65541487, -1.30203682, 2.56856035, 0.0327959 , 4.19304044, -1.00926479, -2.24279789, -0.69572595, -1.76483291, 3.0767504 , -2.20523853, 3.85941305, 0.02224512, 0.51100795, -0.64877433, -0.97541769, -0.55332363, 0.68110681, 1.04656981, -1.66401884, -2.22326276, 2.5260883 , 1.23117647, -0.60578903, 0.08622414, 1.41381078, -2.7653705 , -0.97335699, 2.92662744, -0.83610816, 2.29915347, 0.01851729, -1.31768037, -1.48470864, 1.02320517, 0.44434635, -3.43562133, -0.4494547 , 0.08147359, 3.30459418, 1.80139721, -1.308831 , -0.99884576, -1.46526386, -0.54199541, 1.12811024, 2.97529432, 1.64583481, -0.78990555, -0.74874302, -4.4103771 , -2.48981923]), array([ 3.95391703e+00, -1.07121577e+00, -3.84668853e+00, 6.77840007e+00, -2.19381045e+00, 7.10352670e-01, 6.73618307e-01, 1.37069922e+00, -3.81843396e+00, 1.26967121e+00, -2.22084017e+00, -3.53653835e+00, 9.12261523e-01, 3.46900445e+00, 5.60861189e-01, 1.81888792e+00, 7.13406114e-01, -3.34833646e+00, 1.39887349e+00, -1.53083906e+00, 3.99241572e+00, -1.95620365e+00, -1.32736259e+00, 1.45314767e+00, 1.86896524e+00, 1.41268309e+00, -2.04054499e+00, -3.22104097e+00, -3.38356292e+00, -1.07288730e+00, -2.13342416e+00, -1.17784314e+00, -5.50678185e-01, -2.93018741e+00, 6.09593785e+00, 3.56688350e-01, -2.74400006e+00, 1.41395686e+00, -1.06679209e+00, 3.99608167e+00, -1.63810367e+00, 3.26794993e+00, -2.17703756e+00, 5.76026096e+00, -3.16019468e+00, -2.04739358e+00, -9.21248072e-01, -1.17306562e+00, -1.40941302e+00, -3.39076210e+00, 8.42848917e+00, -2.23424984e+00, 1.51486619e+00, 3.39342705e+00, -3.71272706e+00, 9.32418444e+00, -2.89173783e+00, -7.17807468e-01, 6.45628003e+00, 2.46759215e+00, -5.40677123e-01, 1.03397626e+00, -4.61687260e-01, 2.28964222e+00, -1.45379187e+00, 1.09286059e+00, 1.66547924e+00, 2.60394771e+00, 3.59662329e-02, -1.58705864e+00, -2.26368232e+00, 2.50848563e+00, -1.72671381e+00, -3.19559078e+00, -9.92987939e-01, 8.91871959e-01, 1.03963870e+00, -4.01271402e-01, 3.12010149e+00, -1.35404888e+00, 2.93841033e+00, -9.41879808e-02, 5.56786269e-01, -9.35989605e-01, 1.10483247e+00, -1.21961918e+00, -4.03470597e-01, -1.41275722e-01, 2.15839643e-01, -2.90275833e+00, 6.03367683e+00, 4.09121350e+00, 3.09437534e+00, -2.16658125e-03, 2.75046954e+00, 8.71768377e-01, -2.33004375e+00, -8.64465990e-03, 2.06668848e+00, 5.57575505e-01])] . Pi chart . labels = &#39;python&#39;,&#39;c++&#39;, &#39;ruby&#39;, &#39;java&#39; sizes = [215,130,245,210] colors = [&#39;gold&#39;, &#39;yellowgreen&#39;,&#39;lightcoral&#39;, &#39;lightskyblue&#39;] explode = (0.1,0,0,0) #explode 1st slice #plot plt.pie(sizes,explode=explode,labels=labels,colors=colors, autopct=&#39;%1.1f%%&#39;,shadow=True) plt.axis(&#39;equal&#39;) plt.show() .",
            "url": "https://mr-siddy.github.io/The-Student-Blog/2021/04/23/Matplotlib.html",
            "relUrl": "/2021/04/23/Matplotlib.html",
            "date": " • Apr 23, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Pyforest - Import all Python Data Science Libraries",
            "content": "pip install Pyforest . Collecting Pyforest Downloading pyforest-1.0.3.tar.gz (14 kB) Building wheels for collected packages: Pyforest Building wheel for Pyforest (setup.py): started Building wheel for Pyforest (setup.py): finished with status &#39;done&#39; Created wheel for Pyforest: filename=pyforest-1.0.3-py2.py3-none-any.whl size=13720 sha256=18e08121d5ee96f79c928bf37f744f0c937b1253580c5189dc7f68057ca46f2c Stored in directory: c: users mrsid appdata local pip cache wheels 72 b6 6c b593d021f7e83f481c5208bc23df0084bcfbeb5b141352b882 Successfully built Pyforest Installing collected packages: Pyforest Successfully installed Pyforest-1.0.3 Note: you may need to restart the kernel to use updated packages. . df = pd.read_csv(&#39;http://winterolympicsmedals.com/medals.csv&#39;) . df.head() . Year City Sport Discipline NOC Event Event gender Medal . 0 1924 | Chamonix | Skating | Figure skating | AUT | individual | M | Silver | . 1 1924 | Chamonix | Skating | Figure skating | AUT | individual | W | Gold | . 2 1924 | Chamonix | Skating | Figure skating | AUT | pairs | X | Gold | . 3 1924 | Chamonix | Bobsleigh | Bobsleigh | BEL | four-man | M | Bronze | . 4 1924 | Chamonix | Ice Hockey | Ice Hockey | CAN | ice hockey | M | Gold | . active_imports() # It imports only those libraries that are in use . import pandas as pd . [&#39;import pandas as pd&#39;] . lst1 = [1,2,3,4,5] lst2 = [6,7,8,9,10] plt.plot(lst1,lst2) plt.xlabel(&quot;X-axis&quot;) plt.ylabel(&quot;Y-axis&quot;) plt.show() . np.array([1,2,3,4,5]) . array([1, 2, 3, 4, 5]) . active_imports() . import matplotlib.pyplot as plt import pandas as pd import numpy as np . [&#39;import matplotlib.pyplot as plt&#39;, &#39;import pandas as pd&#39;, &#39;import numpy as np&#39;] . df1= pd.read_csv(&quot;C: Users mrsid Desktop 30 days of ML challenge NumPy and Pandas mercedesbenz.csv&quot;) . df1.head() . ID y X0 X1 X2 X3 X4 X5 X6 X8 ... X375 X376 X377 X378 X379 X380 X382 X383 X384 X385 . 0 0 | 130.81 | k | v | at | a | d | u | j | o | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 6 | 88.53 | k | t | av | e | d | y | l | o | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 7 | 76.26 | az | w | n | c | d | x | j | x | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 3 9 | 80.62 | az | t | n | f | d | x | l | e | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 13 | 78.02 | az | v | n | f | d | h | d | n | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 rows × 378 columns . sns.distplot(df1[&#39;y&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x27bb681d3c8&gt; . active_imports() . import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np . [&#39;import matplotlib.pyplot as plt&#39;, &#39;import seaborn as sns&#39;, &#39;import pandas as pd&#39;, &#39;import numpy as np&#39;] .",
            "url": "https://mr-siddy.github.io/The-Student-Blog/2021/04/22/Pyforest.html",
            "relUrl": "/2021/04/22/Pyforest.html",
            "date": " • Apr 22, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "NumPy || np",
            "content": "import numpy as np . lst1 = [1,2,3,4] arr = np.array(lst1) . type(arr) . numpy.ndarray . arr . array([1, 2, 3, 4]) . arr.shape . (4,) . list1=[1,2,3,4] list2=[6,7,8,9] list3=[3,4,5,6] arr = np.array([list1,list2,list3]) . arr . array([[1, 2, 3, 4], [6, 7, 8, 9], [3, 4, 5, 6]]) . arr.shape . (3, 4) . arr.reshape(4,3) . array([[1, 2, 3], [4, 6, 7], [8, 9, 3], [4, 5, 6]]) . arr.reshape(1,12) . array([[1, 2, 3, 4, 6, 7, 8, 9, 3, 4, 5, 6]]) . arr.shape . (3, 4) . Indexing . arr . array([[1, 2, 3, 4], [6, 7, 8, 9], [3, 4, 5, 6]]) . arr[0][1] . 2 . arr[1:,3:] . array([[9], [6]]) . arr[1:,2:] . array([[8, 9], [5, 6]]) . arr[:,2:] . array([[3, 4], [8, 9], [5, 6]]) . arr[0:2,0:2] # always remember left:right is left exact and right is one value greater than actual one . array([[1, 2], [6, 7]]) . Inbuilt functions . arr = np.arange(0,10) . arr . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . arr = np.arange(0,10,step=2) arr . array([0, 2, 4, 6, 8]) . # shift+tab for elaborate the function np.linspace(1,10,50) . array([ 1. , 1.18367347, 1.36734694, 1.55102041, 1.73469388, 1.91836735, 2.10204082, 2.28571429, 2.46938776, 2.65306122, 2.83673469, 3.02040816, 3.20408163, 3.3877551 , 3.57142857, 3.75510204, 3.93877551, 4.12244898, 4.30612245, 4.48979592, 4.67346939, 4.85714286, 5.04081633, 5.2244898 , 5.40816327, 5.59183673, 5.7755102 , 5.95918367, 6.14285714, 6.32653061, 6.51020408, 6.69387755, 6.87755102, 7.06122449, 7.24489796, 7.42857143, 7.6122449 , 7.79591837, 7.97959184, 8.16326531, 8.34693878, 8.53061224, 8.71428571, 8.89795918, 9.08163265, 9.26530612, 9.44897959, 9.63265306, 9.81632653, 10. ]) . print(arr) arr[3:] =100 # replace all indexes starting from 3rd to all by 100 print(arr) . [0 2 4 6 8] [ 0 2 4 100 100] . arr1=arr . arr1[3:]=500 arr1 . array([ 0, 2, 4, 500, 500]) . arr # array is actually a reference type hence change is reflected to actual array also . array([ 0, 2, 4, 500, 500]) . arr1 = arr.copy() arr1 . array([ 0, 2, 4, 500, 500]) . arr1[3:] = 800 print(arr1) print(arr) . [ 0 2 4 800 800] [ 0 2 4 500 500] . Some useful conditions for Exploratorty data analysis . arr = np.array([1,2,3,4,5]) val = 2 . arr . array([1, 2, 3, 4, 5]) . print(arr&lt;2) print(arr*2) print(arr%2) . [ True False False False False] [ 2 4 6 8 10] [1 0 1 0 1] . arr[arr&lt;2] . array([1]) . np.ones((2,5),dtype=int) . array([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]) . np.ones(4) . array([1., 1., 1., 1.]) . np.random.rand(3,3) . array([[0.97016302, 0.13230666, 0.31222633], [0.85189366, 0.07856671, 0.57296934], [0.71915461, 0.48997742, 0.24332137]]) . arr_ex = np.random.randn(4,4) # selects from random distribution arr_ex . array([[-0.60890655, -0.67170484, -0.28552398, 1.14748824], [-1.27784825, -0.60587355, -0.87103948, -0.75084882], [ 0.1356478 , 0.67908955, -0.18930585, -1.23064491], [ 0.0557476 , 0.96733176, -0.0119645 , 0.94036578]]) . arr_ex.reshape(16,1) . array([[-0.60890655], [-0.67170484], [-0.28552398], [ 1.14748824], [-1.27784825], [-0.60587355], [-0.87103948], [-0.75084882], [ 0.1356478 ], [ 0.67908955], [-0.18930585], [-1.23064491], [ 0.0557476 ], [ 0.96733176], [-0.0119645 ], [ 0.94036578]]) . import seaborn as sns import pandas as pd . sns.distplot(pd.DataFrame(arr_ex.reshape(16,1))) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b903b859c8&gt; . np.random.randint(0,100,8).reshape(4,2) . array([[17, 74], [67, 13], [91, 60], [92, 75]]) . np.random.random_sample((1,5)) . array([[0.02247093, 0.32708592, 0.95730227, 0.40039247, 0.43461314]]) .",
            "url": "https://mr-siddy.github.io/The-Student-Blog/2021/04/22/NumPy.html",
            "relUrl": "/2021/04/22/NumPy.html",
            "date": " • Apr 22, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Python-Intermediate",
            "content": "def even_odd(num): if num%2==0: print(&quot;number is even&quot;) else: print(&quot;number is odd&quot;) . even_odd(24) . number is even . def hello(name, age=29): # name is positional argument and age is keyword argument print(&quot;my name is {} and age is {}&quot;.format(name, age)) . hello(&#39;sid&#39;) . my name is sid and age is 29 . hello(&#39;sid&#39;,20) . my name is sid and age is 20 . def hello(*args, **kwargs): print(args) print(kwargs) . hello(&#39;sid&#39;,&#39;saxena&#39;,age=29,dob=2000) . (&#39;sid&#39;, &#39;saxena&#39;) {&#39;age&#39;: 29, &#39;dob&#39;: 2000} . lst=[&#39;sid&#39;,&#39;saxena&#39;] dict_args={&#39;age&#39;: 20 ,&#39;dob&#39;:2000} . hello(*lst,**dict_args) . (&#39;sid&#39;, &#39;saxena&#39;) {&#39;age&#39;: 20, &#39;dob&#39;: 2000} . lst = [1,2,3,4,5,6,7] def evenoddsum(lst): even_sum=0 odd_sum=0 for i in lst: if i%2==0: even_sum += i else: odd_sum += i return even_sum,odd_sum . evenoddsum(lst) . (12, 16) . Lambda functions . def addition(a,b): return a+b # Single expression can only be converted . addition(4,5) . 9 . addition = lambda a,b:a+b . addition(5,6) . 11 . def even(num): if num%2==0: return True . even(24) . True . even1 = lambda num:num%2==0 . even1(12) . True . def add(x,y,z): return x+y+z . add(1,2,3) . 6 . three_add = lambda x,y,z:x+y+z . three_add(1,2,3) . 6 . Map Function . def even_odd(num): if num%2==0: return True else: return False . even_odd(23) . False . lst=[1,2,3,4,5,6,7,8] # apply same function on multiple values . map(even_odd,lst) # in order to instantiate convert it into a list ## memory is not intialised yet . &lt;map at 0x2a398c7e248&gt; . list(map(even_odd,lst)) . [False, True, False, True, False, True, False, True] . list(map(lambda num:num%2==0,lst)) . [False, True, False, True, False, True, False] . Filter function . def even(num): if num%2==0: return True . lst=[1,2,3,4,5,6,7] . list(filter(even,lst)) # return elements which satisfy the condition . [2, 4, 6] . list(filter(lambda num:num%2==0,lst)) . [2, 4, 6] . List Comprehension . provide a concise way to create lists, It consists of braces containing an expression followed by for clause, then zero or more for or if clauses . lst1=[] def lst_square(lst): for i in lst: lst1.append(i*i) return lst1 . lst_square([1,2,3,4,5,6,7]) . [1, 4, 9, 16, 25, 36, 49] . lst=[1,2,3,4,5,6,7] #list comprehension [i*i for i in lst ] . [1, 4, 9, 16, 25, 36, 49] . list1=[i*i for i in lst ] print(list1) . [1, 4, 9, 16, 25, 36, 49] . [i*i for i in lst if i%2==0] . [4, 16, 36] . String Formatting . print(&quot;hello&quot;) . hello . str=&quot;hello&quot; print(str) . hello . def greetings(name): return &quot;hello {}&quot;.format(name) . greetings(&#39;sid&#39;) . &#39;hello sid&#39; . def welcome_email(firstname,lastname): return &quot;welcome {}. is your last name is {}&quot;.format(firstname,lastname) # order can not be altered . welcome_email(&#39;sid&#39;,&#39;saxena&#39;) . &#39;welcome sid. is your last name is saxena&#39; . def welcome_email(name,age): return &quot;welcome {name}. is your age is {age}&quot;.format(age=age,name=name) # now ordering can alter . welcome_email(&#39;sid&#39;,20) . &#39;welcome sid. is your age is 20&#39; . def welcome_email(name,age): return &quot;welcome {name1}. is your age is {age1}&quot;.format(age1=age,name1=name) . welcome_email(&#39;sid&#39;,20) . &#39;welcome sid. is your age is 20&#39; . List Iterables vs Iterators . lst = [1,2,3,4,5,6,7] # this whole list is getting initialised in the memory for i in lst: print(i) . 1 2 3 4 5 6 7 . list1=iter(lst) #but in case of iterators whole listdose not get stored in memory it will get accessed only via next . list1 . &lt;list_iterator at 0x2a3996beb08&gt; . next(list1) . 1 . next(list1) . 2 . for i in lst1: print(i) . 1 4 9 16 25 36 49 . .",
            "url": "https://mr-siddy.github.io/The-Student-Blog/2021/04/21/Python-Intermediate.html",
            "relUrl": "/2021/04/21/Python-Intermediate.html",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Python-Basics",
            "content": "bool() . False . type(True) . bool . my_str = &quot;siddy&quot; . print(my_str.isalnum()) print(my_str.isupper()) . True False . my_str=&#39;sid123&#39; my_str.isalnum() . True . Lists . type([]) . list . list = [&#39;mathematics&#39;,100,10,20,122,230] print(list) . [&#39;mathematics&#39;, 100, 10, 20, 122, 230] . list[:] # 100 to 230 list[1:] . [100, 10, 20, 122, 230] . list[1:5] # left is the same index and right is index we want+1 . [100, 10, 20, 122] . print(list) . [&#39;mathematics&#39;, 100, 10, 20, 122, 230, &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;], &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;], &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;]] . list.insert(1,&quot;saxena&quot;) print(list) . [&#39;mathematics&#39;, &#39;saxena&#39;, &#39;saxena&#39;, 100, 10, 20, 122, 230, &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;], &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;], &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;]] . list2 = [1,2,3,4,5] print(list2) . [1, 2, 3, 4, 5] . list2.extend([6,7]) list2 . [1, 2, 3, 4, 5, 6, 7] . operations on list . sum(list2) . 28 . list2.pop() . 7 . list2.count(2) . 1 . list2.index(2,1,4) . 1 . print(min(list2)) print(max(list2)) . 1 6 . list2*2 . [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6] .",
            "url": "https://mr-siddy.github.io/The-Student-Blog/2021/04/21/Python-Basics.html",
            "relUrl": "/2021/04/21/Python-Basics.html",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Python-Advanced",
            "content": "correct way to initialise a Class . class Car: def __init__(self,window,door,enginetype): self.windows=window self.doors=door self.enginetype=enginetype def self_driving(self): return &quot;This is a {} car&quot;.format(self.enginetype) . car1=Car(4,5,&#39;petrol&#39;) # by default this init method is called . dir(car1) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;doors&#39;, &#39;enginetype&#39;, &#39;self_driving&#39;, &#39;windows&#39;] . car2=Car(3,4,&quot;diesel&quot;) . print(car1.windows) . 4 . print(car2.enginetype) . diesel . car1.self_driving() . &#39;This is a petrol car&#39; . print(car2.doors) . 4 . car2.enginetype=&quot;diesel&quot; . print(car2.enginetype) . diesel . Python Exception Handling . try: # code block where exception can occur a=b except: print(&quot;some problem may have occured&quot;) . some problem may have occured . try: # code block where exception can occur a=b except Exception as ex: print(ex) . name &#39;b&#39; is not defined . try: # code block where exception can occur a=b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . the user have not defined the error . try: # code block where exception can occur a=1 b=&#39;s&#39; c=a+b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39; . a=1 b=&#39;s&#39; c=a+b . TypeError Traceback (most recent call last) &lt;ipython-input-15-b5351790d4cc&gt; in &lt;module&gt; 1 a=1 2 b=&#39;s&#39; -&gt; 3 c=a+b TypeError: unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39; . try: # code block where exception can occur a=1 b=&#39;s&#39; c=a+b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except TypeError as ex2: print(&quot;the user has given unsupported data types for addition&quot;) print(&quot;try to make the data type similar&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . the user has given unsupported data types for addition try to make the data type similar . try: a=int(input(&quot;enter number 1 = &quot;)) b=int(input(&quot;enter number 2 = &quot;)) c=a/b d=a+b e=a*b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except TypeError as ex2: print(&quot;the user has given unsupported data types for addition&quot;) print(&quot;try to make the data type similar&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . enter number 1 = 12 enter number 2 = 12 . print(c) print(d) print(e) . 1.0 24 144 . try: a=int(input(&quot;enter number 1 = &quot;)) b=int(input(&quot;enter number 2 = &quot;)) c=a/b d=a+b e=a*b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except TypeError as ex2: print(&quot;the user has given unsupported data types for addition&quot;) print(&quot;try to make the data type similar&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . enter number 1 = 1 enter number 2 = 2 . 12/0 . ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-28-898e9759c56e&gt; in &lt;module&gt; -&gt; 1 12/0 ZeroDivisionError: division by zero . try: a=int(input(&quot;enter number 1 = &quot;)) b=int(input(&quot;enter number 2 = &quot;)) c=a/b d=a+b e=a*b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except TypeError as ex2: print(&quot;the user has given unsupported data types for addition&quot;) print(&quot;try to make the data type similar&quot;) except ZeroDivisionError as ex3: print(&quot;12/0 is not defined&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) else: print(c) print(d) print(e) finally: print(&quot;The execution is complete&quot;) . enter number 1 = 12 enter number 2 = 0 12/0 is not defined The execution is complete . Custom Exception . class Error(Exception): # inheriting the exception class pass class dobException(Error): pass . year = int(input(&quot;Enter the year of birth&quot;)) age = 2021-year try: if age&lt;=30 &amp; age&gt;20: print(&quot;Valid age&quot;) else: raise dobException except dobException: print(&quot;year range is not valid&quot;) . Enter the year of birth1555 year range is not valid . Public Private and Protected Access modifiers . class Car(): def __init__(self,windows, doors, enginetypes): self.windows=windows self.doors=doors self.enginetypes=enginetypes . audi = Car(4,5,&quot;Diesel&quot;) . audi . &lt;__main__.Car at 0x1ed3a2b5ac8&gt; . audi.windows . 4 . audi.windows=5 . audi.windows . 5 . dir(audi) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;doors&#39;, &#39;enginetypes&#39;, &#39;windows&#39;] . class Car(): def __init__(self,windows, doors, enginetypes): self._windows=windows self._doors=doors self._enginetypes=enginetypes . class Truck(Car): def __init__(self,windows, doors, enginetypes, hp): super().__init__(windows,doors,enginetypes) self.hp=hp . truck=Truck(4,2,&quot;Petrol&quot;,720) . dir(truck) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_doors&#39;, &#39;_enginetypes&#39;, &#39;_windows&#39;, &#39;hp&#39;] . truck._doors=4 . truck._doors . 4 . audi._windows . AttributeError Traceback (most recent call last) &lt;ipython-input-45-63cafd091289&gt; in &lt;module&gt; -&gt; 1 audi._windows AttributeError: &#39;Car&#39; object has no attribute &#39;_windows&#39; . dir(audi) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;doors&#39;, &#39;enginetypes&#39;, &#39;windows&#39;] . class Car(): def __init__(self,windows, doors, enginetypes): self.__windows=windows self.__doors=doors self.__enginetypes=enginetypes . audi=Car(4,2,&quot;petrol&quot;) . dir(audi) . [&#39;_Car__doors&#39;, &#39;_Car__enginetypes&#39;, &#39;_Car__windows&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;] . Inheritance . ## Car Blueprint class Car(): def __init__(self, windows, doors, enginetype): self.windows=windows self.doors=doors self.enginetype=enginetype def drive(self): print(&quot;the person drive a car&quot;) . car = Car(4,5,&quot;diesel&quot;) . car.windows . 4 . car.drive() . the person drive a car . class audi(Car): def __init__(self,windows,doors,enginetype,enableai): super().__init__(windows,doors,enginetype) self.enableai=enableai def selfdriving(self): print(&quot;Audi Supports Self driving&quot;) . audiQ7=audi(5,5,&#39;diesel&#39;,True) . audiQ7.enableai . True .",
            "url": "https://mr-siddy.github.io/The-Student-Blog/2021/04/21/Python-Advanced.html",
            "relUrl": "/2021/04/21/Python-Advanced.html",
            "date": " • Apr 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey 👋🏽, I’m Siddhant! . . . Hi, I’m Siddhant saxena, a Machine Learning novice and am very passionate about Artificial Intelligence 🚀 . I’m from India and am currently looking to collaborate on ML/DL projects and would love to work with like-minded individuals. . Talking about Me: . 💻 I’m currently working on ML Projects and am looking for collaborators; | 🌱 I’m currently learning Computer Vision; | 💬 Ask me about anything, I’d try my best to help; | 📫 How to reach me: mrsiddy.py@gmail.com; | . . . ⭐️ From Siddhant .",
          "url": "https://mr-siddy.github.io/The-Student-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mr-siddy.github.io/The-Student-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}