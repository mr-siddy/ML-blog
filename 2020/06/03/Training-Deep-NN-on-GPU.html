<h1 id="training-deep-neural-networks-on-a-gpu">Training Deep Neural Networks on a GPU</h1>

<p>Importing Libraries</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span> 
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>
<span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">make_grid</span>
<span class="kn">from</span> <span class="nn">torch.utils.data.dataloader</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">random_split</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>


<span class="n">matplotlib</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'figure.facecolor'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'#ffffff'</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'data/'</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">image</span><span class="p">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">).</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([28, 28, 1])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">'image.shape:'</span><span class="p">,</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>   <span class="c1"># plt.imshow expects channels to be last dimension in an image tensor, so we use permute to reorder
</span><span class="k">print</span><span class="p">(</span><span class="s">'label:'</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image.shape: torch.Size([1, 28, 28])
label: 5
</code></pre></div></div>

<p><img src="output_5_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>60000
</code></pre></div></div>

<p>dataset[0]</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">143</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">'image.shape:'</span><span class="p">,</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>   <span class="c1"># plt.imshow expects channels to be last dimension in an image tensor, so we use permute to reorder
</span><span class="k">print</span><span class="p">(</span><span class="s">'label:'</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image.shape: torch.Size([1, 28, 28])
label: 2
</code></pre></div></div>

<p><img src="output_8_1.png" alt="png" /></p>

<h2 id="validation-set-using-random_split">validation-set using random_split</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">val_size</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">-</span> <span class="n">val_size</span>

<span class="n">train_ds</span><span class="p">,</span> <span class="n">val_ds</span> <span class="o">=</span> <span class="n">random_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">train_size</span><span class="p">,</span> <span class="n">val_size</span><span class="p">])</span>
<span class="nb">len</span><span class="p">(</span><span class="n">train_ds</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_ds</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(50000, 10000)
</code></pre></div></div>

<h2 id="pytorch-data-loaders">PyTorch data loaders</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">val_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>num_workers attribute tells the data loader instance how many sub-processes to use for data loading. By default, the num_workers value is set to zero, and a value of zero tells the loader to load the data inside the main process.</p>

<p>pin_memory (bool, optional) â€“ If True, the data loader will copy tensors into CUDA pinned memory before returning them.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'images.shape'</span><span class="p">,</span> <span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'grid.shape'</span><span class="p">,</span> <span class="n">make_grid</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">16</span><span class="p">).</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">break</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>images.shape torch.Size([128, 1, 28, 28])
grid.shape torch.Size([3, 242, 482])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># visualize a batch of data using grid
</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'image.shape:'</span><span class="p">,</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">make_grid</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">16</span><span class="p">).</span><span class="n">permute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
    <span class="k">break</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image.shape: torch.Size([1, 28, 28])
</code></pre></div></div>

<p><img src="output_16_1.png" alt="png" /></p>

<h2 id="creating-nn">Creating NN</h2>

<p>Hidden Layers, Activation function and Non-Linearity</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># creating a batch of input tensors
</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'images.shape'</span><span class="p">,</span> <span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'inputs.shape'</span><span class="p">,</span> <span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">break</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>images.shape torch.Size([128, 1, 28, 28])
inputs.shape torch.Size([128, 784])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># from Hidden layer using nn.Linear Layer
</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1">#size of output from hidden layer is 32, can be inc or dec to change the learning capacity of model
</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">32</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="p">)</span> <span class="c1"># it will convert 784 to 32
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([128, 784])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">layer1_outputs</span> <span class="o">=</span> <span class="n">layer1</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'layer1_outputs'</span><span class="p">,</span> <span class="n">layer1_outputs</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>layer1_outputs torch.Size([128, 32])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Direct matrix computation
</span>
<span class="n">layer1_outputs_direct</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">@</span> <span class="n">layer1</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">t</span><span class="p">()</span> <span class="o">+</span> <span class="n">layer1</span><span class="p">.</span><span class="n">bias</span>
<span class="n">layer1_outputs_direct</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([128, 32])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">layer1_outputs</span><span class="p">,</span> <span class="n">layer1_outputs_direct</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
</code></pre></div></div>

<p>Thus, <code class="language-plaintext highlighter-rouge">layer1_outputs</code> and <code class="language-plaintext highlighter-rouge">inputs</code> have a linear relationship, i.e., each element of <code class="language-plaintext highlighter-rouge">layer_outputs</code> is a weighted sum of elements from <code class="language-plaintext highlighter-rouge">inputs</code>. Thus, even as we train the model and modify the weights, <code class="language-plaintext highlighter-rouge">layer1</code> can only capture linear relationships between <code class="language-plaintext highlighter-rouge">inputs</code> and <code class="language-plaintext highlighter-rouge">outputs</code>.</p>

<p><img src="https://i.imgur.com/inXsLuq.png" width="360" /></p>

<p>Next, weâ€™ll use the Rectified Linear Unit (ReLU) function as the activation function for the outputs. It has the formula <code class="language-plaintext highlighter-rouge">relu(x) = max(0,x)</code> i.e. it simply replaces negative values in a given tensor with the value 0. ReLU is a non-linear function, as seen here visually:</p>

<p><img src="https://i.imgur.com/yijV4xF.png" width="420" /></p>

<p>We can use the <code class="language-plaintext highlighter-rouge">F.relu</code> method to apply ReLU to the elements of a tensor.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                     <span class="p">[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="p">.</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[1.0000, 0.0000, 0.0000],
        [0.0000, 0.2000, 3.0000]])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">layer1_outputs</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([128, 32])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># apply the activation function to layer1_outputs
</span>
<span class="n">relu_outputs</span> <span class="o">=</span>  <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">layer1_outputs</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'relu_outputs.shape:'</span><span class="p">,</span> <span class="n">relu_outputs</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'min(layer1_outputs):'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">layer1_outputs</span><span class="p">).</span><span class="n">item</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">'min(relu_outputs):'</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">relu_outputs</span><span class="p">).</span><span class="n">item</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>relu_outputs.shape: torch.Size([128, 32])
min(layer1_outputs): -0.6917587518692017
min(relu_outputs): 0.0
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># creating output layer to convert vectors into 10 target label size 
</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># pass relu_outputs to layer2 to obtain relu_outputs
</span>
<span class="n">layer2_outputs</span> <span class="o">=</span> <span class="n">layer2</span><span class="p">(</span><span class="n">relu_outputs</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'relu_outputs.shape:'</span><span class="p">,</span> <span class="n">relu_outputs</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'layer2_outputs.shape:'</span><span class="p">,</span> <span class="n">layer2_outputs</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>relu_outputs.shape: torch.Size([128, 32])
layer2_outputs.shape: torch.Size([128, 10])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([128, 784])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># now we can use this outputs to compute the loss using F.cross_entropy and adjust weights of layer1 and layer2 using gradient descent 
</span>
<span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">layer2_outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(2.2991, grad_fn=&lt;NllLossBackward&gt;)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># to verify linearity in layer 1 and non-linearity in layer 2  using re-computing the output using basic matrix operations
</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">inputs</span> <span class="o">@</span> <span class="n">layer1</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">t</span><span class="p">()</span> <span class="o">+</span><span class="n">layer1</span><span class="p">.</span><span class="n">bias</span><span class="p">))</span> <span class="o">@</span> <span class="n">layer2</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">t</span><span class="p">()</span> <span class="o">+</span> <span class="n">layer2</span><span class="p">.</span><span class="n">bias</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">layer2_outputs</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
</code></pre></div></div>

<p>if we hadnâ€™t included a non-linear activation between the two linear layers, the final relationship b/w inputs and outputs would be Linear</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># implementing --&gt;  layer2(layer1(inputs))
</span>
<span class="n">outputs2</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span> <span class="o">@</span> <span class="n">layer1</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">t</span><span class="p">()</span> <span class="o">+</span> <span class="n">layer1</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span> <span class="o">@</span> <span class="n">layer2</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">t</span><span class="p">()</span> <span class="o">+</span> <span class="n">layer2</span><span class="p">.</span><span class="n">bias</span> 
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create a single layer to replace two linear layers
</span>
<span class="n">combined_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

<span class="n">combined_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">layer2</span><span class="p">.</span><span class="n">weight</span> <span class="o">@</span> <span class="n">layer1</span><span class="p">.</span><span class="n">weight</span>
<span class="n">combined_layer</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">layer1</span><span class="p">.</span><span class="n">bias</span> <span class="o">@</span> <span class="n">layer2</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">t</span><span class="p">()</span> <span class="o">+</span> <span class="n">layer2</span><span class="p">.</span><span class="n">bias</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># implementation of combined_layer(inputs)
</span>
<span class="n">outputs3</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">@</span> <span class="n">combined_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">t</span><span class="p">()</span> <span class="o">+</span> <span class="n">combined_layer</span><span class="p">.</span><span class="n">bias</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">outputs2</span><span class="p">,</span> <span class="n">outputs3</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>False
</code></pre></div></div>

<h2 id="model">Model</h2>

<p>We are now ready to define our model. As discussed above, weâ€™ll create a neural network with one hidden layer. Hereâ€™s what that means:</p>

<ul>
  <li>
    <p>Instead of using a single <code class="language-plaintext highlighter-rouge">nn.Linear</code> object to transform a batch of inputs (pixel intensities) into outputs (class probabilities), weâ€™ll use two <code class="language-plaintext highlighter-rouge">nn.Linear</code> objects. Each of these is called a <em>layer</em> in the network.</p>
  </li>
  <li>
    <p>The first layer (also known as the hidden layer) will transform the input matrix of shape <code class="language-plaintext highlighter-rouge">batch_size x 784</code> into an intermediate output matrix of shape <code class="language-plaintext highlighter-rouge">batch_size x hidden_size</code>. The parameter <code class="language-plaintext highlighter-rouge">hidden_size</code> can be configured manually (e.g., 32 or 64).</p>
  </li>
  <li>
    <p>Weâ€™ll then apply a non-linear <em>activation function</em> to the intermediate outputs. The activation function transforms individual elements of the matrix.</p>
  </li>
  <li>
    <p>The result of the activation function, which is also of size <code class="language-plaintext highlighter-rouge">batch_size x hidden_size</code>, is passed into the second layer (also known as the output layer).  The second layer transforms it into a matrix of size <code class="language-plaintext highlighter-rouge">batch_size x 10</code>. We can use this output to compute the loss and adjust weights using gradient descent.</p>
  </li>
</ul>

<p>As discussed above, our model will contain one hidden layer. Hereâ€™s what it looks like visually:</p>

<p><img src="https://i.imgur.com/eN7FrpF.png" width="480" /></p>

<p>Letâ€™s define the model by extending the <code class="language-plaintext highlighter-rouge">nn.Module</code> class from PyTorch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MnistModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">Linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>  <span class="c1"># hidden layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">Linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">)</span> <span class="c1"># output layer
</span>        
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">):</span>
        
        <span class="n">xb</span> <span class="o">=</span> <span class="n">xb</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">xb</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># flatten image tensor
</span>         
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">Linear1</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>      <span class="c1"># intermediate outputs using hidden layer
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>           <span class="c1"># applying activation function
</span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">Linear2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>     <span class="c1"># predictions using o/p layer
</span>        
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
    
    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">{</span><span class="s">'val_loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s">'val_acc'</span><span class="p">:</span> <span class="n">acc</span><span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        
        <span class="n">batch_loss</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">]</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">batch_loss</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>   <span class="c1"># Combine losses
</span>        
        <span class="n">batch_accs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s">'val_acc'</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">]</span>
        <span class="n">epoch_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">batch_accs</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>      <span class="c1"># combine accuracies
</span>        
        <span class="k">return</span> <span class="p">{</span><span class="s">'val_loss'</span><span class="p">:</span> <span class="n">epoch_loss</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="s">'val_acc'</span><span class="p">:</span> <span class="n">epoch_acc</span><span class="p">.</span><span class="n">item</span><span class="p">()}</span>
    
    <span class="k">def</span> <span class="nf">epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">result</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Epoch [{}], val_loss: {:4f}, val_acc: {:4f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="s">'val_acc'</span><span class="p">]))</span>
        
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">preds</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># creating a model of hidden layers with 32 activations
</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="mi">784</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">MnistModel</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">out_size</span> <span class="o">=</span> <span class="n">num_classes</span><span class="p">)</span> 
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># model's parameters
</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">():</span>                   <span class="c1"># weights and bias for linear layer and hidden layer
</span>    <span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>                       
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([32, 784])
torch.Size([32])
torch.Size([10, 32])
torch.Size([10])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate some outputs using our model
</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
    <span class="k">break</span>
    
<span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Loss:'</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>    
<span class="k">print</span><span class="p">(</span><span class="s">'outputs.shape: '</span><span class="p">,</span> <span class="n">outputs</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Sample outputs :'</span><span class="p">,</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">2</span><span class="p">].</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Loss: 2.3032402992248535
outputs.shape:  torch.Size([128, 10])
Sample outputs : tensor([[ 0.0308,  0.1783, -0.0268, -0.1578, -0.1123, -0.0013,  0.1285, -0.0645,
         -0.0745, -0.0817],
        [-0.0028,  0.2039, -0.0196, -0.1190, -0.1853,  0.0436,  0.0258,  0.0813,
         -0.1292, -0.0431]])
</code></pre></div></div>

<h2 id="training-the-model-using-gpu">Training the Model using GPU</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>True
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># code to ensure our model uses GPU if available and default CPU
</span>
<span class="k">def</span> <span class="nf">get_default_device</span><span class="p">():</span>
    
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">device</span> <span class="o">=</span> <span class="n">get_default_device</span><span class="p">()</span>
<span class="n">device</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>device(type='cuda')
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># function that can move data and model to chosen device
</span>
<span class="k">def</span> <span class="nf">to_device</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="s">"""MOve tensors to chosen device"""</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span><span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">to_device</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>       <span class="c1"># to method 
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">to_device</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">images</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">break</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([128, 1, 28, 28])
cpu
cuda:0
</code></pre></div></div>

<p>DeviceDataLeader class to wrap our existing data loaders and move batches of data to the selected device, <strong>iter</strong> method to retrieve batches of data and an <strong>len</strong> to get number of batches</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DeviceDataLoader</span><span class="p">():</span>
                                         <span class="c1"># wrap a dataloader to move data to device
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dl</span> <span class="p">,</span> <span class="n">device</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dl</span> <span class="o">=</span> <span class="n">dl</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
                                         <span class="c1"># yield a batch of data after moving it to device
</span>    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">dl</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">to_device</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> 
                                               <span class="c1"># number of batches
</span>    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dl</span><span class="p">)</span>
        
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># yield --&gt; create a generator function that can be used within a for loop 
</span>
<span class="c1"># example
</span>
<span class="k">def</span> <span class="nf">some_numbers</span><span class="p">():</span>
    <span class="k">yield</span> <span class="mi">10</span>
    <span class="k">yield</span> <span class="mi">20</span> 
    <span class="k">yield</span> <span class="mi">30</span>
    
<span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">some_numbers</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>10
20
30
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># wrap data loaders using DeviceDataLoader
</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DeviceDataLoader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="n">DeviceDataLoader</span><span class="p">(</span><span class="n">val_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'xb.device:'</span><span class="p">,</span> <span class="n">xb</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'yb:'</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    <span class="k">break</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>xb.device: cuda:0
yb: tensor([1, 4, 3, 7, 8, 2, 2, 2, 0, 1, 3, 7, 1, 2, 7, 6, 5, 2, 7, 6, 4, 6, 3, 0,
        3, 7, 2, 1, 5, 1, 5, 0, 6, 0, 7, 1, 9, 3, 5, 6, 6, 6, 3, 1, 2, 7, 0, 1,
        7, 4, 3, 9, 7, 2, 1, 4, 3, 4, 8, 3, 1, 0, 9, 6, 4, 0, 2, 5, 2, 6, 7, 4,
        4, 6, 7, 3, 4, 0, 0, 2, 5, 5, 5, 5, 0, 9, 4, 3, 9, 6, 0, 4, 8, 6, 2, 8,
        1, 7, 8, 2, 4, 4, 6, 6, 7, 0, 7, 4, 0, 1, 1, 9, 4, 0, 9, 2, 4, 2, 8, 6,
        7, 1, 5, 3, 7, 8, 4, 2, 6, 8, 1, 7, 3, 8, 4, 4, 7, 9, 7, 2, 9, 6, 8, 7,
        9, 4, 3, 5, 1, 9, 8, 9, 1, 3, 9, 6, 9, 9, 9, 9, 7, 4, 3, 3, 1, 2, 7, 8,
        5, 8, 0, 8, 3, 1, 6, 3, 1, 0, 6, 6, 1, 5, 4, 7, 9, 4, 5, 4, 2, 3, 3, 2,
        9, 6, 6, 3, 8, 4, 4, 2, 1, 7, 7, 3, 4, 5, 8, 2, 9, 9, 6, 8, 7, 6, 0, 6,
        0, 0, 1, 1, 1, 4, 0, 9, 5, 2, 0, 3, 0, 0, 0, 4, 7, 9, 6, 0, 3, 4, 5, 6,
        0, 0, 9, 7, 8, 6, 3, 0, 7, 8, 7, 7, 4, 0, 8, 0], device='cuda:0')
</code></pre></div></div>

<p>Training part</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">):</span>
    
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="p">.</span><span class="n">validation_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">.</span><span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">opt_func</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">):</span>
    
    <span class="n">history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">opt_func</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1">#training phase
</span>        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">training_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            
        <span class="c1">#validation phase
</span>        <span class="n">result</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="n">epoch_end</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
        <span class="n">history</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">history</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model(on GPU)
</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MnistModel</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">out_size</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">to_device</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MnistModel(
  (Linear1): Linear(in_features=784, out_features=32, bias=True)
  (Linear2): Linear(in_features=32, out_features=10, bias=True)
)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="p">[</span><span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)]</span>
<span class="n">history</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[{'val_loss': 2.309469223022461, 'val_acc': 0.12490234524011612}]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">+=</span> <span class="n">fit</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch [0], val_loss: 0.202114, val_acc: 0.941016
Epoch [1], val_loss: 0.156000, val_acc: 0.953223
Epoch [2], val_loss: 0.156326, val_acc: 0.953516
Epoch [3], val_loss: 0.127720, val_acc: 0.962598
Epoch [4], val_loss: 0.119708, val_acc: 0.962793
</code></pre></div></div>

<p>try with more less lr</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">+=</span> <span class="n">fit</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch [0], val_loss: 0.109468, val_acc: 0.967578
Epoch [1], val_loss: 0.105522, val_acc: 0.968164
Epoch [2], val_loss: 0.105320, val_acc: 0.969824
Epoch [3], val_loss: 0.104688, val_acc: 0.969629
Epoch [4], val_loss: 0.104357, val_acc: 0.968555
</code></pre></div></div>

<h2 id="lmao-acc--96">Lmao acc ~ 96%</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># plot losses and accuracies
</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">history</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="s">'-x'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'epoch'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Loss v/s Epochs'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 1.0, 'Loss v/s Epochs')
</code></pre></div></div>

<p><img src="output_67_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">accuracies</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s">'val_acc'</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">history</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">accuracies</span><span class="p">,</span> <span class="s">'-x'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'accuracies'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Accuracies v/s Epochs'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 1.0, 'Accuracies v/s Epochs')
</code></pre></div></div>

<p><img src="output_68_1.png" alt="png" /></p>

<h2 id="testing-with-individual-images">Testing with Individual Images</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'data/'</span><span class="p">,</span>
                     <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                     <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict_image</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="n">xb</span> <span class="o">=</span> <span class="n">to_device</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">device</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">xb</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">yb</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">yb</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">item</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Label:'</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="s">'Prediction:'</span><span class="p">,</span> <span class="n">predict_image</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">model</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cuda:0
Label: 7 Prediction: 7
</code></pre></div></div>

<p><img src="output_72_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">[</span><span class="mi">123</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Label:'</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="s">'Prediction:'</span><span class="p">,</span> <span class="n">predict_image</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">model</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cuda:0
Label: 6 Prediction: 6
</code></pre></div></div>

<p><img src="output_73_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">[</span><span class="mi">183</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Label:'</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="s">'Prediction:'</span><span class="p">,</span> <span class="n">predict_image</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">model</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cuda:0
Label: 0 Prediction: 0
</code></pre></div></div>

<p><img src="output_74_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># overall loss and accuracy of model on training set
</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DeviceDataLoader</span><span class="p">(</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">),</span> <span class="n">device</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
<span class="n">result</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'val_loss': 0.10134970396757126, 'val_acc': 0.9697265625}
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># saving the model
</span>
<span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s">'mnist-feedforward.pth'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>
