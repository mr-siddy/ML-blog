{
  
    
        "post0": {
            "title": "Time Series Anomaly Detection and LSTM Autoencoder for ECG Data using Pytorch",
            "content": "Importing Libraries . import torch import copy import numpy as np import pandas as pd import seaborn as sns from pylab import rcParams import matplotlib.pyplot as plt from matplotlib import rc from sklearn.model_selection import train_test_split from torch import nn, optim import torch.nn.functional as F from arff2pandas import a2p %matplotlib inline %config InlineBackend.figure_format=&#39;retina&#39; sns.set(style=&#39;whitegrid&#39;, palette=&#39;muted&#39;, font_scale=1.2) HAPPY_COLORS_PALETTE = [&quot;#01BEFE&quot;, &quot;#FFDD00&quot;, &quot;#FF7D00&quot;, &quot;#FF006D&quot;, &quot;#ADFF02&quot;, &quot;#8F00FF&quot;] sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE)) rcParams[&#39;figure.figsize&#39;] = 12, 8 RANDOM_SEED = 42 np.random.seed(RANDOM_SEED) torch.manual_seed(RANDOM_SEED) . &lt;torch._C.Generator at 0x7f976c556370&gt; . Dataset Description . We have 5 types of hearbeats (classes): . Normal (N) | R-on-T Premature Ventricular Contraction (R-on-T PVC) | Premature Ventricular Contraction (PVC) | Supra-ventricular Premature or Ectopic Beat (SP or EB) | Unclassified Beat (UB). | . Assuming a healthy heart and a typical rate of 70 to 75 beats per minute, each cardiac cycle, or heartbeat, takes about 0.8 seconds to complete the cycle. Frequency: 60–100 per minute (Humans) Duration: 0.6–1 second (Humans) . device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . device . device(type=&#39;cuda&#39;) . with open(&#39;ECG5000_TRAIN.arff&#39;) as f: train = a2p.load(f) with open(&#39;ECG5000_TEST.arff&#39;) as f: test = a2p.load(f) . df = train.append(test) df = df.sample(frac=1.0) df.shape . (5000, 141) . df.head() . att1@NUMERIC att2@NUMERIC att3@NUMERIC att4@NUMERIC att5@NUMERIC att6@NUMERIC att7@NUMERIC att8@NUMERIC att9@NUMERIC att10@NUMERIC ... att132@NUMERIC att133@NUMERIC att134@NUMERIC att135@NUMERIC att136@NUMERIC att137@NUMERIC att138@NUMERIC att139@NUMERIC att140@NUMERIC target@{1,2,3,4,5} . 1001 1.469756 | -1.048520 | -3.394356 | -4.254399 | -4.162834 | -3.822570 | -3.003609 | -1.799773 | -1.500033 | -1.025095 | ... | 0.945178 | 1.275588 | 1.617218 | 1.580279 | 1.306195 | 1.351674 | 1.915517 | 1.672103 | -1.039932 | 1 | . 2086 -1.998602 | -3.770552 | -4.267091 | -4.256133 | -3.515288 | -2.554540 | -1.699639 | -1.566366 | -1.038815 | -0.425483 | ... | 1.008577 | 1.024698 | 1.051141 | 1.015352 | 0.988475 | 1.050191 | 1.089509 | 1.465382 | 0.799517 | 1 | . 2153 -1.187772 | -3.365038 | -3.695653 | -4.094781 | -3.992549 | -3.425381 | -2.057643 | -1.277729 | -1.307397 | -0.623098 | ... | 1.085007 | 1.467196 | 1.413850 | 1.283822 | 0.923126 | 0.759235 | 0.932364 | 1.216265 | -0.824489 | 1 | . 555 0.604969 | -1.671363 | -3.236131 | -3.966465 | -4.067820 | -3.551897 | -2.582864 | -1.804755 | -1.688151 | -1.025897 | ... | 0.545222 | 0.649363 | 0.986846 | 1.234495 | 1.280039 | 1.215985 | 1.617971 | 2.196543 | 0.023843 | 1 | . 205 -1.197203 | -3.270123 | -3.778723 | -3.977574 | -3.405060 | -2.392634 | -1.726322 | -1.572748 | -0.920075 | -0.388731 | ... | 0.828168 | 0.914338 | 1.063077 | 1.393479 | 1.469756 | 1.392281 | 1.144732 | 1.668263 | 1.734676 | 1 | . 5 rows × 141 columns . CLASS_NORMAL = 1 class_names = [&#39;Normal&#39;, &#39;R on T&#39;, &#39;PVC&#39;, &#39;SP&#39;, &#39;UB&#39;] . new_columns = list(df.columns) new_columns[-1] = &#39;target&#39; df.columns = new_columns . df.head() . att1@NUMERIC att2@NUMERIC att3@NUMERIC att4@NUMERIC att5@NUMERIC att6@NUMERIC att7@NUMERIC att8@NUMERIC att9@NUMERIC att10@NUMERIC ... att132@NUMERIC att133@NUMERIC att134@NUMERIC att135@NUMERIC att136@NUMERIC att137@NUMERIC att138@NUMERIC att139@NUMERIC att140@NUMERIC target . 1001 1.469756 | -1.048520 | -3.394356 | -4.254399 | -4.162834 | -3.822570 | -3.003609 | -1.799773 | -1.500033 | -1.025095 | ... | 0.945178 | 1.275588 | 1.617218 | 1.580279 | 1.306195 | 1.351674 | 1.915517 | 1.672103 | -1.039932 | 1 | . 2086 -1.998602 | -3.770552 | -4.267091 | -4.256133 | -3.515288 | -2.554540 | -1.699639 | -1.566366 | -1.038815 | -0.425483 | ... | 1.008577 | 1.024698 | 1.051141 | 1.015352 | 0.988475 | 1.050191 | 1.089509 | 1.465382 | 0.799517 | 1 | . 2153 -1.187772 | -3.365038 | -3.695653 | -4.094781 | -3.992549 | -3.425381 | -2.057643 | -1.277729 | -1.307397 | -0.623098 | ... | 1.085007 | 1.467196 | 1.413850 | 1.283822 | 0.923126 | 0.759235 | 0.932364 | 1.216265 | -0.824489 | 1 | . 555 0.604969 | -1.671363 | -3.236131 | -3.966465 | -4.067820 | -3.551897 | -2.582864 | -1.804755 | -1.688151 | -1.025897 | ... | 0.545222 | 0.649363 | 0.986846 | 1.234495 | 1.280039 | 1.215985 | 1.617971 | 2.196543 | 0.023843 | 1 | . 205 -1.197203 | -3.270123 | -3.778723 | -3.977574 | -3.405060 | -2.392634 | -1.726322 | -1.572748 | -0.920075 | -0.388731 | ... | 0.828168 | 0.914338 | 1.063077 | 1.393479 | 1.469756 | 1.392281 | 1.144732 | 1.668263 | 1.734676 | 1 | . 5 rows × 141 columns . Exploratory Data Analysis . df.target.value_counts() . 1 2919 2 1767 4 194 3 96 5 24 Name: target, dtype: int64 . ax = sns.countplot(df.target) ax.set_xticklabels(class_names); . /home/siddy/anaconda3/envs/torch/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . The normal class, has by far, the most examples. This is great because we&#39;ll use it to train our model. . Let&#39;s have a look at an averaged (smoothed out with one standard deviation on top and bottom of it) Time Series for each class: . def plot_time_series_class(data, class_name, ax, n_steps=10): time_series_df = pd.DataFrame(data) smooth_path = time_series_df.rolling(n_steps).mean() path_deviation = 2 * time_series_df.rolling(n_steps).std() under_line = (smooth_path - path_deviation)[0] over_line = (smooth_path + path_deviation)[0] ax.plot(smooth_path, linewidth=2) ax.fill_between( path_deviation.index, under_line, over_line, alpha=.125 ) ax.set_title(class_name) . classes = df.target.unique() fig, axs = plt.subplots( nrows = len(classes) // 3 + 1, ncols = 3, sharey=True, figsize=(14,8) ) for i, cls in enumerate(classes): ax = axs.flat[i] data = df[df.target == cls] .drop(labels=&#39;target&#39;, axis=1) .mean(axis=0) .to_numpy() plot_time_series_class(data, class_names[i], ax) fig.delaxes(axs.flat[-1]) fig.tight_layout(); . LSTM Autoencoder . I&#39;ll have a look at how to feed Time Series data to an Autoencoder. We&#39;ll use a couple of LSTM layers (hence the LSTM Autoencoder) to capture the temporal dependencies of the data. . To classify a sequence as normal or an anomaly, we&#39;ll pick a threshold above which a heartbeat is considered abnormal. . Reconstruction Loss . When training an Autoencoder, the objective is to reconstruct the input as best as possible. This is done by minimizing a loss function (just like in supervised learning). This function is known as reconstruction loss. Cross-entropy loss and Mean squared error are common examples. . Data Preprocessing . normal_df = df[df.target == str(CLASS_NORMAL)].drop(labels=&#39;target&#39;, axis=1) normal_df.shape . (2919, 140) . anomaly_df = df[df.target != str(CLASS_NORMAL)].drop(labels=&#39;target&#39;, axis=1) anomaly_df.shape . (2081, 140) . train_df, val_df = train_test_split( normal_df, test_size=0.15, random_state=RANDOM_SEED ) val_df, test_df = train_test_split( val_df, test_size=0.33, random_state=RANDOM_SEED ) . print(test_df.shape) print(val_df.shape) print(test_df.shape) . (145, 140) (293, 140) (145, 140) . def create_dataset(df): sequences = df.astype(np.float32).to_numpy().tolist() dataset = [torch.tensor(s).unsqueeze(1).float() for s in sequences] n_seq, seq_len, n_features = torch.stack(dataset).shape return dataset, seq_len, n_features . Each Time Series will be converted to a 2D Tensor in the shape sequence length x number of features (140x1 in our case). . train_dataset, seq_len, n_features = create_dataset(train_df) val_dataset, _, _ = create_dataset(val_df) test_normal_dataset, _, _ = create_dataset(test_df) test_anomaly_dataset, _, _ = create_dataset(anomaly_df) . LSTM Autoencoder . The general Autoencoder architecture consists of two components. An Encoder that compresses the input and a Decoder that tries to reconstruct it. . We&#39;ll use the LSTM Autoencoder from this GitHub repo with some small tweaks. Our model&#39;s job is to reconstruct Time Series data. Let&#39;s start with the Encoder: . class Encoder(nn.Module): def __init__(self, seq_len, n_features, embedding_dim=64): super(Encoder, self).__init__() self.seq_len, self.n_features = seq_len, n_features self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim self.rnn1 = nn.LSTM( input_size=n_features, hidden_size=self.hidden_dim, num_layers=1, batch_first=True ) self.rnn2 = nn.LSTM( input_size=self.hidden_dim, hidden_size=embedding_dim, num_layers=1, batch_first=True ) def forward(self, x): x = x.reshape((1, self.seq_len, self.n_features)) x, (_, _) = self.rnn1(x) x, (hidden_n, _) = self.rnn2(x) return hidden_n.reshape((self.n_features, self.embedding_dim)) . class Decoder(nn.Module): def __init__(self, seq_len, input_dim=64, n_features=1): super(Decoder, self).__init__() self.seq_len, self.input_dim = seq_len, input_dim self.hidden_dim, self.n_features = 2 * input_dim, n_features self.rnn1 = nn.LSTM( input_size=input_dim, hidden_size=input_dim, num_layers=1, batch_first=True ) self.rnn2 = nn.LSTM( input_size=input_dim, hidden_size=self.hidden_dim, num_layers=1, batch_first=True ) self.output_layer = nn.Linear(self.hidden_dim, n_features) def forward(self, x): x = x.repeat(self.seq_len, self.n_features) x = x.reshape((self.n_features, self.seq_len, self.input_dim)) x, (hidden_n, cell_n) = self.rnn1(x) x, (hidden_n, cell_n) = self.rnn2(x) x = x.reshape((self.seq_len, self.hidden_dim)) return self.output_layer(x) . class RecurrentAutoencoder(nn.Module): def __init__(self, seq_len, n_features, embedding_dim=64): super(RecurrentAutoencoder, self).__init__() self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device) self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device) def forward(self, x): x = self.encoder(x) x = self.decoder(x) return x . model = RecurrentAutoencoder(seq_len, n_features, 128) model = model.to(device) . Training . def train_model(model, train_dataset, val_dataset, n_epochs): optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) criterion = nn.L1Loss(reduction=&#39;sum&#39;).to(device) history = dict(train=[], val=[]) best_model_wts = copy.deepcopy(model.state_dict()) best_loss = 10000.0 for epoch in range(1, n_epochs + 1): model = model.train() train_losses = [] for seq_true in train_dataset: optimizer.zero_grad() seq_true = seq_true.to(device) seq_pred = model(seq_true) loss = criterion(seq_pred, seq_true) loss.backward() optimizer.step() train_losses.append(loss.item()) val_losses = [] model = model.eval() with torch.no_grad(): for seq_true in val_dataset: seq_true = seq_true.to(device) seq_pred = model(seq_true) loss = criterion(seq_pred, seq_true) val_losses.append(loss.item()) train_loss = np.mean(train_losses) val_loss = np.mean(val_losses) history[&#39;train&#39;].append(train_loss) history[&#39;val&#39;].append(val_loss) if val_loss &lt; best_loss: best_loss = val_loss best_model_wts = copy.deepcopy(model.state_dict()) print(f&#39;Epoch {epoch}: train loss {train_loss} val loss {val_loss}&#39;) model.load_state_dict(best_model_wts) return model.eval(), history . model, history = train_model( model, train_dataset, val_dataset, n_epochs=10 ) . Epoch 1: train loss 78.96793919924237 val loss 57.008918449740364 Epoch 2: train loss 55.24031583285534 val loss 51.29970774471556 Epoch 3: train loss 50.97752316869684 val loss 50.35719702919189 Epoch 4: train loss 50.59584151271465 val loss 40.614140123230605 Epoch 5: train loss 38.15368703352449 val loss 37.547762724319824 Epoch 6: train loss 34.309216836055214 val loss 37.58409660261239 Epoch 7: train loss 31.98069694416026 val loss 34.298767298154864 Epoch 8: train loss 28.60677365553278 val loss 27.40926725628433 Epoch 9: train loss 26.80304576254141 val loss 24.187094398732885 Epoch 10: train loss 25.63979911073856 val loss 29.823875609518318 . ax = plt.figure().gca() ax.plot(history[&#39;train&#39;]) ax.plot(history[&#39;val&#39;]) plt.ylabel(&#39;Loss&#39;) plt.xlabel(&#39;Epoch&#39;) plt.legend([&#39;train&#39;, &#39;test&#39;]) plt.title(&#39;Loss over training epochs&#39;) plt.show(); . MODEL_PATH = &#39;Time-Series-ECG5000-Pytorch.pth&#39; torch.save(model, MODEL_PATH) . Choosing a threshold . def predict(model, dataset): predictions, losses = [], [] criterion = nn.L1Loss(reduction=&#39;sum&#39;).to(device) with torch.no_grad(): model = model.eval() for seq_true in dataset: seq_true = seq_true.to(device) seq_pred = model(seq_true) loss = criterion(seq_pred, seq_true) predictions.append(seq_pred.cpu().numpy().flatten()) losses.append(loss.item()) return predictions, losses . _, losses = predict(model, train_dataset) sns.distplot(losses, bins=50, kde=True); . /home/siddy/anaconda3/envs/torch/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . THRESHOLD = 26 . Evaluation . Normal hearbeats . predictions, pred_losses = predict(model, test_normal_dataset) sns.distplot(pred_losses, bins=50, kde=True); . /home/siddy/anaconda3/envs/torch/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . correct = sum(l &lt;= THRESHOLD for l in pred_losses) print(f&#39;Correct normal predictions: {correct}/{len(test_normal_dataset)}&#39;) . Correct normal predictions: 111/145 . Anomalies . anomaly_dataset = test_anomaly_dataset[:len(test_normal_dataset)] . predictions, pred_losses = predict(model, anomaly_dataset) sns.distplot(pred_losses, bins=50, kde=True); . /home/siddy/anaconda3/envs/torch/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . correct = sum(l &gt; THRESHOLD for l in pred_losses) print(f&#39;Correct anomaly predictions: {correct}/{len(anomaly_dataset)}&#39;) . Correct anomaly predictions: 144/145 . Looking at Examples . def plot_prediction(data, model, title, ax): predictions, pred_losses = predict(model, [data]) ax.plot(data, label=&#39;true&#39;) ax.plot(predictions[0], label=&#39;reconstructed&#39;) ax.set_title(f&#39;{title} (loss: {np.around(pred_losses[0], 2)})&#39;) ax.legend() . fig, axs = plt.subplots( nrows=2, ncols=6, sharey=True, sharex=True, figsize=(24, 8) ) for i, data in enumerate(test_normal_dataset[:6]): plot_prediction(data, model, title=&#39;Normal&#39;, ax=axs[0, i]) for i, data in enumerate(test_anomaly_dataset[:6]): plot_prediction(data, model, title=&#39;Anomaly&#39;, ax=axs[1, i]) fig.tight_layout(); .",
            "url": "https://mr-siddy.github.io/ML-blog/rnn/2021/07/17/Time-Series-ECG5000-Pytorch.html",
            "relUrl": "/rnn/2021/07/17/Time-Series-ECG5000-Pytorch.html",
            "date": " • Jul 17, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Transfer Learning for Image Classification in PyTorch",
            "content": "How CNN Learns . see CNN explainer - poloclub cnn . Pytorch CNN visualizations - very cool github . . Layer Visualizations - . CNN creates a layered understanding of data, Idea behind transfer learning is we take a CNN that has been already trained on a very large dataset (ex: Imagenet) ie. pretrained model, and we use some of the layers of these model to train custom models for a custom datasets that we are working with, so the features learned in those layers they are going to be useful for solving any image classifiation problem or any CV problem, the only needs to change is the classifier in the end . Dataset: Qxford-IIIT Pets dataset : https://www.robots.ox.ac.uk/~vgg/data/pets/ . from torchvision.datasets.utils import download_url . download_url(&#39;https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz&#39;,&#39;.&#39;) . import tarfile with tarfile.open(&#39;./oxford-iiit-pet.tgz&#39;, &#39;r:gz&#39;) as tar: tar.extractall(path=&#39;./data&#39;) . import os DATA_DIR = &#39;./data/oxford-iiit-pet/images&#39; files = os.listdir(DATA_DIR) files[:5] . [&#39;Maine_Coon_238.jpg&#39;, &#39;staffordshire_bull_terrier_32.jpg&#39;, &#39;great_pyrenees_10.jpg&#39;, &#39;leonberger_57.jpg&#39;, &#39;scottish_terrier_2.jpg&#39;] . def parse_breed(fname): parts = fname.split(&#39;_&#39;) return &#39; &#39;.join(parts[:-1]) . parse_breed(files[3]) . &#39;leonberger&#39; . from PIL import Image def open_image(path): with open(path, &#39;rb&#39;) as f: img = Image.open(f) return img.convert(&#39;RGB&#39;) . import matplotlib.pyplot as plt plt.imshow(open_image(os.path.join(DATA_DIR,files[4]))) . &lt;matplotlib.image.AxesImage at 0x7f0fd892b580&gt; . plt.imshow(open_image(os.path.join(DATA_DIR,files[500]))) . &lt;matplotlib.image.AxesImage at 0x7f0fd504dc40&gt; . Creating a Custom Pytorch Dataset . from torch.utils.data import Dataset # Dataset class needs 3 funcs to be implemented -- __init__, __len__, __getitem__ class PetsDataset(Dataset): def __init__(self, root, transform): super().__init__() self.root = root self.files = [fname for fname in os.listdir(root) if fname.endswith(&#39;.jpg&#39;)] self.classes = list(set(parse_breed(fname) for fname in files)) self.transform = transform def __len__(self): return len(self.files) def __getitem__(self, i): fname = self.files[i] fpath = os.path.join(self.root, fname) img = self.transform(open_image(fpath)) class_idx = self.classes.index(parse_breed(fname)) return img, class_idx . import torchvision.transforms as T img_size = 224 imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization stats that were used to normalize imagenet model dataset = PetsDataset(DATA_DIR, T.Compose([T.Resize(img_size), T.Pad(8, padding_mode=&#39;reflect&#39;), T.RandomCrop(img_size), T.ToTensor(), T.Normalize(*imagenet_stats)])) . len(dataset) . 7390 . dataset.classes . [&#39;english setter&#39;, &#39;chihuahua&#39;, &#39;german shorthaired&#39;, &#39;beagle&#39;, &#39;leonberger&#39;, &#39;newfoundland&#39;, &#39;Russian Blue&#39;, &#39;basset hound&#39;, &#39;american bulldog&#39;, &#39;scottish terrier&#39;, &#39;Ragdoll&#39;, &#39;pug&#39;, &#39;Abyssinian&#39;, &#39;Sphynx&#39;, &#39;english cocker spaniel&#39;, &#39;great pyrenees&#39;, &#39;Siamese&#39;, &#39;pomeranian&#39;, &#39;Persian&#39;, &#39;Maine Coon&#39;, &#39;Egyptian Mau&#39;, &#39;Bombay&#39;, &#39;Birman&#39;, &#39;miniature pinscher&#39;, &#39;keeshond&#39;, &#39;boxer&#39;, &#39;shiba inu&#39;, &#39;British Shorthair&#39;, &#39;saint bernard&#39;, &#39;staffordshire bull terrier&#39;, &#39;havanese&#39;, &#39;Bengal&#39;, &#39;wheaten terrier&#39;, &#39;samoyed&#39;, &#39;japanese chin&#39;, &#39;american pit bull terrier&#39;, &#39;yorkshire terrier&#39;] . len(dataset.classes) . 37 . import torch torch.cuda.empty_cache() import matplotlib.pyplot as plt %matplotlib inline def denormalize(images, means, stds): if len(images.shape) ==3: images = images.unsqueeze(0) means = torch.tensor(means).reshape(1,3,1,1) stds = torch.tensor(stds).reshape(1,3,1,1) return images*stds + means def show_image(img_tensor, label): print(&#39;Label:&#39;, dataset.classes[label], &#39;(&#39; + str(label) + &#39;)&#39;) img_tensor = denormalize(img_tensor, *imagenet_stats)[0].permute((1, 2, 0)) plt.imshow(img_tensor) . show_image(*dataset[10]) . Label: staffordshire bull terrier (29) . show_image(*dataset[111]) . Label: Abyssinian (12) . Creating Training and Validation Sets . from torch.utils.data import random_split . val_pct = 0.1 val_size = int(val_pct *len(dataset)) train_ds, valid_ds = random_split(dataset, [len(dataset) - val_size, val_size]) . len(train_ds), len(valid_ds) . (6651, 739) . from torch.utils.data import DataLoader batch_size = 64 train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True) valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=4, pin_memory=True) . from torchvision.utils import make_grid def show_batch(dl): for images, labels in dl: fig, ax = plt.subplots(figsize=(16, 16)) ax.set_xticks([]); ax.set_yticks([]) images = denormalize(images[:64], *imagenet_stats) ax.imshow(make_grid(images, nrow=8).permute(1, 2, 0)) break . show_batch(train_dl) . Modifying a Pretrained Model (ResNet34) . Transfer learning : https://ruder.io/transfer-learning/ . import torch.nn as nn import torch.nn.functional as F def accuracy(outputs, labels): _, preds = torch.max(outputs, dim=1) return torch.tensor(torch.sum(preds == labels).item()/ len(preds)) class ImageClassificationBase(nn.Module): def training_step(self, batch): images, labels = batch out = self(images) loss = F.cross_entropy(out, labels) return loss def validation_step(self, batch): images, labels = batch out = self(images) loss = F.cross_entropy(out, labels) acc = accuracy(out, labels) return {&#39;val_loss&#39;: loss.detach(), &#39;val_acc&#39;: acc} def validation_epoch_end(self, outputs): batch_losses = [x[&#39;val_loss&#39;] for x in outputs] epoch_loss = torch.stack(batch_losses).mean() batch_accs = [x[&#39;val_acc&#39;] for x in outputs] epoch_acc = torch.stack(batch_accs).mean() return {&#39;val_loss&#39;: epoch_loss.item(), &#39;val_acc&#39;: epoch_acc.item()} def epoch_end(self, epoch, result): print(&quot;Epoch [{}], {} train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}&quot;.format( epoch, &quot;last_lr: {:.5f}&quot;.format(result[&#39;lrs&#39;][-1]) if &#39;lrs&#39; in result else &#39;&#39;, result[&#39;train_loss&#39;], result[&#39;val_loss&#39;], result[&#39;val_acc&#39;])) . from torchvision import models class PetsModel(ImageClassificationBase): def __init__(self, num_classes, pretrained=True): super().__init__() #use a pretrained mode self.network = models.resnet34(pretrained = pretrained) #replace last layer self.network.fc = nn.Linear(self.network.fc.in_features, num_classes) def forward(self, xb): return self.network(xb) . resnet = models.resnet34(pretrained=True) #condesed knowledge is captured and we got them bro we got them . resnet . ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer2): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer3): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=1000, bias=True) ) . GPU Utilities and Training Loop . def get_default_device(): &quot;&quot;&quot;Pick GPU if available, else CPU&quot;&quot;&quot; if torch.cuda.is_available(): return torch.device(&#39;cuda&#39;) else: return torch.device(&#39;cpu&#39;) def to_device(data, device): &quot;&quot;&quot;Move tensor(s) to chosen device&quot;&quot;&quot; if isinstance(data, (list, tuple)): return [to_device(x, device) for x in data] return data.to(device, non_blocking=True) class DeviceDataLoader(): &quot;&quot;&quot;Wrap a dataloader to move data to a device&quot;&quot;&quot; def __init__(self, dl, device): self.dl = dl self.device = device def __iter__(self): &quot;&quot;&quot;Yield a batch of data after moving it to device&quot;&quot;&quot; for b in self.dl: yield to_device(b, self.device) def __len__(self): &quot;&quot;&quot;Number of batches&quot;&quot;&quot; return len(self.dl) . import torch from tqdm.notebook import tqdm @torch.no_grad() def evaluate(model, val_loader): model.eval() outputs = [model.validation_step(batch) for batch in val_loader] return model.validation_epoch_end(outputs) def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD): history = [] optimizer = opt_func(model.parameters(), lr) for epoch in range(epochs): # Training Phase model.train() train_losses = [] for batch in tqdm(train_loader): loss = model.training_step(batch) train_losses.append(loss) loss.backward() optimizer.step() optimizer.zero_grad() # Validation phase result = evaluate(model, val_loader) result[&#39;train_loss&#39;] = torch.stack(train_losses).mean().item() model.epoch_end(epoch, result) history.append(result) return history def get_lr(optimizer): for param_group in optimizer.param_groups: return param_group[&#39;lr&#39;] def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD): torch.cuda.empty_cache() history = [] # Set up custom optimizer with weight decay optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay) # Set up one-cycle learning rate scheduler sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader)) for epoch in range(epochs): # Training Phase model.train() train_losses = [] lrs = [] for batch in tqdm(train_loader): loss = model.training_step(batch) train_losses.append(loss) loss.backward() # Gradient clipping if grad_clip: nn.utils.clip_grad_value_(model.parameters(), grad_clip) optimizer.step() optimizer.zero_grad() # Record &amp; update learning rate lrs.append(get_lr(optimizer)) sched.step() # Validation phase result = evaluate(model, val_loader) result[&#39;train_loss&#39;] = torch.stack(train_losses).mean().item() result[&#39;lrs&#39;] = lrs model.epoch_end(epoch, result) history.append(result) return history . device = get_default_device() device . device(type=&#39;cuda&#39;) . train_dl = DeviceDataLoader(train_dl, device) valid_dl = DeviceDataLoader(valid_dl, device) . Finetuning the Pretrained Model . model = PetsModel(len(dataset.classes), pretrained = True) to_device(model, device); . history = [evaluate(model, valid_dl)] history . /home/siddy/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /opt/conda/conda-bld/pytorch_1623448234945/work/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . [{&#39;val_loss&#39;: 3.8598475456237793, &#39;val_acc&#39;: 0.037063341587781906}] . epochs = 5 max_lr = 0.01 grad_clip = 0.1 weight_decay = 1e-4 opt_func = torch.optim.Adam . %%time history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, grad_clip = grad_clip, weight_decay = weight_decay, opt_func = opt_func) . Epoch [0], last_lr: 0.00757 train_loss: 2.0365, val_loss: 7.6447, val_acc: 0.0372 Epoch [1], last_lr: 0.00950 train_loss: 2.2747, val_loss: 2.6041, val_acc: 0.3282 Epoch [2], last_lr: 0.00611 train_loss: 1.6090, val_loss: 2.3778, val_acc: 0.3658 Epoch [3], last_lr: 0.00188 train_loss: 0.9982, val_loss: 1.0220, val_acc: 0.6620 Epoch [4], last_lr: 0.00000 train_loss: 0.5643, val_loss: 0.7257, val_acc: 0.7617 CPU times: user 3min 45s, sys: 1min 41s, total: 5min 26s Wall time: 5min 28s . torch.cuda.memory_summary(device=None, abbreviated=False) . &#39;|===========================================================================| n| PyTorch CUDA memory summary, device ID 0 | n|| n| CUDA OOMs: 4 | cudaMalloc retries: 8 | n|===========================================================================| n| Metric | Cur Usage | Peak Usage | Tot Alloc | Tot Freed | n|| n| Allocated memory | 336430 KB | 2387 MB | 7256 GB | 7256 GB | n| from large pool | 312192 KB | 2363 MB | 7244 GB | 7243 GB | n| from small pool | 24238 KB | 25 MB | 12 GB | 12 GB | n|| n| Active memory | 336430 KB | 2387 MB | 7256 GB | 7256 GB | n| from large pool | 312192 KB | 2363 MB | 7244 GB | 7243 GB | n| from small pool | 24238 KB | 25 MB | 12 GB | 12 GB | n|| n| GPU reserved memory | 2412 MB | 2474 MB | 6472 MB | 4060 MB | n| from large pool | 2384 MB | 2446 MB | 6438 MB | 4054 MB | n| from small pool | 28 MB | 28 MB | 34 MB | 6 MB | n|| n| Non-releasable memory | 527826 KB | 945 MB | 6983 GB | 6982 GB | n| from large pool | 525440 KB | 943 MB | 6967 GB | 6967 GB | n| from small pool | 2386 KB | 3 MB | 15 GB | 15 GB | n|| n| Allocations | 548 | 803 | 448412 | 447864 | n| from large pool | 72 | 147 | 185894 | 185822 | n| from small pool | 476 | 659 | 262518 | 262042 | n|| n| Active allocs | 548 | 803 | 448412 | 447864 | n| from large pool | 72 | 147 | 185894 | 185822 | n| from small pool | 476 | 659 | 262518 | 262042 | n|| n| GPU reserved segments | 37 | 43 | 71 | 34 | n| from large pool | 23 | 31 | 54 | 31 | n| from small pool | 14 | 14 | 17 | 3 | n|| n| Non-releasable allocs | 58 | 68 | 254011 | 253953 | n| from large pool | 14 | 30 | 84721 | 84707 | n| from small pool | 44 | 54 | 169290 | 169246 | n|===========================================================================| n&#39; . Training a model from scratch . model2 = PetsModel(len(dataset.classes), pretrained=False) to_device(model2, device); . history2 = [evaluate(model2, valid_dl)] history2 . [{&#39;val_loss&#39;: 44.194881439208984, &#39;val_acc&#39;: 0.02642308734357357}] . While the pretrained model reached an accuracy of 80% in less than 3 minutes, the model without pretrained weights could only reach an accuracy of 24%. .",
            "url": "https://mr-siddy.github.io/ML-blog/deep_learning/2021/07/08/Transfer-learning-pytorch.html",
            "relUrl": "/deep_learning/2021/07/08/Transfer-learning-pytorch.html",
            "date": " • Jul 8, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Anime Image Generation using GANS - Pytorch",
            "content": "Introduction to Generative Modeling . Generative Adversarial Networks (GANs) use neural network for generative modeling, It is an unsupervised learning task in machine learning that involves automatically discovering and learning the ragularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset ex:- thispersondoesnotexist.com . . HOW this works bruh : . GAN have two different neural networks, Generator network(takes input a random vector and it generates an image) and Discriminator network(model whose job is to diffrentiate between real images which are drawn from dataset and generated images) . WAY the training works : . we train Discriminator on batches of both Real and Generated images so it should be good at discriminating, we then use discriminator as a part of the loss function of the Generator we take a generated image from the generator we pass them through the discriminator and we try to fool the discriminator, then update the generator. . Generator tries to fool and Discriminator tries to catch the bluff . . GANs are difficult to train and are extremely sensitive to hyperparameters, activation function and regularisation . More Resources: off convex blog, GANs and Divergence Minimization . Dataset : Anime Face Dataset --&gt; https://www.kaggle.com/splcher/animefacedataset . project_name = &#39;anime-dcgan&#39; . Download and Explore the dataset using opendatasets library import opendatasets as od dataset_url = &#39;https://www.kaggle.com/splcher/animefacedataset&#39; od.download(dataset_url) . import os DATA_DIR = &quot;./animefacedataset&quot; print(os.listdir(DATA_DIR)) . [&#39;images&#39;] . print(os.listdir(DATA_DIR + &quot;/images&quot;)[:10]) . [&#39;23203_2008.jpg&#39;, &#39;21344_2008.jpg&#39;, &#39;50099_2015.jpg&#39;, &#39;42947_2013.jpg&#39;, &#39;59998_2018.jpg&#39;, &#39;5180_2003.jpg&#39;, &#39;27731_2009.jpg&#39;, &#39;39250_2012.jpg&#39;, &#39;13478_2006.jpg&#39;, &#39;39408_2012.jpg&#39;] . from torch.utils.data import DataLoader from torchvision.datasets import ImageFolder import torchvision.transforms as T . image_size = 64 # crop images to 64x64 pixels batch_size = 128 stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) # normalize the pixel values with mean and std deviation of 0.5, to get pixel value in range (-1,1) . train_ds = ImageFolder(DATA_DIR, transform=T.Compose([ T.Resize(image_size), T.CenterCrop(image_size), T.ToTensor(), T.Normalize(*stats) ])) train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True) . import torch from torchvision.utils import make_grid import matplotlib.pyplot as plt %matplotlib inline . def denorm(img_tensors): #std=0.5 mean return img_tensors * stats[1][0] + stats[0][0] . def show_images(images, nmax=64): fig, ax = plt.subplots(figsize=(8, 8)) ax.set_xticks([]); ax.set_yticks([]) ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0)) #pytorch images have color channels in the first dimension, where as matplotlib require color channels to be in the last dimension so we permute def show_batch(dl, nmax=64): for images, _ in dl: show_images(images, nmax) break . show_batch(train_dl) . Utilities for working with GPUs . def get_default_device(): if torch.cuda.is_available(): return torch.device(&#39;cuda&#39;) else: return torch.device(&#39;cpu&#39;) def to_device(data, device): if isinstance(data, (list, tuple)): return [to_device(x, device) for x in data] return data.to(device, non_blocking=True) class DeviceDataLoader(): def __init__(self, dl, device): self.dl = dl self.device = device def __iter__(self): for b in self.dl: yield to_device(b, self.device) def __len__(self): return len(self.dl) . device = get_default_device() device . device(type=&#39;cuda&#39;) . train_dl = DeviceDataLoader(train_dl, device) . Discriminator Network . it takes image as input and tries to classify is as &#39;real&#39; or &#39;generated&#39;. I&#39;ll use CNN with binary classification . . Also we&#39;ll use Leaky ReLU as activation function and sigmod function too . . import torch.nn as nn . discriminator = nn.Sequential( # in: 3 x 64 x 64 nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(64), nn.LeakyReLU(0.2, inplace=True), # out: 64 x 32 x 32 nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, inplace=True), # out: 128 x 16 x 16 nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, inplace=True), # out: 256 x 8 x 8 nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(512), nn.LeakyReLU(0.2, inplace=True), # out: 512 x 4 x 4 nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False), # out: 1 x 1 x 1 nn.Flatten(), nn.Sigmoid() ) . Because we use discriminator as a part of the loss function to train the genrator, ReLU will lead to a lot of outputs from the generator being lost, so Leaky ReLU allows the pass of a gradient signal even for the negative value, that makes the gradients from the discriminator flows stronger into the generator. . so the idea here is,none of imformation whould get lost using discriminator and gradient function to train generator . discriminator = to_device(discriminator, device) . Generator Network . input to generator is typically a vector or a matrix of random numbers (ie. latent tensor) . Generator will take a latent tensor of shape (128, 1, 1) and convert to an image tensor of shape (3 x 28 x 28) . To achive this we&#39;ll use ConvTranspose2d Layer from pytorch, which performs as a transposed convolution of deconvolution . To get a better understanding of Transposed Convolution . also read this Convolution arithmetic . . latent_size = 128 . generator = nn.Sequential( # in: latent_size x 1 x 1 nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False), nn.BatchNorm2d(512), nn.ReLU(True), # out: 512 x 4 x 4 nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(256), nn.ReLU(True), # out: 256 x 8 x 8 nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(128), nn.ReLU(True), # out: 128 x 16 x 16 nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False), nn.BatchNorm2d(64), nn.ReLU(True), # out: 64 x 32 x 32 nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False), nn.Tanh() # out: 3 x 64 x 64 ) . we are using tanh(): . hyperbolic tangent activation function : reduces values into the range -1 to 1 --&gt; when 3 x 64 x 64 feature map is generated from convtranspose2d its values can range between -infy to infy , so it convert them to -1 to 1 so that outputs of images are in range -1 to 1 . xb = torch.randn(batch_size, latent_size, 1, 1) # random latent tensors print(xb.shape) fake_images = generator(xb) print(fake_images.shape) show_images(fake_images) . torch.Size([128, 128, 1, 1]) torch.Size([128, 3, 64, 64]) . generator = to_device(generator, device) . Discriminator Training . as the Discriminator is a binary classifier, we can use binary cross entropy loss function to quantify how well it is able to differentiate between real and generated images . . def train_discriminator(real_images, opt_d): # clear discriminator gradients opt_d.zero_grad() # Pass real images through the discriminator real_preds = discriminator(real_images) real_targets = torch.ones(real_images.size(0), 1, device=device) real_loss = F.binary_cross_entropy(real_preds, real_targets) real_score = torch.mean(real_preds).item() # Generate fake images latent = torch.randn(batch_size, latent_size, 1, 1, device=device) fake_images = generator(latent) # Pass fake images through discriminator fake_targets = torch.zeros(fake_images.size(0), 1, device=device) fake_preds = discriminator(fake_images) fake_loss = F.binary_cross_entropy(fake_preds, fake_targets) fake_score = torch.mean(fake_preds).item() # Update discriminator weights loss = real_loss + fake_loss loss.backward() opt_d.step() return loss.item(), real_score, fake_score . Here are the steps involved in training the discriminator. . We expect the discriminator to output 1 if the image was picked from the real MNIST dataset, and 0 if it was generated using the generator network. . | We first pass a batch of real images, and compute the loss, setting the target labels to 1. . | Then we pass a batch of fake images (generated using the generator) pass them into the discriminator, and compute the loss, setting the target labels to 0. . | Finally we add the two losses and use the overall loss to perform gradient descent to adjust the weights of the discriminator. . | . It&#39;s important to note that we don&#39;t change the weights of the generator model while training the discriminator (opt_d only affects the discriminator.parameters()) . Generator Training . Since the outputs of the generator are images, it&#39;s not obvious how we can train the generator. This is where we employ a rather elegant trick, which is to use the discriminator as a part of the loss function. . We generate a batch of images using the generator, pass the into the discriminator. . | We calculate the loss by setting the target labels to 1 i.e. real. We do this because the generator&#39;s objective is to &quot;fool&quot; the discriminator. . | We use the loss to perform gradient descent i.e. change the weights of the generator, so it gets better at generating real-like images to &quot;fool&quot; the discriminator. . | . def train_generator(opt_g): opt_g.zero_grad() #generate fake images latent = torch.randn(batch_size, latent_size, 1, 1, device=device) fake_images = generator(latent) #try to fool the discriminator preds = discriminator(fake_images) targets = torch.ones(batch_size, 1, device=device) loss = F.binary_cross_entropy(preds, targets) #update generator weights loss.backward() opt_g.step() return loss.item() . from torchvision.utils import save_image . sample_dir = &#39;generated&#39; os.makedirs(sample_dir, exist_ok=True) . def save_sample(index, latent_tensors, show=True): fake_images = generator(latent_tensors) fake_fname = &#39;generated-images-{0:0=4d}.png&#39;.format(index) save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8) print(&#39;Saving&#39;, fake_fname) if show: fig, ax = plt.subplots(figsize=(8,8)) ax.set_xticks([]); ax.set_yticks([]) ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0)) . # save one set of images before start training our model fixed_latent = torch.randn(64, latent_size, 1, 1, device=device) . save_sample(0, fixed_latent) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Saving generated-images-0000.png . Full Training Loop . Let&#39;s define a fit function to train the discriminator and generator in tandem for each batch of training data. We&#39;ll use the Adam optimizer with some custom parameters (betas) that are known to work well for GANs. We will also save some sample generated images at regular intervals for inspection. . . from tqdm import tqdm import torch.nn.functional as F . def fit(epoch, lr, start_idx=1): torch.cuda.empty_cache() # Losses and scores losses_g = [] losses_d = [] real_scores = [] fake_scores = [] # create optimizers opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999)) opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999)) for epoch in range(epochs): for real_images, _ in tqdm(train_dl): # Train discriminator loss_d, real_score, fake_score = train_discriminator(real_images, opt_d) # Train generator loss_g = train_generator(opt_g) # Record losses and scores losses_g.append(loss_g) losses_d.append(loss_d) real_scores.append(real_score) fake_scores.append(fake_score) # log losses and scores (last batch) print(&quot;Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}&quot;.format(epoch+1, epochs, loss_g, loss_d, real_score, fake_score)) save_sample(epoch+start_idx, fixed_latent, show=False) return losses_g, losses_d, real_scores, fake_scores . lr = 0.0002 epochs = 10 . history = fit(epochs, lr) . 100%|██████████| 497/497 [06:34&lt;00:00, 1.26it/s] 0%| | 0/497 [00:00&lt;?, ?it/s] . Epoch [1/10], loss_g: 7.7820, loss_d: 0.7976, real_score: 0.9692, fake_score: 0.5054 Saving generated-images-0001.png . 100%|██████████| 497/497 [03:09&lt;00:00, 2.62it/s] 0%| | 0/497 [00:00&lt;?, ?it/s] . Epoch [2/10], loss_g: 6.3187, loss_d: 0.5204, real_score: 0.8973, fake_score: 0.3050 Saving generated-images-0002.png . 100%|██████████| 497/497 [03:08&lt;00:00, 2.63it/s] 0%| | 0/497 [00:00&lt;?, ?it/s] . Epoch [3/10], loss_g: 3.3551, loss_d: 0.6221, real_score: 0.6364, fake_score: 0.0227 Saving generated-images-0003.png . 100%|██████████| 497/497 [03:10&lt;00:00, 2.61it/s] 0%| | 0/497 [00:00&lt;?, ?it/s] . Epoch [4/10], loss_g: 5.2019, loss_d: 0.2498, real_score: 0.8556, fake_score: 0.0619 Saving generated-images-0004.png . 100%|██████████| 497/497 [03:12&lt;00:00, 2.58it/s] 0%| | 0/497 [00:00&lt;?, ?it/s] . Epoch [5/10], loss_g: 5.7978, loss_d: 0.1832, real_score: 0.8876, fake_score: 0.0381 Saving generated-images-0005.png . 100%|██████████| 497/497 [03:11&lt;00:00, 2.60it/s] 0%| | 0/497 [00:00&lt;?, ?it/s] . Epoch [6/10], loss_g: 5.6346, loss_d: 0.0792, real_score: 0.9730, fake_score: 0.0479 Saving generated-images-0006.png . 100%|██████████| 497/497 [03:08&lt;00:00, 2.64it/s] 0%| | 0/497 [00:00&lt;?, ?it/s] . Epoch [7/10], loss_g: 4.3606, loss_d: 0.1008, real_score: 0.9223, fake_score: 0.0060 Saving generated-images-0007.png . 100%|██████████| 497/497 [03:18&lt;00:00, 2.50it/s] 0%| | 0/497 [00:00&lt;?, ?it/s] . Epoch [8/10], loss_g: 5.5023, loss_d: 0.2340, real_score: 0.9572, fake_score: 0.1537 Saving generated-images-0008.png . 100%|██████████| 497/497 [03:23&lt;00:00, 2.44it/s] 0%| | 0/497 [00:00&lt;?, ?it/s] . Epoch [9/10], loss_g: 2.8159, loss_d: 0.7364, real_score: 0.6190, fake_score: 0.0310 Saving generated-images-0009.png . 100%|██████████| 497/497 [03:20&lt;00:00, 2.48it/s] . Epoch [10/10], loss_g: 4.4092, loss_d: 0.1745, real_score: 0.8755, fake_score: 0.0136 Saving generated-images-0010.png . . losses_g, losses_d, real_scores, fake_scores = history . torch.save(generator.state_dict(), &#39;Anime-Generator-state.pth&#39;) torch.save(discriminator.state_dict(), &#39;Anime-Discriminator-state.pth&#39;) . from IPython.display import Image . Image(&#39;./generated/generated-images-0001.png&#39;) . Image(&#39;./generated/generated-images-0007.png&#39;) . Image(&#39;./generated/generated-images-0010.png&#39;) . import cv2 import os vid_fname = &#39;gans_training.avi&#39; files = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if &#39;generated&#39; in f] files.sort() out = cv2.VideoWriter(vid_fname,cv2.VideoWriter_fourcc(*&#39;MP4V&#39;), 1, (530,530)) [out.write(cv2.imread(fname)) for fname in files] out.release() . Training in one video . We can also visualize how the loss changes over time. Visualizing losses is quite useful for debugging the training process. For GANs, we expect the generator&#39;s loss to reduce over time, without the discriminator&#39;s loss getting too high. . plt.plot(losses_d, &#39;-&#39;) plt.plot(losses_g, &#39;-&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.legend([&#39;Discriminator&#39;, &#39;Generator&#39;]) plt.title(&#39;Losses&#39;); . plt.plot(real_scores, &#39;-&#39;) plt.plot(fake_scores, &#39;-&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;score&#39;) plt.legend([&#39;Real&#39;, &#39;Fake&#39;]) plt.title(&#39;Scores&#39;); .",
            "url": "https://mr-siddy.github.io/ML-blog/gans/2021/06/25/GANs-1-Pytorch.html",
            "relUrl": "/gans/2021/06/25/GANs-1-Pytorch.html",
            "date": " • Jun 25, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Anomaly Detection",
            "content": "A Mixed Deep Learning and Statistical approach for Anomaly Detection . Siddhant Saxena1, Vineeta Soni2 . Department of Information and technology, Manipal University Jaipur, 3030071,2 . ABSTRACT . Keywords: Anomaly detection, Network Intrusion, Statistics, Neural Networks . Anomaly detection refers to the problem of finding data patterns that do not confirm expected behavior, but it is a tedious task to prepare a model that performs quite good in a zero-day attack situation, getting the anomaly containing data required to train model is very a complex job because each attack and anomaly varies as per the situation. So here we propose a mixed statistical and deep learning approach that relies on freely available data, we capture data by tapping the network and acquire a .pcap file which can be used to extract features using the cybersectk tool then we compute probabilities of each packet w.r.t Gaussian distribution because every normal packet will lie in normal distribution and packet containing anomaly will deviate from the normal distribution. . The probability of each packet will be calculated using the equation mentioned below, . Probability if each packet will get converted to a tensor of n-dimensions and hence now all these probabilities can be used as the input layer to our deep learning model and hidden layers contain a linear function with ReLU as non-linearity, . And the output layer will provide probabilities of normal and anomalous behavior of the packet. For validation purposes, we will use the joint probability-based approach, which assumes features are independent. . Joint Probabilities of each packet’s probability can be calculated as follows: . . And we assume E by hit and trial and if P(x) &lt; E then the packet is anomalous and if P(x) &gt;= E then it is regular. . . Flow chart for the Anomaly Detection Model .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/06/24/Anomaly-Detection.html",
            "relUrl": "/2021/06/24/Anomaly-Detection.html",
            "date": " • Jun 24, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Classifying Cifar-10 using ResNets - Pytorch",
            "content": "Understanding use of Regularization and Data Augmentation . Exploring the CIFAR10 Dataset . CIFAR-10 is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. . Link:- https://www.kaggle.com/c/cifar-10 . Dataset-credentials:- https://www.cs.toronto.edu/~kriz/cifar.html . . import os import torch import torchvision import tarfile import torch.nn as nn import numpy as np import torch.nn.functional as F from torchvision.datasets.utils import download_url from torchvision.datasets import ImageFolder from torch.utils.data import DataLoader import torchvision.transforms as tt from torch.utils.data import random_split from torchvision.utils import make_grid import matplotlib import matplotlib.pyplot as plt %matplotlib inline matplotlib.rcParams[&#39;figure.facecolor&#39;] = &#39;#ffffff&#39; . project_name = &#39;cifar10-resnet&#39; . data_dir = &#39;./data/cifar10&#39; print(os.listdir(data_dir)) classes = os.listdir(data_dir + &quot;/train&quot;) print(classes) . [&#39;train&#39;, &#39;test&#39;] [&#39;ship&#39;, &#39;cat&#39;, &#39;automobile&#39;, &#39;frog&#39;, &#39;airplane&#39;, &#39;dog&#39;, &#39;truck&#39;, &#39;deer&#39;, &#39;bird&#39;, &#39;horse&#39;] . Data Preprocessing . going to use test set for validation . Channel wise data normalisation . we will normalize the image tensors by substracting the mean and dividing by the standard deviation across each channel. it will mean the data across each channel to 0 and standard deviation to 1. normalizing data prevents the values from any one channel from disproportionately affecting the losses and gradients while training simply by having a higher or wider range of values that others . . Randomized data augmentation . Applying chosen transformations while loading images from the training dataset. we will pad each image by 4 pixels and then take a random crop of size 32 x 32 and then flip the image horizontly with a 50% probability. Since the transformation will be applied randomly and dynamically each time a particular image is loaded, the model sees slightly different images in each epoch of training, which allows it generalize better . . stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) train_tfms = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode=&#39;reflect&#39;), # it is going to shift the image around upto 4 pixels(top/bottom/left/right) each time tt.RandomHorizontalFlip(), # default probab = 0.5 #tt.RandomRotation(), #tt.RandomResizedCrop(256, scale=(0.5, 0.9), ratio=(1, 1)), #tt.ColorJitter(brightness=0.1, contrast =0.1, saturation=0.1), tt.ToTensor(), tt.Normalize(*stats, inplace=True)]) valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)]) # we can not apply transformations to validation set coz it is for testing on real data . we need normalized transformation for validation set too bcoz when we train the model using normalized data then model no longer understands the original pixel values, it only understands the mormalize pixel values which has been shifted using mean and standard deviations, therefore any new input that is used should also contain same normalization . train_ds = ImageFolder(data_dir+&#39;/train&#39;, train_tfms) valid_ds = ImageFolder(data_dir+&#39;/test&#39;, valid_tfms) . batch_size = 400 . train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True) valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True) . def denormalize(images, means, stds): means = torch.tensor(means).reshape(1, 3, 1, 1) stds = torch.tensor(stds).reshape(1, 3, 1, 1) return images * stds + means def show_batch(dl): for images, labels in dl: fig, ax = plt.subplots(figsize=(12, 12)) ax.set_xticks([]); ax.set_yticks([]) denorm_images = denormalize(images, *stats) ax.imshow(make_grid(denorm_images[:64], nrow=8).permute(1, 2, 0).clamp(0, 1)) break . show_batch(train_dl) . Using a GPU . def get_default_device(): if torch.cuda.is_available(): return torch.device(&#39;cuda&#39;) else: return torch.device(&#39;cpu&#39;) . device = get_default_device() device . device(type=&#39;cuda&#39;) . def to_device(data, device): &quot;&quot;&quot;Move tensors to chosen device&quot;&quot;&quot; if isinstance(data, (list,tuple)): return [to_device(x, device) for x in data] return data.to(device, non_blocking = True) # to method . class DeviceDataLoader(): def __init__(self, dl, device): self.dl = dl self.device = device def __iter__(self): for b in self.dl: yield to_device(b, self.device) def __len__(self): return len(self.dl) . train_dl = DeviceDataLoader(train_dl, device) valid_dl = DeviceDataLoader(valid_dl, device) . Model with Residual Blocks and Batch Normalization . we will add residual block, that adds the original input back to the output feature map obtained by passing the input through one or more conv layers . without residual block, these layers are responsible for transforming the input into the output, our entire network in responsible for transforming images 3d rgb 32x32 color images into 10 output class probabilities but when we pass a residual layer then our conv layers are no longer responsible for converting the i/p to o/p rather they only have to calculate the difference between i/p and o/p coz, final o/p is the o/p of covn layer + i/p so simply o/p of conv layer is desired op - i/p layer . so the wieghts can learn more powerful features . . class SimpleResidualBlock(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1) self.relu1 = nn.ReLU() self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1) self.relu2 = nn.ReLU() # in a residual block we can not change the no of o/p channels, coz if we change it ex to 128 then we&#39;ll not be able to add the orginal i/p to o/p coz then i/p shape and o/p shape would not match def forward(self, x): out = self.conv1(x) out = self.relu1(out) out = self.conv2(out) return self.relu2(out) + x . simple_resnet = to_device(SimpleResidualBlock(), device) for images, labels in train_dl: print(images.shape) out = simple_resnet(images) print(out.shape) break del simple_resnet, images, labels torch.cuda.empty_cache() . torch.Size([400, 3, 32, 32]) torch.Size([400, 3, 32, 32]) . Batch Normalization layer: normalizes the o/p of previous layer . some cool resources : Residual blocks Batch Normalization ResNet9 . lmao cool animation down there ResNet34 : . Cmon dude see some more about ResNets . ResNet18 : . . we&#39;ll use ResNet9 architecture: . bro only diagram above is good for easy understanding so do not ignore it and for clarity --&gt; ctrl + mbtn-up . def accuracy(outputs, labels): _, preds = torch.max(outputs, dim=1) return torch.tensor(torch.sum(preds == labels).item()/len(preds)) class ImageClassificationBase(nn.Module): def training_step(self, batch): images, labels = batch out = self(images) loss = F.cross_entropy(out, labels) return loss def validation_step(self, batch): images, labels = batch out = self(images) loss = F.cross_entropy(out, labels) acc = accuracy(out, labels) return {&#39;val_loss&#39;: loss.detach(), &#39;val_acc&#39;: acc} def validation_epoch_end(self, outputs): batch_losses = [x[&#39;val_loss&#39;] for x in outputs] epoch_loss = torch.stack(batch_losses).mean() batch_accs = [x[&#39;val_acc&#39;] for x in outputs] epoch_acc = torch.stack(batch_accs).mean() return {&#39;val_loss&#39;: epoch_loss.item(), &#39;val_acc&#39;: epoch_acc.item()} def epoch_end(self, epoch, result): print(&quot;Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}&quot;.format( epoch, result[&#39;lrs&#39;][-1], result[&#39;train_loss&#39;], result[&#39;val_loss&#39;], result[&#39;val_acc&#39;])) . def conv_block(in_channels, out_channels, pool=False): layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)] if pool: layers.append(nn.MaxPool2d(2)) return nn.Sequential(*layers) # * layers is same as passing one by one args to nn.Sequential class ResNet9(ImageClassificationBase): def __init__(self, in_channels, num_classes): super().__init__() # 400 x 3 x 32 x 32 self.conv1 = conv_block(in_channels, 64) # 64 x 32 x 32 self.conv2 = conv_block(64, 128, pool=True) # 128 x 16 x 16 self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128)) # 128 x 16 x 16 self.conv3 = conv_block(128, 256, pool=True) # 256 x 8 x 8 self.conv4 = conv_block(256, 512, pool=True) # 512 x 4 x 4 self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512)) # 512 x 4 x 4 self.classifier = nn.Sequential(nn.MaxPool2d(4), # 512 x 1 x 1 nn.Flatten(), # 512 nn.Dropout(0.2), # 512 --&gt;used to avoid overfitting nn.Linear(512, num_classes)) # 10 def forward(self, xb): out = self.conv1(xb) out = self.conv2(out) out = self.res1(out) + out out = self.conv3(out) out = self.conv4(out) out = self.res2(out) + out out = self.classifier(out) return out . model = to_device(ResNet9(3, 10), device) model . ResNet9( (conv1): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (conv2): Sequential( (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (res1): Sequential( (0): Sequential( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) ) (conv3): Sequential( (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (conv4): Sequential( (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (res2): Sequential( (0): Sequential( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) ) (classifier): Sequential( (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False) (1): Flatten(start_dim=1, end_dim=-1) (2): Dropout(p=0.2, inplace=False) (3): Linear(in_features=512, out_features=10, bias=True) ) ) . Training the model . Before we train the model, we&#39;re going to make a bunch of small but important improvements to our fit function: . Learning rate scheduling: Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and the one we&#39;ll use is called the &quot;One Cycle Learning Rate Policy&quot;, which involves starting with a low learning rate, gradually increasing it batch-by-batch to a high learning rate for about 30% of epochs, then gradually decreasing it to a very low value for the remaining epochs. Learn more: https://sgugger.github.io/the-1cycle-policy.html | . Yo start with a low learning rate and then slowly keeps inc lr so that model keeps making larger steps and then start dec the lr again coz once model approaches the good set of weights then we want to narrow down and pick the set of weights by taking really small steps towrds that optimal one . . Weight decay: regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function.Learn more: https://towardsdatascience.com/this-thing-called-weight-decay-a7cd4bcfccab | . now Loss = MSE(y_hat, y) + wd * sum(w^2) . When we update weights using gradient descent we do the following: . w(t) = w(t-1) - lr * dLoss / dw . Now since our loss function has 2 terms in it, the derivative of the 2nd term w.r.t w would be: . d(wd w^2) / dw = 2 wd * w (similar to d(x^2)/dx = 2x) . Gradient clipping: Apart from the layer weights and outputs, it also helpful to limit the values of gradients to a small range to prevent undesirable changes in parameters due to large gradient values. This simple yet effective technique is called gradient clipping. Learn more: https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48 | . . @torch.no_grad() def evaluate(model, val_loader): model.eval() # tells model that we are currently evaluating and not training outputs = [model.validation_step(batch) for batch in val_loader] return model.validation_epoch_end(outputs) def get_lr(optimizer): for param_group in optimizer.param_groups: return param_group[&#39;lr&#39;] def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD): torch.cuda.empty_cache() history =[] optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay) sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader)) for epoch in range(epochs): model.train() train_losses= [] lrs = [] for batch in train_loader: loss = model.training_step(batch) train_losses.append(loss) loss.backward() if grad_clip: nn.utils.clip_grad_value_(model.parameters(), grad_clip) optimizer.step() optimizer.zero_grad() lrs.append(get_lr(optimizer)) sched.step() result = evaluate(model, val_loader) result[&#39;train_loss&#39;] = torch.stack(train_losses).mean().item() result[&#39;lrs&#39;] = lrs model.epoch_end(epoch, result) history.append(result) return history . history = [evaluate(model, valid_dl)] history . [{&#39;val_loss&#39;: 1.5891424417495728, &#39;val_acc&#39;: 0.4678846001625061}] . epochs = 16 max_lr = 0.01 grad_clip = 0.1 weight_decay = 1e-4 opt_func = torch.optim.Adam . %%time history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, grad_clip=grad_clip, weight_decay = weight_decay) . Epoch [0], last_lr: 0.00138, train_loss: 0.4685, val_loss: 0.5093, val_acc: 0.8230 Epoch [1], last_lr: 0.00394, train_loss: 0.5068, val_loss: 0.5091, val_acc: 0.8308 Epoch [2], last_lr: 0.00703, train_loss: 0.5393, val_loss: 0.8572, val_acc: 0.7250 Epoch [3], last_lr: 0.00935, train_loss: 0.5397, val_loss: 0.6107, val_acc: 0.8067 Epoch [4], last_lr: 0.00999, train_loss: 0.5024, val_loss: 0.5370, val_acc: 0.8198 Epoch [5], last_lr: 0.00972, train_loss: 0.4943, val_loss: 0.6554, val_acc: 0.7915 Epoch [6], last_lr: 0.00908, train_loss: 0.4549, val_loss: 0.4965, val_acc: 0.8401 Epoch [7], last_lr: 0.00812, train_loss: 0.4190, val_loss: 0.7664, val_acc: 0.7785 Epoch [8], last_lr: 0.00691, train_loss: 0.3934, val_loss: 0.4607, val_acc: 0.8482 Epoch [9], last_lr: 0.00556, train_loss: 0.3638, val_loss: 0.4654, val_acc: 0.8466 Epoch [10], last_lr: 0.00416, train_loss: 0.3323, val_loss: 0.4287, val_acc: 0.8637 Epoch [11], last_lr: 0.00283, train_loss: 0.2908, val_loss: 0.3947, val_acc: 0.8729 Epoch [12], last_lr: 0.00167, train_loss: 0.2651, val_loss: 0.3649, val_acc: 0.8847 Epoch [13], last_lr: 0.00077, train_loss: 0.2317, val_loss: 0.3137, val_acc: 0.8944 Epoch [14], last_lr: 0.00020, train_loss: 0.2126, val_loss: 0.2992, val_acc: 0.9022 Epoch [15], last_lr: 0.00000, train_loss: 0.1960, val_loss: 0.2925, val_acc: 0.9031 CPU times: user 8min 27s, sys: 3min 43s, total: 12min 10s Wall time: 12min 14s . def plot_accuracies(history): accuracies = [x[&#39;val_acc&#39;] for x in history] plt.plot(accuracies, &#39;-x&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;accuracy&#39;) plt.title(&#39;Accuracy vs. No. of epochs&#39;); . plot_accuracies(history) . def plot_losses(history): train_losses = [x.get(&#39;train_loss&#39;) for x in history] val_losses = [x[&#39;val_loss&#39;] for x in history] plt.plot(train_losses, &#39;-bx&#39;) plt.plot(val_losses, &#39;-rx&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.legend([&#39;Training&#39;, &#39;Validation&#39;]) plt.title(&#39;Loss vs. No of epochs&#39;); . plot_losses(history) . def plot_lrs(history): lrs = np.concatenate([x.get(&#39;lrs&#39;, []) for x in history]) plt.plot(lrs) plt.xlabel(&#39;Batch no.&#39;) plt.ylabel(&#39;Learning rate&#39;) plt.title(&#39;Learning Rate vs Batch no.&#39;); . plot_lrs(history) . Testing with individual images . def predict_image(img, model): xb = to_device(img.unsqueeze(0), device) yb = model(xb) _, pred = torch.max(yb, dim=1) return train_ds.classes[pred[0].item()] . img, label = valid_ds[2341] plt.imshow(img.permute(1, 2, 0).clamp(0, 1)) print(&#39;Label:&#39;, train_ds.classes[label], &#39;, Predicted:&#39;, predict_image(img, model)) . Label: bird , Predicted: bird . img, label = valid_ds[1002] plt.imshow(img.permute(1, 2, 0)) print(&#39;Label:&#39;, valid_ds.classes[label], &#39;, Predicted:&#39;, predict_image(img, model)) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Label: automobile , Predicted: truck . img, label = valid_ds[1111] plt.imshow(img.permute(1, 2, 0)) print(&#39;Label:&#39;, valid_ds.classes[label], &#39;, Predicted:&#39;, predict_image(img, model)) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Label: automobile , Predicted: automobile . img, label = valid_ds[1921] plt.imshow(img.permute(1, 2, 0)) print(&#39;Label:&#39;, valid_ds.classes[label], &#39;, Predicted:&#39;, predict_image(img, model)) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Label: automobile , Predicted: automobile . torch.save(model.state_dict(), &#39;cifar-10-resnet9.pth&#39;) .",
            "url": "https://mr-siddy.github.io/ML-blog/deep_learning/2021/06/19/cifar10-resnets.html",
            "relUrl": "/deep_learning/2021/06/19/cifar10-resnets.html",
            "date": " • Jun 19, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Image Classification using Convolutional Neural Networks - Pytorch",
            "content": "Exploring the CIFAR10 Dataset . CIFAR-10 is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. . Link:- https://www.kaggle.com/c/cifar-10 . Dataset-credentials:- https://www.cs.toronto.edu/~kriz/cifar.html . . Importing Libraries . import os import torch import torchvision import tarfile from torchvision.datasets.utils import download_url from torch.utils.data import random_split . project_name = &#39;cifar10-cnn&#39; . dataset_url = &quot;https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz&quot; download_url(dataset_url, &#39;.&#39;) . with tarfile.open(&#39;./cifar10.tgz&#39;, &#39;r:gz&#39;) as tar: # r:gz --&gt; read mode in g zip format tar.extractall(path=&#39;./data&#39;) . data_dir = &#39;./data/cifar10&#39; print(os.listdir(data_dir)) classes = os.listdir(data_dir + &quot;/train&quot;) print(classes) . [&#39;test&#39;, &#39;train&#39;] [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;] . airplane_files = os.listdir(data_dir + &quot;/train/airplane&quot;) print(&quot;No. of test examples for airplanes: &quot;, len(airplane_files)) print(airplane_files[:5]) . No. of test examples for airplanes: 5000 [&#39;0001.png&#39;, &#39;0002.png&#39;, &#39;0003.png&#39;, &#39;0004.png&#39;, &#39;0005.png&#39;] . ship_test_files = os.listdir(data_dir + &quot;/test/ship&quot;) print(&quot;No of test examples for ship:&quot;, len(ship_test_files)) print(ship_test_files[10:50]) . No of test examples for ship: 1000 [&#39;0011.png&#39;, &#39;0012.png&#39;, &#39;0013.png&#39;, &#39;0014.png&#39;, &#39;0015.png&#39;, &#39;0016.png&#39;, &#39;0017.png&#39;, &#39;0018.png&#39;, &#39;0019.png&#39;, &#39;0020.png&#39;, &#39;0021.png&#39;, &#39;0022.png&#39;, &#39;0023.png&#39;, &#39;0024.png&#39;, &#39;0025.png&#39;, &#39;0026.png&#39;, &#39;0027.png&#39;, &#39;0028.png&#39;, &#39;0029.png&#39;, &#39;0030.png&#39;, &#39;0031.png&#39;, &#39;0032.png&#39;, &#39;0033.png&#39;, &#39;0034.png&#39;, &#39;0035.png&#39;, &#39;0036.png&#39;, &#39;0037.png&#39;, &#39;0038.png&#39;, &#39;0039.png&#39;, &#39;0040.png&#39;, &#39;0041.png&#39;, &#39;0042.png&#39;, &#39;0043.png&#39;, &#39;0044.png&#39;, &#39;0045.png&#39;, &#39;0046.png&#39;, &#39;0047.png&#39;, &#39;0048.png&#39;, &#39;0049.png&#39;, &#39;0050.png&#39;] . from torchvision.datasets import ImageFolder from torchvision.transforms import ToTensor . dataset = ImageFolder(data_dir + &#39;/train&#39;, transform = ToTensor()) . img, label = dataset[0] print(img.shape, label) img[2] # img tensor . torch.Size([3, 32, 32]) 0 . tensor([[0.7804, 0.7804, 0.7882, ..., 0.7843, 0.7804, 0.7765], [0.7961, 0.7961, 0.8000, ..., 0.8039, 0.7961, 0.7882], [0.8118, 0.8157, 0.8235, ..., 0.8235, 0.8157, 0.8078], ..., [0.8706, 0.8392, 0.7765, ..., 0.9686, 0.9686, 0.9686], [0.8745, 0.8667, 0.8627, ..., 0.9608, 0.9608, 0.9608], [0.8667, 0.8627, 0.8667, ..., 0.9529, 0.9529, 0.9529]]) . print(dataset.classes) . [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;] . import matplotlib import matplotlib.pyplot as plt %matplotlib inline matplotlib.rcParams[&#39;figure.facecolor&#39;] = &#39;#ffffff&#39; . def show_example(img, label): print(&#39;Label: &#39;, dataset.classes[label], &quot;(&quot;+str(label)+&quot;)&quot;) plt.imshow(img.permute(1, 2, 0)) # matplot lib expects channels in final dimension . img, label = dataset[5] show_example(img, label) . Label: airplane (0) . show_example(*dataset[0]) . Label: airplane (0) . show_example(*dataset[1099]) . Label: airplane (0) . Training and Validation Datasets . random_seed = 42 torch.manual_seed(random_seed); # It helps to standardise validation set . val_size = 5000 train_size = len(dataset) - val_size train_ds, val_ds = random_split(dataset, [train_size, val_size]) len(train_ds), len(val_ds) . (45000, 5000) . Creating DataLoaders . from torch.utils.data.dataloader import DataLoader batch_size = 128 . train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory = True) val_dl = DataLoader(val_ds, batch_size*2, num_workers = 4, pin_memory = True) . from torchvision.utils import make_grid def show_batch(dl): for images, labels in dl: fig, ax = plt.subplots(figsize = (12, 6)) ax.set_xticks([]); ax.set_yticks([]) ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0)) break . show_batch(train_dl) . Defining the Conv Model . nn.Linear gives full connected layer architecture nn.Conv2d gives convolutional neural network . Basic working of CNN can be described as follows : . . Working of kernel can be describes as : . . Implementation of a convolution operation on a 1 channel image with a 3x3 kernel . def apply_kernel(image, kernel): ri, ci = image.shape #image dimension rk, ck = kernel.shape #kernel dimension ro, co = ri-rk+1, ci-ck+1 #output dimension output = torch.zeros([ro, co]) for i in range(ro): for j in range(co): output[i, j] = torch.sum(image[i:i+rk, j:j+ck] * kernel) return output . sample_image = torch.tensor([ [0, 0, 75, 80, 80], [0, 75, 80, 80, 80], [0, 75, 80, 80, 80], [0, 70, 75, 80, 80], [0, 0, 0, 0, 0] ], dtype = torch.float32) sample_kernel = torch.tensor([ [-1, -2, -1], [0, 0, 0], [1, 2, 1] ], dtype = torch.float32) apply_kernel(sample_image, sample_kernel) . tensor([[ 155., 85., 5.], [ -15., -15., -5.], [-230., -315., -320.]]) . For Multiple channels kernel will have same function over all channels . Refer this blog post for more in-depth intution of CNN :- Convolution in depth . Observation : 5x5 image got reduced to a 3x3 output, while kernel in running over internal values multiple times, but still values of corner are covered only once, so we&#39;ll use padding and it will return output as the same size as input . Padding can be understood using following diagram: . Now we have moved kernel by 1 position each time, we can move kernel by 2 positons too, this is call Stride . Stride can be understood using folloeing diagram: . For multi-channel images, a different kernel is applied to each channels, and the outputs are added together pixel-wise. . There are certain advantages offered by convolutional layers when working with image data: . Fewer parameters: A small set of parameters (the kernel) is used to calculate outputs of the entire image, so the model has much fewer parameters compared to a fully connected layer. | Sparsity of connections: In each layer, each output element only depends on a small number of input elements, which makes the forward and backward passes more efficient. | Parameter sharing and spatial invariance: The features learned by a kernel in one part of the image can be used to detect similar pattern in a different part of another image. | . We will also use a max-pooling layers to progressively decrease the height &amp; width of the output tensors from each convolutional layer. . . Applying Single Convolutional Layer followed by max pooling . import torch.nn as nn import torch.nn.functional as F . conv = nn.Conv2d(3, 8, kernel_size = 3, stride = 1, padding = 1) # 8 is no of kernels which also descide no of output channels ie feature map . pool = nn.MaxPool2d(2, 2) . for images, labels in train_dl: print(&#39;images.shape:&#39;, images.shape) out = conv(images) print(&#39;output.shape:&#39;, out.shape) out = pool(out) print(&#39;max-pool output.shape:&#39;, out.shape) # max pool will reduce size break . images.shape: torch.Size([128, 3, 32, 32]) output.shape: torch.Size([128, 8, 32, 32]) max-pool output.shape: torch.Size([128, 8, 16, 16]) . conv.weight.shape # we have 8 kernel and each kernel contain 3 matrices for 3 input channel and each of 3 matrix have 3x3 matrix that is gonna slide . torch.Size([8, 3, 3, 3]) . conv.weight[0, 0] . tensor([[-0.1398, 0.1772, 0.0425], [-0.0123, 0.1491, -0.0260], [ 0.0454, 0.1039, 0.0843]], grad_fn=&lt;SelectBackward&gt;) . conv.weight[0] . tensor([[[-0.1398, 0.1772, 0.0425], [-0.0123, 0.1491, -0.0260], [ 0.0454, 0.1039, 0.0843]], [[-0.1612, -0.0831, -0.1083], [ 0.1363, -0.0058, -0.0148], [ 0.0635, 0.0091, 0.0112]], [[-0.0657, -0.0093, 0.1648], [ 0.1584, -0.0931, 0.0160], [ 0.1131, -0.1040, 0.0193]]], grad_fn=&lt;SelectBackward&gt;) . simple_model = nn.Sequential( nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1), nn.MaxPool2d(2,2) ) . for images, labels, in train_dl: print(&#39;images.shape:&#39;, images.shape) out = simple_model(images) print(&#39;out.shape:&#39;, out.shape) break . images.shape: torch.Size([128, 3, 32, 32]) out.shape: torch.Size([128, 8, 16, 16]) . Now lets define CNN model . class ImageClassificationBase(nn.Module): def training_step(self, batch): images, labels = batch out = self(images) loss = F.cross_entropy(out, labels) return loss def validation_step(self, batch): images, labels = batch out = self(images) loss = F.cross_entropy(out, labels) acc = accuracy(out, labels) return {&#39;val_loss&#39;: loss.detach(), &#39;val_acc&#39;: acc} def validation_epoch_end(self, outputs): batch_losses = [x[&#39;val_loss&#39;] for x in outputs] epoch_loss = torch.stack(batch_losses).mean() batch_accs = [x[&#39;val_acc&#39;] for x in outputs] epoch_acc = torch.stack(batch_accs).mean() return {&#39;val_loss&#39;: epoch_loss.item(), &#39;val_acc&#39;: epoch_acc.item()} def epoch_end(self, epoch, result): print(&quot;Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}&quot;.format(epoch, result[&#39;train_loss&#39;], result[&#39;val_loss&#39;], result[&#39;val_acc&#39;])) def accuracy(outputs, labels): _, preds = torch.max(outputs, dim=1) return torch.tensor(torch.sum(preds == labels).item() / len(preds)) . class Cifar10CnnModel(ImageClassificationBase): def __init__(self): super().__init__() self.network = nn.Sequential( # input: 3 x 32 x 32 nn.Conv2d(3, 32, kernel_size=3, padding=1), # i/p 3 channels, applies 32 kernels to create o/p: 32 x 32 # output: 32 x 32 x 32 nn.ReLU(), nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # output: 64 x 32 x 32 nn.ReLU(), nn.MaxPool2d(2, 2), # 32 x 32 --&gt; 16 x 16 #output 64 x 16 x 16 nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2), # output 128 x 8 x 8 nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2), # output 256 x 4 x 4 nn.Flatten(), # take 256x4x4 o/p feature map and flatten it out into a vector nn.Linear(256*4*4, 1024), nn.ReLU(), nn.Linear(1024, 512), nn.ReLU(), nn.Linear(512, 10)) def forward(self, xb): return self.network(xb) . model = Cifar10CnnModel() model . Cifar10CnnModel( (network): Sequential( (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU() (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU() (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU() (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU() (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU() (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (15): Flatten(start_dim=1, end_dim=-1) (16): Linear(in_features=4096, out_features=1024, bias=True) (17): ReLU() (18): Linear(in_features=1024, out_features=512, bias=True) (19): ReLU() (20): Linear(in_features=512, out_features=10, bias=True) ) ) . Look at out model : . for images, labels in train_dl: print(&#39;images.shape:&#39;, images.shape) out = model(images) print(&#39;out.shape:&#39;, out.shape) print(&#39;out[0]&#39;, out[0]) # out will have prob of each classes break . images.shape: torch.Size([128, 3, 32, 32]) out.shape: torch.Size([128, 10]) out[0] tensor([ 0.0391, -0.0104, 0.0005, 0.0281, -0.0311, -0.0069, 0.0147, -0.0170, 0.0353, 0.0030], grad_fn=&lt;SelectBackward&gt;) . Training our model using GPU . def get_default_device(): if torch.cuda.is_available(): return torch.device(&#39;cuda&#39;) else: return torch.device(&#39;cpu&#39;) . device = get_default_device() device . device(type=&#39;cuda&#39;) . def to_device(data, device): &quot;&quot;&quot;Move tensors to chosen device&quot;&quot;&quot; if isinstance(data, (list,tuple)): return [to_device(x, device) for x in data] return data.to(device, non_blocking = True) # to method . class DeviceDataLoader(): def __init__(self, dl, device): self.dl = dl self.device = device def __iter__(self): for b in self.dl: yield to_device(b, self.device) def __len__(self): return len(self.dl) . train_dl = DeviceDataLoader(train_dl, device) val_dl = DeviceDataLoader(val_dl, device) to_device(model, device); . @torch.no_grad() # tells when evaluate is being executed we dont want to compute any gradient def evaluate(model, val_loader): model.eval() # tells pytorch that these layers should be put into validation mode outputs = [model.validation_step(batch) for batch in val_loader] return model.validation_epoch_end(outputs) def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD): history = [] optimizer = opt_func(model.parameters(), lr) for epoch in range(epochs): # Training Phase model.train() # tells pytorch that these layers should be put into training mode train_losses = [] for batch in train_loader: loss = model.training_step(batch) train_losses.append(loss) loss.backward() optimizer.step() optimizer.zero_grad() # Validation Phase result = evaluate(model, val_loader) result[&#39;train_loss&#39;] = torch.stack(train_losses).mean().item() model.epoch_end(epoch, result) history.append(result) return history . model = to_device(Cifar10CnnModel(), device) . evaluate(model, val_dl) # with initial set of parameters --&gt; random result . {&#39;val_loss&#39;: 2.3025383949279785, &#39;val_acc&#39;: 0.10006892681121826} . num_epochs = 10 opt_func = torch.optim.Adam lr = 0.001 . history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func) . Epoch [0], train_loss: 1.7970, val_loss: 1.5241, val_acc: 0.4321 Epoch [1], train_loss: 1.2961, val_loss: 1.1054, val_acc: 0.5995 Epoch [2], train_loss: 1.0374, val_loss: 0.9509, val_acc: 0.6613 Epoch [3], train_loss: 0.8626, val_loss: 0.9137, val_acc: 0.6765 Epoch [4], train_loss: 0.7339, val_loss: 0.7842, val_acc: 0.7312 Epoch [5], train_loss: 0.6354, val_loss: 0.7274, val_acc: 0.7429 Epoch [6], train_loss: 0.5439, val_loss: 0.7468, val_acc: 0.7462 Epoch [7], train_loss: 0.4626, val_loss: 0.7059, val_acc: 0.7701 Epoch [8], train_loss: 0.3961, val_loss: 0.7297, val_acc: 0.7634 Epoch [9], train_loss: 0.3301, val_loss: 0.7733, val_acc: 0.7660 . def plot_accuracies(history): accuracies = [x[&#39;val_acc&#39;] for x in history] plt.plot(accuracies, &#39;-x&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;accuracy&#39;) plt.title(&#39;Accuracy vs. No. of epochs&#39;); . plot_accuracies(history) . def plot_losses(history): train_losses = [x.get(&#39;train_loss&#39;) for x in history] val_losses = [x[&#39;val_loss&#39;] for x in history] plt.plot(train_losses, &#39;-x&#39;) plt.plot(val_losses, &#39;-rx&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.legend([&#39;Training&#39;, &#39;Validation&#39;]) plt.title(&#39;Loss vs No of epochs&#39;) . plot_losses(history) . Training error goes down and val error getting up after some time : overfitting . example: . Testing with Individual Images . test_dataset = ImageFolder(data_dir+&#39;/test&#39;, transform=ToTensor()) . def predict_image(img, model): xb = to_device(img.unsqueeze(0), device) # convert to batch of 1 yb = model(xb) # get predictions from model _, preds = torch.max(yb, dim=1) # pick max probab return dataset.classes[preds[0].item()] # retrive label . img, label = test_dataset[1] plt.imshow(img.permute(1, 2, 0)) print(&#39;Label:&#39;, dataset.classes[label], &#39;, Predicted:&#39;, predict_image(img, model)) . Label: airplane , Predicted: airplane . img, label = test_dataset[1002] plt.imshow(img.permute(1, 2, 0)) print(&#39;Label:&#39;, dataset.classes[label], &#39;, Predicted:&#39;, predict_image(img, model)) . Label: automobile , Predicted: automobile . img, label = test_dataset[0] plt.imshow(img.permute(1, 2, 0)) print(&#39;Label:&#39;, dataset.classes[label], &#39;, Predicted:&#39;, predict_image(img, model)) . Label: airplane , Predicted: automobile . img, label = test_dataset[6153] plt.imshow(img.permute(1, 2, 0)) print(&#39;Label:&#39;, dataset.classes[label], &#39;, Predicted:&#39;, predict_image(img, model)) . Label: frog , Predicted: frog . test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device) result = evaluate(model, test_loader) result . {&#39;val_loss&#39;: 0.7635641694068909, &#39;val_acc&#39;: 0.765625} . torch.save(model.state_dict(), &#39;cifar10-cnn.pth&#39;) .",
            "url": "https://mr-siddy.github.io/ML-blog/deep_learning/2021/06/09/Image-Classification-CNN-pytorch.html",
            "relUrl": "/deep_learning/2021/06/09/Image-Classification-CNN-pytorch.html",
            "date": " • Jun 9, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Training Deep Neural Networks on a GPU",
            "content": "Importing Libraries . import torch import torchvision import numpy as np import matplotlib import matplotlib.pyplot as plt import torch.nn as nn import torch.nn.functional as F from torchvision.datasets import MNIST from torchvision.transforms import ToTensor from torchvision.utils import make_grid from torch.utils.data.dataloader import DataLoader from torch.utils.data import random_split %matplotlib inline matplotlib.rcParams[&#39;figure.facecolor&#39;] = &#39;#ffffff&#39; . dataset = MNIST(root=&#39;data/&#39;, download=False, transform=ToTensor()) . image, label = dataset[0] image.permute(1, 2, 0).shape . torch.Size([28, 28, 1]) . image, label = dataset[0] print(&#39;image.shape:&#39;, image.shape) plt.imshow(image.permute(1, 2, 0), cmap=&#39;gray&#39;) # plt.imshow expects channels to be last dimension in an image tensor, so we use permute to reorder print(&#39;label:&#39;, label) . image.shape: torch.Size([1, 28, 28]) label: 5 . len(dataset) . 60000 . dataset[0] . image, label = dataset[143] print(&#39;image.shape:&#39;, image.shape) plt.imshow(image.permute(1, 2, 0), cmap=&#39;gray&#39;) # plt.imshow expects channels to be last dimension in an image tensor, so we use permute to reorder print(&#39;label:&#39;, label) . image.shape: torch.Size([1, 28, 28]) label: 2 . validation-set using random_split . val_size = 10000 train_size = len(dataset) - val_size train_ds, val_ds = random_split(dataset, [train_size, val_size]) len(train_ds), len(val_ds) . (50000, 10000) . PyTorch data loaders . batch_size = 128 . train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True) val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True) . num_workers attribute tells the data loader instance how many sub-processes to use for data loading. By default, the num_workers value is set to zero, and a value of zero tells the loader to load the data inside the main process. . pin_memory (bool, optional) – If True, the data loader will copy tensors into CUDA pinned memory before returning them. . for images, _ in train_loader: print(&#39;images.shape&#39;, images.shape) print(&#39;grid.shape&#39;, make_grid(images, nrow=16).shape) break . images.shape torch.Size([128, 1, 28, 28]) grid.shape torch.Size([3, 242, 482]) . for images, _ in train_loader: print(&#39;image.shape:&#39;, image.shape) plt.figure(figsize=(16,8)) plt.axis(&#39;off&#39;) plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0))) break . image.shape: torch.Size([1, 28, 28]) . Creating NN . Hidden Layers, Activation function and Non-Linearity . for images, labels in train_loader: print(&#39;images.shape&#39;, images.shape) inputs = images.reshape(-1, 784) print(&#39;inputs.shape&#39;, inputs.shape) break . images.shape torch.Size([128, 1, 28, 28]) inputs.shape torch.Size([128, 784]) . input_size = inputs.shape[-1] #size of output from hidden layer is 32, can be inc or dec to change the learning capacity of model hidden_size = 32 . layer1 = nn.Linear(input_size, hidden_size ) # it will convert 784 to 32 . inputs.shape . torch.Size([128, 784]) . layer1_outputs = layer1(inputs) print(&#39;layer1_outputs&#39;, layer1_outputs.shape) . layer1_outputs torch.Size([128, 32]) . layer1_outputs_direct = inputs @ layer1.weight.t() + layer1.bias layer1_outputs_direct.shape . torch.Size([128, 32]) . torch.allclose(layer1_outputs, layer1_outputs_direct, 1e-3) . True . Thus, layer1_outputs and inputs have a linear relationship, i.e., each element of layer_outputs is a weighted sum of elements from inputs. Thus, even as we train the model and modify the weights, layer1 can only capture linear relationships between inputs and outputs. . . Next, we&#39;ll use the Rectified Linear Unit (ReLU) function as the activation function for the outputs. It has the formula relu(x) = max(0,x) i.e. it simply replaces negative values in a given tensor with the value 0. ReLU is a non-linear function . We can use the F.relu method to apply ReLU to the elements of a tensor. . . . F.relu(torch.tensor([[1, -1, 0], [-0.1, .2, 3]])) . tensor([[1.0000, 0.0000, 0.0000], [0.0000, 0.2000, 3.0000]]) . layer1_outputs.shape . torch.Size([128, 32]) . relu_outputs = F.relu(layer1_outputs) print(&#39;relu_outputs.shape:&#39;, relu_outputs.shape) print(&#39;min(layer1_outputs):&#39;, torch.min(layer1_outputs).item()) print(&#39;min(relu_outputs):&#39;, torch.min(relu_outputs).item()) . relu_outputs.shape: torch.Size([128, 32]) min(layer1_outputs): -0.6917587518692017 min(relu_outputs): 0.0 . output_size = 10 layer2 = nn.Linear(hidden_size, output_size) . layer2_outputs = layer2(relu_outputs) print(&#39;relu_outputs.shape:&#39;, relu_outputs.shape) print(&#39;layer2_outputs.shape:&#39;, layer2_outputs.shape) . relu_outputs.shape: torch.Size([128, 32]) layer2_outputs.shape: torch.Size([128, 10]) . inputs.shape . torch.Size([128, 784]) . F.cross_entropy(layer2_outputs, labels) . tensor(2.2991, grad_fn=&lt;NllLossBackward&gt;) . outputs = (F.relu(inputs @ layer1.weight.t() +layer1.bias)) @ layer2.weight.t() + layer2.bias . torch.allclose(outputs, layer2_outputs, 1e-3) . True . if we hadn&#39;t included a non-linear activation between the two linear layers, the final relationship b/w inputs and outputs would be Linear . outputs2 = (inputs @ layer1.weight.t() + layer1.bias) @ layer2.weight.t() + layer2.bias . combined_layer = nn.Linear(input_size, output_size) combined_layer.weight.data = layer2.weight @ layer1.weight combined_layer.bias.data = layer1.bias @ layer2.weight.t() + layer2.bias . outputs3 = inputs @ combined_layer.weight.t() + combined_layer.bias . torch.allclose(outputs2, outputs3, 1e-3) . False . Model . We are now ready to define our model. As discussed above, we&#39;ll create a neural network with one hidden layer. Here&#39;s what that means: . Instead of using a single nn.Linear object to transform a batch of inputs (pixel intensities) into outputs (class probabilities), we&#39;ll use two nn.Linear objects. Each of these is called a layer in the network. . | The first layer (also known as the hidden layer) will transform the input matrix of shape batch_size x 784 into an intermediate output matrix of shape batch_size x hidden_size. The parameter hidden_size can be configured manually (e.g., 32 or 64). . | We&#39;ll then apply a non-linear activation function to the intermediate outputs. The activation function transforms individual elements of the matrix. . | The result of the activation function, which is also of size batch_size x hidden_size, is passed into the second layer (also known as the output layer). The second layer transforms it into a matrix of size batch_size x 10. We can use this output to compute the loss and adjust weights using gradient descent. . | . As discussed above, our model will contain one hidden layer. Here&#39;s what it looks like visually: . . Let&#39;s define the model by extending the nn.Module class from PyTorch. . class MnistModel(nn.Module): def __init__(self, in_size, hidden_size, out_size): super().__init__() self.Linear1 = nn.Linear(in_size, hidden_size) # hidden layer self.Linear2 = nn.Linear(hidden_size, out_size) # output layer def forward(self, xb): xb = xb.view(xb.size(0),-1) # flatten image tensor out = self.Linear1(xb) # intermediate outputs using hidden layer out = F.relu(out) # applying activation function out = self.Linear2(out) # predictions using o/p layer return out def training_step(self, batch): images, labels = batch out = self(images) loss = F.cross_entropy(out, labels) return loss def validation_step(self, batch): images, labels = batch out = self(images) loss = F.cross_entropy(out, labels) acc = accuracy(out, labels) return {&#39;val_loss&#39;: loss, &#39;val_acc&#39;: acc} def validation_epoch_end(self, outputs): batch_loss = [x[&#39;val_loss&#39;] for x in outputs] epoch_loss = torch.stack(batch_loss).mean() # Combine losses batch_accs = [x[&#39;val_acc&#39;] for x in outputs] epoch_acc = torch.stack(batch_accs).mean() # combine accuracies return {&#39;val_loss&#39;: epoch_loss.item(), &#39;val_acc&#39;: epoch_acc.item()} def epoch_end(self, epoch, result): print(&quot;Epoch [{}], val_loss: {:4f}, val_acc: {:4f}&quot;.format(epoch, result[&#39;val_loss&#39;], result[&#39;val_acc&#39;])) . def accuracy(outputs, labels): _, preds = torch.max(outputs, dim=1) return torch.tensor(torch.sum(preds == labels).item() / len(preds)) . input_size = 784 hidden_size = 32 num_classes = 10 . model = MnistModel(input_size, hidden_size, out_size = num_classes) . for t in model.parameters(): # weights and bias for linear layer and hidden layer print(t.shape) . torch.Size([32, 784]) torch.Size([32]) torch.Size([10, 32]) torch.Size([10]) . for images, labels in train_loader: outputs = model(images) break loss = F.cross_entropy(outputs, labels) print(&#39;Loss:&#39;, loss.item()) print(&#39;outputs.shape: &#39;, outputs.shape) print(&#39;Sample outputs :&#39;, outputs[:2].data) . Loss: 2.3032402992248535 outputs.shape: torch.Size([128, 10]) Sample outputs : tensor([[ 0.0308, 0.1783, -0.0268, -0.1578, -0.1123, -0.0013, 0.1285, -0.0645, -0.0745, -0.0817], [-0.0028, 0.2039, -0.0196, -0.1190, -0.1853, 0.0436, 0.0258, 0.0813, -0.1292, -0.0431]]) . Training the Model using GPU . torch.cuda.is_available() . True . def get_default_device(): if torch.cuda.is_available(): return torch.device(&#39;cuda&#39;) else: return torch.device(&#39;cpu&#39;) . device = get_default_device() device . device(type=&#39;cuda&#39;) . def to_device(data, device): &quot;&quot;&quot;MOve tensors to chosen device&quot;&quot;&quot; if isinstance(data, (list,tuple)): return [to_device(x, device) for x in data] return data.to(device, non_blocking = True) # to method . for images, labels in train_loader: print(images.shape) print(images.device) images = to_device(images, device) print(images.device) break . torch.Size([128, 1, 28, 28]) cpu cuda:0 . DeviceDataLeader class to wrap our existing data loaders and move batches of data to the selected device, iter method to retrieve batches of data and an len to get number of batches . class DeviceDataLoader(): # wrap a dataloader to move data to device def __init__(self, dl , device): self.dl = dl self.device = device # yield a batch of data after moving it to device def __iter__(self): for b in self.dl: yield to_device(b, self.device) # number of batches def __len__(self): return len(self.dl) . # example def some_numbers(): yield 10 yield 20 yield 30 for value in some_numbers(): print(value) . 10 20 30 . train_loader = DeviceDataLoader(train_loader, device) val_loader = DeviceDataLoader(val_loader, device) . for xb, yb in val_loader: print(&#39;xb.device:&#39;, xb.device) print(&#39;yb:&#39;, yb) break . xb.device: cuda:0 yb: tensor([1, 4, 3, 7, 8, 2, 2, 2, 0, 1, 3, 7, 1, 2, 7, 6, 5, 2, 7, 6, 4, 6, 3, 0, 3, 7, 2, 1, 5, 1, 5, 0, 6, 0, 7, 1, 9, 3, 5, 6, 6, 6, 3, 1, 2, 7, 0, 1, 7, 4, 3, 9, 7, 2, 1, 4, 3, 4, 8, 3, 1, 0, 9, 6, 4, 0, 2, 5, 2, 6, 7, 4, 4, 6, 7, 3, 4, 0, 0, 2, 5, 5, 5, 5, 0, 9, 4, 3, 9, 6, 0, 4, 8, 6, 2, 8, 1, 7, 8, 2, 4, 4, 6, 6, 7, 0, 7, 4, 0, 1, 1, 9, 4, 0, 9, 2, 4, 2, 8, 6, 7, 1, 5, 3, 7, 8, 4, 2, 6, 8, 1, 7, 3, 8, 4, 4, 7, 9, 7, 2, 9, 6, 8, 7, 9, 4, 3, 5, 1, 9, 8, 9, 1, 3, 9, 6, 9, 9, 9, 9, 7, 4, 3, 3, 1, 2, 7, 8, 5, 8, 0, 8, 3, 1, 6, 3, 1, 0, 6, 6, 1, 5, 4, 7, 9, 4, 5, 4, 2, 3, 3, 2, 9, 6, 6, 3, 8, 4, 4, 2, 1, 7, 7, 3, 4, 5, 8, 2, 9, 9, 6, 8, 7, 6, 0, 6, 0, 0, 1, 1, 1, 4, 0, 9, 5, 2, 0, 3, 0, 0, 0, 4, 7, 9, 6, 0, 3, 4, 5, 6, 0, 0, 9, 7, 8, 6, 3, 0, 7, 8, 7, 7, 4, 0, 8, 0], device=&#39;cuda:0&#39;) . Training part . def evaluate(model, val_loader): outputs = [model.validation_step(batch) for batch in val_loader] return model.validation_epoch_end(outputs) def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD): history = [] optimizer = opt_func(model.parameters(), lr) for epoch in range(epochs): #training phase for batch in train_loader: loss = model.training_step(batch) loss.backward() optimizer.step() optimizer.zero_grad() #validation phase result = evaluate(model, val_loader) model.epoch_end(epoch, result) history.append(result) return history . model = MnistModel(input_size, hidden_size= hidden_size, out_size=num_classes) to_device(model, device) . MnistModel( (Linear1): Linear(in_features=784, out_features=32, bias=True) (Linear2): Linear(in_features=32, out_features=10, bias=True) ) . history = [evaluate(model, val_loader)] history . [{&#39;val_loss&#39;: 2.309469223022461, &#39;val_acc&#39;: 0.12490234524011612}] . history += fit(5, 0.5, model, train_loader, val_loader) . Epoch [0], val_loss: 0.202114, val_acc: 0.941016 Epoch [1], val_loss: 0.156000, val_acc: 0.953223 Epoch [2], val_loss: 0.156326, val_acc: 0.953516 Epoch [3], val_loss: 0.127720, val_acc: 0.962598 Epoch [4], val_loss: 0.119708, val_acc: 0.962793 . try with more less lr . history += fit(5, 0.1, model, train_loader, val_loader) . Epoch [0], val_loss: 0.109468, val_acc: 0.967578 Epoch [1], val_loss: 0.105522, val_acc: 0.968164 Epoch [2], val_loss: 0.105320, val_acc: 0.969824 Epoch [3], val_loss: 0.104688, val_acc: 0.969629 Epoch [4], val_loss: 0.104357, val_acc: 0.968555 . Lmao acc ~ 96% . losses = [x[&#39;val_loss&#39;] for x in history] plt.plot(losses, &#39;-x&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.title(&#39;Loss v/s Epochs&#39;) . Text(0.5, 1.0, &#39;Loss v/s Epochs&#39;) . accuracies = [x[&#39;val_acc&#39;] for x in history] plt.plot(accuracies, &#39;-x&#39;) plt.xlabel(&#39;epochs&#39;) plt.ylabel(&#39;accuracies&#39;) plt.title(&#39;Accuracies v/s Epochs&#39;) . Text(0.5, 1.0, &#39;Accuracies v/s Epochs&#39;) . Testing with Individual Images . test_dataset = MNIST(root=&#39;data/&#39;, train=False, transform=ToTensor()) . def predict_image(img, model): xb = to_device(img.unsqueeze(0), device) print(xb.device) yb = model(xb) _, preds = torch.max(yb, dim=1) return preds[0].item() . img, label = test_dataset[0] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;Label:&#39;, label, &#39;Prediction:&#39;, predict_image(img, model)) . cuda:0 Label: 7 Prediction: 7 . img, label = test_dataset[123] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;Label:&#39;, label, &#39;Prediction:&#39;, predict_image(img, model)) . cuda:0 Label: 6 Prediction: 6 . img, label = test_dataset[183] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;Label:&#39;, label, &#39;Prediction:&#39;, predict_image(img, model)) . cuda:0 Label: 0 Prediction: 0 . test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size=256), device) result = evaluate(model, test_loader) result . {&#39;val_loss&#39;: 0.10134970396757126, &#39;val_acc&#39;: 0.9697265625} . torch.save(model.state_dict(), &#39;mnist-feedforward.pth&#39;) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/06/03/NN-on-GPU.html",
            "relUrl": "/2021/06/03/NN-on-GPU.html",
            "date": " • Jun 3, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Logistic Regression on MNIST Database using Pytorch",
            "content": "import torch import torchvision from torchvision.datasets import MNIST . Working with Image . We&#39;ll use the famous MNIST Handwritten Digits Database as our training dataset. It consists of 28px by 28px grayscale images of handwritten digits (0 to 9) and labels for each image indicating which digit it represents. Here are some sample images from the dataset: . . Data Preprocessing . dataset = MNIST(root=&#39;data/&#39;, download=True) . len(dataset) . 60000 . test_dataset = MNIST(root=&#39;data/&#39;, train=False) len(test_dataset) . 10000 . dataset[0] # it gives :- image(part of pillow), 5 (label digit) . (&lt;PIL.Image.Image image mode=L size=28x28 at 0x25EA9AE4748&gt;, 5) . import matplotlib.pyplot as plt #indicates to jupyter we want to plot graphs within the notebook, without this it will show in popup %matplotlib inline . image, label = dataset[0] plt.imshow(image, cmap=&#39;gray&#39;) print(&#39;Label:&#39;, label) . Label: 5 . image, label = dataset[10] plt.imshow(image, cmap=&#39;gray&#39;) print(&#39;Label:&#39;, label) . Label: 3 . Pytorch dosen&#39;t know how to work with images, we need to convert images to tensors by specifying transform while creating our dataset . import torchvision.transforms as transform . dataset = MNIST(root=&#39;data/&#39;, train=True, transform=transform.ToTensor()) . img_tensor, label = dataset[0] print(img_tensor.shape,label) . torch.Size([1, 28, 28]) 5 . img_tensor . tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706, 0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922, 0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137, 0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000, 0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275, 0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922, 0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294, 0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098, 0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922, 0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922, 0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706, 0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922, 0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922, 0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]) . print(img_tensor[:,10:15,10:15]) print(torch.max(img_tensor),torch.min(img_tensor)) . tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000], [0.0000, 0.5451, 0.9922, 0.7451, 0.0078], [0.0000, 0.0431, 0.7451, 0.9922, 0.2745], [0.0000, 0.0000, 0.1373, 0.9451, 0.8824], [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]]) tensor(1.) tensor(0.) . plt.imshow(img_tensor[0,10:15,10:15], cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x25eadd15c08&gt; . Training and Validation Datasets . Training set - used to train the model, i.e., compute the loss and adjust the model&#39;s weights using gradient descent. | Validation set - used to evaluate the model during training, adjust hyperparameters (learning rate, etc.), and pick the best version of the model. | Test set - used to compare different models or approaches and report the model&#39;s final accuracy. | In the MNIST dataset, there are 60,000 training images and 10,000 test images. The test set is standardized so that different researchers can report their models&#39; results against the same collection of images. . Since there&#39;s no predefined validation set, we must manually split the 60,000 images into training and validation datasets. Let&#39;s set aside 10,000 randomly chosen images for validation. We can do this using the random_spilt method from PyTorch. . # using random_split from torch.utils.data import random_split train_ds, val_ds = random_split(dataset, [50000, 10000]) len(train_ds), len(val_ds) . (50000, 10000) . from torch.utils.data import DataLoader batch_size = 128 train_loader = DataLoader(train_ds, batch_size, shuffle=True) # shuffle=True for the training data loader to ensure batches generated in each epoch are different # randomization helps generalize and speed=up the training process val_loader = DataLoader(val_ds, batch_size ) . Model . A logistic regression model is almost identical to a linear regression model. It contains weights and bias matrices, and the output is obtained using simple matrix operations (pred = x @ w.t() + b). . | As we did with linear regression, we can use nn.Linear to create the model instead of manually creating and initializing the matrices. . | Since nn.Linear expects each training example to be a vector, each 1x28x28 image tensor is flattened into a vector of size 784 (28*28) before being passed into the model. . | The output for each image is a vector of size 10, with each element signifying the probability of a particular target label (i.e., 0 to 9). The predicted label for an image is simply the one with the highest probability. . | . import torch.nn as nn input_size = 28*28 num_classes = 10 #Logistic regression model model = nn.Linear(input_size, num_classes) . print(model.weight.shape) model.weight . torch.Size([10, 784]) . Parameter containing: tensor([[ 0.0097, -0.0244, -0.0155, ..., -0.0295, 0.0300, -0.0210], [ 0.0036, 0.0262, -0.0189, ..., 0.0118, 0.0162, -0.0324], [ 0.0041, -0.0117, -0.0011, ..., 0.0128, 0.0156, -0.0292], ..., [-0.0163, -0.0195, -0.0283, ..., -0.0236, 0.0138, 0.0091], [ 0.0336, -0.0344, 0.0232, ..., -0.0277, -0.0007, -0.0115], [ 0.0130, 0.0136, 0.0206, ..., 0.0128, -0.0171, -0.0017]], requires_grad=True) . print(model.bias.shape) model.bias . torch.Size([10]) . Parameter containing: tensor([ 0.0144, -0.0100, 0.0354, -0.0225, -0.0262, 0.0286, 0.0230, -0.0162, -0.0203, 0.0171], requires_grad=True) . so total parameters = 7850 . for images, labels in train_loader: print(labels) print(images.shape) outputs = model(images) print(outputs) break . tensor([7, 2, 1, 5, 4, 8, 6, 0, 0, 5, 3, 7, 9, 0, 8, 4, 5, 2, 5, 9, 3, 0, 6, 6, 4, 9, 2, 1, 5, 9, 3, 3, 4, 1, 0, 4, 2, 0, 0, 5, 6, 8, 9, 7, 6, 0, 4, 1, 2, 1, 9, 2, 6, 8, 5, 2, 8, 7, 7, 9, 7, 6, 7, 5, 9, 5, 5, 9, 1, 7, 5, 0, 7, 7, 2, 5, 0, 8, 1, 1, 7, 8, 7, 4, 7, 3, 1, 6, 4, 9, 9, 3, 9, 3, 5, 6, 6, 5, 6, 4, 2, 4, 5, 1, 8, 7, 0, 3, 9, 9, 5, 2, 7, 6, 9, 8, 5, 1, 3, 2, 3, 6, 4, 1, 3, 9, 7, 3]) torch.Size([128, 1, 28, 28]) . RuntimeError Traceback (most recent call last) &lt;ipython-input-21-170d6f073b97&gt; in &lt;module&gt; 4 print(labels) 5 print(images.shape) -&gt; 6 outputs = model(images) 7 print(outputs) 8 break ~ anaconda3 envs torch lib site-packages torch nn modules module.py in _call_impl(self, *input, **kwargs) 887 result = self._slow_forward(*input, **kwargs) 888 else: --&gt; 889 result = self.forward(*input, **kwargs) 890 for hook in itertools.chain( 891 _global_forward_hooks.values(), ~ anaconda3 envs torch lib site-packages torch nn modules linear.py in forward(self, input) 92 93 def forward(self, input: Tensor) -&gt; Tensor: &gt; 94 return F.linear(input, self.weight, self.bias) 95 96 def extra_repr(self) -&gt; str: ~ anaconda3 envs torch lib site-packages torch nn functional.py in linear(input, weight, bias) 1751 if has_torch_function_variadic(input, weight): 1752 return handle_torch_function(linear, (input, weight), input, weight, bias=bias) -&gt; 1753 return torch._C._nn.linear(input, weight, bias) 1754 1755 RuntimeError: mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10) . error occurred bcoz, input data dosen&#39;t have right shape. images are of the shape 1x28x28, but we need them to be vectors of size 784 ie we need to flatten them --&gt; using .reshape . images.shape . torch.Size([128, 1, 28, 28]) . images.reshape(128,784).shape . torch.Size([128, 784]) . To include this additional function within out model, we need to define a custom model by extending the nn.Module class . Inside the __init__ constructor method, we instantiate the weights and biases using nn.Linear. And inside the forward method, which is invoked when we pass a batch of inputs to the model, we flatten the input tensor and pass it into self.linear. . xb.reshape(-1, 28*28) indicates to PyTorch that we want a view of the xb tensor with two dimensions. The length along the 2nd dimension is 28*28 (i.e., 784). One argument to .reshape can be set to -1 (in this case, the first dimension) to let PyTorch figure it out automatically based on the shape of the original tensor. . Note that the model no longer has .weight and .bias attributes (as they are now inside the .linear attribute), but it does have a .parameters method that returns a list containing the weights and bias. . class MnistModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(input_size, num_classes) def forward(self, xb): xb = xb.reshape(-1, 784) out = self.linear(xb) return out model = MnistModel() #object . model.linear . Linear(in_features=784, out_features=10, bias=True) . print(model.linear.weight.shape, model.linear.bias.shape) list(model.parameters()) # bundle all weights and biases . torch.Size([10, 784]) torch.Size([10]) . [Parameter containing: tensor([[ 3.4617e-02, 2.8410e-02, 2.5686e-02, ..., -1.5863e-02, -3.8656e-03, 1.5305e-02], [-1.8018e-02, 1.7917e-02, -3.0232e-02, ..., -5.3763e-03, -3.2701e-02, 1.7136e-02], [-3.4262e-02, 8.5667e-03, -2.0011e-02, ..., -1.8257e-02, 3.4308e-02, 1.8136e-02], ..., [ 9.5943e-03, 1.6986e-02, 2.2343e-02, ..., 4.5605e-03, 9.5017e-03, -2.5854e-04], [-1.4468e-02, -2.7529e-02, 2.9830e-02, ..., -9.3204e-03, 6.1549e-03, -2.5384e-02], [ 3.3574e-02, 2.1621e-03, -8.8337e-03, ..., -4.9651e-05, -2.1324e-02, 2.4071e-03]], requires_grad=True), Parameter containing: tensor([ 0.0054, -0.0311, 0.0240, 0.0014, -0.0334, 0.0090, -0.0151, -0.0068, 0.0042, -0.0279], requires_grad=True)] . for images, labels in train_loader: outputs = model(images) break print(&#39;output.shape :&#39;, outputs.shape) print(&#39;Sample outputs layer 0 :&#39;, outputs[0].data) print(&#39;Sample outputs layer 0 and 1 :&#39;, outputs[:2].data) . output.shape : torch.Size([128, 10]) Sample outputs layer 0 : tensor([-0.1530, -0.4023, 0.0156, -0.0506, 0.0551, -0.2542, 0.0123, -0.3361, -0.1795, 0.1995]) Sample outputs layer 0 and 1 : tensor([[-0.1530, -0.4023, 0.0156, -0.0506, 0.0551, -0.2542, 0.0123, -0.3361, -0.1795, 0.1995], [-0.2933, -0.3964, 0.2764, -0.0050, -0.0035, -0.0671, 0.2888, -0.2759, -0.1555, 0.0238]]) . For each of the 100 input images, we get 10 outputs, one for each class. As discussed earlier, we&#39;d like these outputs to represent probabilities. Each output row&#39;s elements must lie between 0 to 1 and add up to 1, which is not the case. . To convert the output rows into probabilities, we use the softmax function, which has the following formula: . . First, we replace each element yi in an output row by e^yi, making all the elements positive. . . Then, we divide them by their sum to ensure that they add up to 1. The resulting vector can thus be interpreted as probabilities. . we&#39;ll use the implementation that&#39;s provided within PyTorch because it works well with multidimensional tensors (a list of output rows in our case). . import torch.nn.functional as F . outputs[0:2] . tensor([[-0.1530, -0.4023, 0.0156, -0.0506, 0.0551, -0.2542, 0.0123, -0.3361, -0.1795, 0.1995], [-0.2933, -0.3964, 0.2764, -0.0050, -0.0035, -0.0671, 0.2888, -0.2759, -0.1555, 0.0238]], grad_fn=&lt;SliceBackward&gt;) . probs = F.softmax(outputs, dim=1) # apply softmax for each output row # see why dim=0 dosen&#39;t workout print(&quot;sample probabilities: n&quot;, probs[:2].data) #sample probs print(&quot;Sum: &quot;, torch.sum(probs[0]).item()) # addup probs of an output row . sample probabilities: tensor([[0.0942, 0.0734, 0.1115, 0.1044, 0.1160, 0.0851, 0.1111, 0.0784, 0.0917, 0.1340], [0.0774, 0.0698, 0.1368, 0.1032, 0.1034, 0.0970, 0.1385, 0.0787, 0.0888, 0.1063]]) Sum: 1.0 . max_probs, preds = torch.max(probs, dim=1) print(preds) print(max_probs) . tensor([9, 6, 2, 9, 2, 2, 9, 5, 3, 9, 3, 8, 6, 9, 2, 9, 9, 8, 9, 9, 9, 3, 6, 3, 6, 3, 5, 3, 2, 2, 3, 9, 9, 8, 2, 2, 9, 6, 6, 4, 2, 0, 9, 9, 2, 2, 9, 9, 2, 9, 9, 9, 9, 9, 9, 9, 6, 9, 3, 3, 6, 9, 9, 3, 2, 3, 9, 8, 4, 8, 8, 4, 9, 2, 5, 2, 9, 2, 8, 2, 2, 6, 3, 4, 9, 9, 9, 9, 6, 6, 9, 6, 9, 3, 9, 9, 0, 6, 3, 2, 3, 5, 5, 3, 8, 3, 8, 2, 4, 3, 0, 9, 9, 6, 6, 3, 5, 2, 3, 9, 3, 2, 7, 9, 9, 2, 9, 9]) tensor([0.1340, 0.1385, 0.1331, 0.1312, 0.1378, 0.1389, 0.1765, 0.1426, 0.1205, 0.1485, 0.1351, 0.1217, 0.1359, 0.1315, 0.1463, 0.1313, 0.1251, 0.1250, 0.1213, 0.1374, 0.1270, 0.1285, 0.1210, 0.1202, 0.1537, 0.1311, 0.1201, 0.1155, 0.1454, 0.1945, 0.1293, 0.1238, 0.1146, 0.1202, 0.1568, 0.1285, 0.1396, 0.1146, 0.1350, 0.1179, 0.1579, 0.1335, 0.1622, 0.1176, 0.1568, 0.1461, 0.1368, 0.1345, 0.1403, 0.1409, 0.1426, 0.1331, 0.1502, 0.1324, 0.1265, 0.1440, 0.1352, 0.1367, 0.1186, 0.1284, 0.1301, 0.1446, 0.1604, 0.1328, 0.1556, 0.1515, 0.1254, 0.1221, 0.1191, 0.1351, 0.1306, 0.1152, 0.1264, 0.1515, 0.1167, 0.1381, 0.1342, 0.1423, 0.1233, 0.1406, 0.1184, 0.1247, 0.1140, 0.1443, 0.1542, 0.1410, 0.1169, 0.1249, 0.1407, 0.1257, 0.1382, 0.1283, 0.1415, 0.1145, 0.1345, 0.1229, 0.1267, 0.1291, 0.1408, 0.1413, 0.1231, 0.1289, 0.1252, 0.1473, 0.1149, 0.1233, 0.1328, 0.1257, 0.1239, 0.1239, 0.1235, 0.1495, 0.1488, 0.1119, 0.1191, 0.1328, 0.1396, 0.1598, 0.1483, 0.1271, 0.1297, 0.1417, 0.1131, 0.1655, 0.1422, 0.1257, 0.1339, 0.1229], grad_fn=&lt;MaxBackward0&gt;) . labels . tensor([9, 8, 5, 3, 3, 3, 2, 1, 0, 7, 2, 5, 3, 4, 8, 7, 4, 1, 9, 4, 9, 4, 4, 7, 0, 9, 1, 7, 0, 0, 8, 5, 1, 8, 0, 4, 7, 8, 2, 5, 2, 1, 4, 6, 0, 8, 6, 9, 5, 4, 9, 4, 9, 3, 4, 6, 3, 8, 1, 2, 2, 4, 9, 9, 5, 6, 9, 1, 8, 5, 8, 2, 9, 0, 3, 0, 9, 7, 1, 1, 7, 8, 9, 7, 9, 4, 4, 6, 6, 3, 7, 5, 9, 8, 1, 9, 9, 0, 2, 0, 0, 5, 1, 3, 9, 8, 1, 6, 9, 4, 6, 2, 5, 6, 8, 2, 1, 0, 2, 5, 7, 6, 7, 7, 0, 1, 4, 6]) . Evaluation Metric and Loss Function . outputs[:2] . tensor([[-0.1530, -0.4023, 0.0156, -0.0506, 0.0551, -0.2542, 0.0123, -0.3361, -0.1795, 0.1995], [-0.2933, -0.3964, 0.2764, -0.0050, -0.0035, -0.0671, 0.2888, -0.2759, -0.1555, 0.0238]], grad_fn=&lt;SliceBackward&gt;) . preds == labels . tensor([ True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, True, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, True, False, True, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, True, False, False, True, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False]) . torch.sum(preds == labels) # this is telling how many out of 128 were get predicted correctly . tensor(21) . Accuracy of predictions . def accuracy(outputs, labels): _, preds = torch.max(outputs, dim=1) return torch.tensor(torch.sum(preds == labels).item() / len(preds)) . accuracy(outputs,labels) . tensor(0.1641) . The == operator performs an element-wise comparison of two tensors with the same shape and returns a tensor of the same shape, containing True for unequal elements and False for equal elements. Passing the result to torch.sum returns the number of labels that were predicted correctly. Finally, we divide by the total number of images to get the accuracy. . Note that we don&#39;t need to apply softmax to the outputs since its results have the same relative order. This is because e^x is an increasing function, i.e., if y1 &gt; y2, then e^y1 &gt; e^y2. The same holds after averaging out the values to get the softmax. . Let&#39;s calculate the accuracy of the current model on the first batch of data. . Accuracy is an excellent way for us (humans) to evaluate the model. However, we can&#39;t use it as a loss function for optimizing our model using gradient descent for the following reasons: . It&#39;s not a differentiable function. torch.max and == are both non-continuous and non-differentiable operations, so we can&#39;t use the accuracy for computing gradients w.r.t the weights and biases. . | It doesn&#39;t take into account the actual probabilities predicted by the model, so it can&#39;t provide sufficient feedback for incremental improvements. . | For these reasons, accuracy is often used as an evaluation metric for classification, but not as a loss function. A commonly used loss function for classification problems is the cross-entropy, which has the following formula: . . While it looks complicated, it&#39;s actually quite simple: . For each output row, pick the predicted probability for the correct label. E.g., if the predicted probabilities for an image are [0.1, 0.3, 0.2, ...] and the correct label is 1, we pick the corresponding element 0.3 and ignore the rest. . | Then, take the logarithm of the picked probability. If the probability is high, i.e., close to 1, then its logarithm is a very small negative value, close to 0. And if the probability is low (close to 0), then the logarithm is a very large negative value. We also multiply the result by -1, which results is a large postive value of the loss for poor predictions. . | . . Finally, take the average of the cross entropy across all the output rows to get the overall loss for a batch of data. | . Unlike accuracy, cross-entropy is a continuous and differentiable function. It also provides useful feedback for incremental improvements in the model (a slightly higher probability for the correct label leads to a lower loss). These two factors make cross-entropy a better choice for the loss function. . As you might expect, PyTorch provides an efficient and tensor-friendly implementation of cross-entropy as part of the torch.nn.functional package. Moreover, it also performs softmax internally, so we can directly pass in the model&#39;s outputs without converting them into probabilities. . probs . tensor([[0.0942, 0.0734, 0.1115, ..., 0.0784, 0.0917, 0.1340], [0.0774, 0.0698, 0.1368, ..., 0.0787, 0.0888, 0.1063], [0.1088, 0.0800, 0.1331, ..., 0.0638, 0.0798, 0.1175], ..., [0.1022, 0.0864, 0.1257, ..., 0.0881, 0.1148, 0.1158], [0.0974, 0.0822, 0.0975, ..., 0.0675, 0.0971, 0.1339], [0.0905, 0.1213, 0.1075, ..., 0.0760, 0.1040, 0.1229]], grad_fn=&lt;SoftmaxBackward&gt;) . loss_fn = F.cross_entropy . loss = loss_fn(outputs, labels) . print(loss) . tensor(2.3150, grad_fn=&lt;NllLossBackward&gt;) . Training the model . def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD): optimizer = opt_func(model.parameters(), lr) history = [] # for reading epoch-wise results for epoch in range(epochs): #training phase for batch in train_loader: loss = model.training_step(batch) loss.backward() optimizer.step() # change the gradients using the learning rate optimizer.zero_grad() #validation phase result = evaluate(model, val_loader) model.epoch_end(epoch, result) history.append(result) return history . def evaluate(model, val_loader): outputs = [model.validation_step(batch) for batch in val_loader] #list comprehension return model.validation_epoch_end(outputs) . Gaps to fill-in are : training_step, validation_step, validation_epoch_end, epoch_end used by : fit, evaluate . class MnistModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(input_size, num_classes) def forward(self, xb): xb = xb.reshape(-1, 784) out = self.linear(xb) return out def training_step(self, batch): images, labels = batch out = self(images) # Generate predictions loss = F.cross_entropy(out, labels) # Calculate loss return loss def validation_step(self, batch): images, labels = batch out = self(images) # Generate predictions loss = F.cross_entropy(out, labels) #loss acc = accuracy(out, labels) # clac accuracy return {&#39;val_loss&#39;: loss, &#39;val_acc&#39;: acc} def validation_epoch_end(self, outputs): batch_losses = [x[&#39;val_loss&#39;] for x in outputs] epoch_loss = torch.stack(batch_losses).mean() #combine losses batch_accs = [x[&#39;val_acc&#39;] for x in outputs] epoch_accs = torch.stack(batch_accs).mean() #combine accuracies return {&#39;val_loss&#39;: epoch_loss.item(), &#39;val_acc&#39;: epoch_accs.item()} def epoch_end(self, epoch, result): print(&quot;Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}&quot;.format(epoch, result[&#39;val_loss&#39;], result[&#39;val_acc&#39;])) model = MnistModel() . result0 = evaluate(model, val_loader) result0 . {&#39;val_loss&#39;: 2.283280611038208, &#39;val_acc&#39;: 0.10749604552984238} . Train the model . history1 = fit(5, 0.001, model, train_loader, val_loader) . Epoch [0], val_loss: 1.9208, val_acc: 0.6665 Epoch [1], val_loss: 1.6565, val_acc: 0.7396 Epoch [2], val_loss: 1.4599, val_acc: 0.7663 Epoch [3], val_loss: 1.3116, val_acc: 0.7836 Epoch [4], val_loss: 1.1978, val_acc: 0.7975 . history2 = fit(5, 0.001, model, train_loader, val_loader) . Epoch [0], val_loss: 1.1083, val_acc: 0.8057 Epoch [1], val_loss: 1.0365, val_acc: 0.8137 Epoch [2], val_loss: 0.9777, val_acc: 0.8194 Epoch [3], val_loss: 0.9287, val_acc: 0.8240 Epoch [4], val_loss: 0.8873, val_acc: 0.8287 . history3 = fit(5, 0.001, model, train_loader, val_loader) . Epoch [0], val_loss: 0.8517, val_acc: 0.8326 Epoch [1], val_loss: 0.8210, val_acc: 0.8360 Epoch [2], val_loss: 0.7939, val_acc: 0.8392 Epoch [3], val_loss: 0.7700, val_acc: 0.8419 Epoch [4], val_loss: 0.7487, val_acc: 0.8445 . history4 = fit(5, 0.001, model, train_loader, val_loader) . Epoch [0], val_loss: 0.7295, val_acc: 0.8467 Epoch [1], val_loss: 0.7123, val_acc: 0.8480 Epoch [2], val_loss: 0.6966, val_acc: 0.8495 Epoch [3], val_loss: 0.6822, val_acc: 0.8513 Epoch [4], val_loss: 0.6691, val_acc: 0.8527 . Visualization . history = [result0] + history1 + history2 + history3 + history4 accuracies = [result[&#39;val_acc&#39;] for result in history] plt.plot(accuracies, &#39;-x&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;accuracy&#39;) plt.title(&#39;Accuarcy vs No of epochs&#39;); . Testing on Individual Images . from torchvision import transforms . test_dataset = MNIST(root=&#39;data/&#39;, train = False, transform = transforms.ToTensor()) . img, label = test_dataset[0] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;shape&#39;, img.shape) print(&#39;label&#39;,label) . shape torch.Size([1, 28, 28]) label 7 . def predict_image(img, model): xb = img.unsqueeze(0) yb = model(xb) _, preds = torch.max(yb, dim=1) return preds[0].item() . img.unsqueeze simply adds another dimension at the begining of the 1x28x28 tensor, making it a 1x1x28x28 tensor, which the model views as a batch containing a single image. . img, label = test_dataset[0] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;label:&#39;,label, &#39;, predicted:&#39;,predict_image(img, model)) . label: 7 , predicted: 7 . img, label = test_dataset[1] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;label:&#39;,label, &#39;, predicted:&#39;,predict_image(img, model)) . label: 2 , predicted: 2 . img, label = test_dataset[5] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;label:&#39;,label, &#39;, predicted:&#39;,predict_image(img, model)) . label: 1 , predicted: 1 . img, label = test_dataset[193] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;label:&#39;,label, &#39;, predicted:&#39;,predict_image(img, model)) . label: 9 , predicted: 9 . img, label = test_dataset[1839] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;label:&#39;,label, &#39;, predicted:&#39;,predict_image(img, model)) # Here Model in Breaking Up . label: 2 , predicted: 8 . test_loader = DataLoader(test_dataset, batch_size=256) result = evaluate(model, test_loader) result . {&#39;val_loss&#39;: 0.6401068568229675, &#39;val_acc&#39;: 0.8603515625} . Saving and Loading the Model . torch.save(model.state_dict(), &#39;mnist-logistic.pth&#39;) # .state_dict returns Ordered Dict containig all the weights and bias matrices mapped to right attributes of the model . model.state_dict() . OrderedDict([(&#39;linear.weight&#39;, tensor([[ 0.0064, -0.0261, 0.0092, ..., 0.0320, 0.0143, 0.0129], [-0.0179, 0.0100, 0.0158, ..., 0.0125, -0.0167, 0.0209], [ 0.0081, 0.0290, -0.0355, ..., 0.0334, -0.0284, -0.0074], ..., [-0.0097, -0.0077, 0.0133, ..., 0.0153, -0.0036, 0.0283], [ 0.0289, -0.0290, -0.0317, ..., -0.0190, 0.0308, -0.0353], [-0.0043, 0.0184, -0.0096, ..., 0.0319, -0.0038, -0.0067]])), (&#39;linear.bias&#39;, tensor([-0.0544, 0.1269, 0.0047, -0.0240, 0.0016, 0.0284, -0.0175, 0.0452, -0.0609, -0.0155]))]) . model_2 = MnistModel() model_2.load_state_dict(torch.load(&#39;mnist-logistic.pth&#39;)) model_2.state_dict() . OrderedDict([(&#39;linear.weight&#39;, tensor([[ 0.0064, -0.0261, 0.0092, ..., 0.0320, 0.0143, 0.0129], [-0.0179, 0.0100, 0.0158, ..., 0.0125, -0.0167, 0.0209], [ 0.0081, 0.0290, -0.0355, ..., 0.0334, -0.0284, -0.0074], ..., [-0.0097, -0.0077, 0.0133, ..., 0.0153, -0.0036, 0.0283], [ 0.0289, -0.0290, -0.0317, ..., -0.0190, 0.0308, -0.0353], [-0.0043, 0.0184, -0.0096, ..., 0.0319, -0.0038, -0.0067]])), (&#39;linear.bias&#39;, tensor([-0.0544, 0.1269, 0.0047, -0.0240, 0.0016, 0.0284, -0.0175, 0.0452, -0.0609, -0.0155]))]) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/06/01/Logistic-MNIST-Pytorch.html",
            "relUrl": "/2021/06/01/Logistic-MNIST-Pytorch.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Introduction to Pytorch",
            "content": "import torch . Tensors . Tenson is a number, vector, matrix, or any N- Dimensional array. . t1 = torch.tensor(4.) t1 . tensor(4.) . t1.dtype . torch.float32 . t2 = torch.tensor([1., 2, 3, 4]) print(t2) # ALl the tensor elements are of same data type # Matrix - 2D tensor t3 = torch.tensor([[5, 6], [7,8], [9,10]]) print(t3) . tensor([1., 2., 3., 4.]) tensor([[ 5, 6], [ 7, 8], [ 9, 10]]) . # most of the time we will use floating point numbers t4 = torch.tensor([ [[11,12,13], [13,14,15]], [[15,16,17], [17,18,19.]] ]) t4 . tensor([[[11., 12., 13.], [13., 14., 15.]], [[15., 16., 17.], [17., 18., 19.]]]) . Tensors can have any number of dimensions and different lengths along each dimension. we can inspect length using .shape property . print(t1) t1.shape . tensor(4.) . torch.Size([]) . print(t2) t2.shape . tensor([1., 2., 3., 4.]) . torch.Size([4]) . print(t3) t3.shape . tensor([[ 5, 6], [ 7, 8], [ 9, 10]]) . torch.Size([3, 2]) . print(t4) t4.shape # start from outer most bracket and count number of elements in that ie. here 2 elements both matrices than we go 1 bracket in and there is also 2 list elements and so on ..... . tensor([[[11., 12., 13.], [13., 14., 15.]], [[15., 16., 17.], [17., 18., 19.]]]) . torch.Size([2, 2, 3]) . we can not make a tensor with an improper shape . Tensor Operations and Gradients . x = torch.tensor(3.) w = torch.tensor(4., requires_grad=True) # b = torch.tensor(5., requires_grad=True) x,w,b . (tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True)) . y = w * x + b y . tensor(17., grad_fn=&lt;AddBackward0&gt;) . y.backward() # derivatives of y w.r.t each of input tensors are stored in .grad property of respective tensors . print(&#39;dy/dx&#39;, x.grad) print(&#39;dy/dw&#39;, w.grad) print(&#39;dy/db&#39;, b.grad) . dy/dx None dy/dw tensor(3.) dy/db tensor(1.) . we have not specified requires_grad=True in x, this tells pytorch that we are not intrested in drivatives of any future output w.r.t x but we are intrested for w and b .... so requires_grad property is important to save millions of usless coputations of derrivatives as per requirement . &quot;grad&quot; in w.grad is short for gradient, which is another term for derivative primarily used while dealing with vectors and matrices . Tensor Functions . t6 = torch.full((3,2), 42) t6 . tensor([[42, 42], [42, 42], [42, 42]]) . t7 = torch.cat((t3,t6)) t7 . tensor([[ 5, 6], [ 7, 8], [ 9, 10], [42, 42], [42, 42], [42, 42]]) . t8 = torch.sin(t7) t8 . tensor([[-0.9589, -0.2794], [ 0.6570, 0.9894], [ 0.4121, -0.5440], [-0.9165, -0.9165], [-0.9165, -0.9165], [-0.9165, -0.9165]]) . t9 = t8.reshape(3,2,2) t9 . tensor([[[-0.9589, -0.2794], [ 0.6570, 0.9894]], [[ 0.4121, -0.5440], [-0.9165, -0.9165]], [[-0.9165, -0.9165], [-0.9165, -0.9165]]]) . Inter-operability with Numpy . import numpy as np . x = np.array([[1,2],[3, 4.]]) x . array([[1., 2.], [3., 4.]]) . y = torch.from_numpy(x) y . tensor([[1., 2.], [3., 4.]], dtype=torch.float64) . x.dtype, y.dtype . (dtype(&#39;float64&#39;), torch.float64) . z = y.numpy() z . array([[1., 2.], [3., 4.]]) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/05/29/PyTorch-Basics.html",
            "relUrl": "/2021/05/29/PyTorch-Basics.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Implementation of Linear Regression and Gradient Descent using Pytorch",
            "content": "Linear regression. model that predicts crop yields for apples and oranges (target variables) by looking at the average temperature, rainfall, and humidity (input variables or features) in a region. Here&#39;s the training data: . . In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias : . yield_apple = w11 * temp + w12 * rainfall + w13 * humidity + b1 yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2 . Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity: . . import torch import numpy as np . Training Data . inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70]], dtype=&#39;float32&#39;) . targets = np.array([[56,70], [81, 101], [119, 133], [22, 37], [103, 119]], dtype=&#39;float32&#39;) . inputs = torch.from_numpy(inputs) targets = torch.from_numpy(targets) print(inputs) print(targets) . tensor([[ 73., 67., 43.], [ 91., 88., 64.], [ 87., 134., 58.], [102., 43., 37.], [ 69., 96., 70.]]) tensor([[ 56., 70.], [ 81., 101.], [119., 133.], [ 22., 37.], [103., 119.]]) . Linear Regression Model from Scratch . w = torch.randn(2, 3, requires_grad=True) # torch.randn : creates a tensor with givent shape with random elements picked with normal distribution b = torch.randn(2, requires_grad= True) print(w) print(b) . tensor([[ 0.0728, -2.0486, 0.2053], [ 1.4556, -1.4721, -1.4280]], requires_grad=True) tensor([-2.6483, -2.7893], requires_grad=True) . Our model is just X * W_transpose + Bias . def model(x): return x @ w.t() + b # @-&gt; matrix multiplication in pytorch, .t() returns the transpose of a tensor . inputs @ w.t() + b . tensor([[-125.7638, -56.5679], [-163.1629, -91.2699], [-258.9209, -156.2401], [ -75.7193, 29.5415], [-179.9207, -143.6369]], grad_fn=&lt;AddBackward0&gt;) . preds = model(inputs) preds . tensor([[-125.7638, -56.5679], [-163.1629, -91.2699], [-258.9209, -156.2401], [ -75.7193, 29.5415], [-179.9207, -143.6369]], grad_fn=&lt;AddBackward0&gt;) . print(targets) . tensor([[ 56., 70.], [ 81., 101.], [119., 133.], [ 22., 37.], [103., 119.]]) . diff = preds - targets # diff * diff # * means element wise multiplication not matrix multiplication torch.sum(diff*diff) / diff.numel() # numel -&gt; number of element in diff matrix . tensor(53075.1758, grad_fn=&lt;DivBackward0&gt;) . Loss function . MSE Loss :- On average, each element in prediction differs from the actual target by the square root of the loss . def mse(t1,t2): diff = t1 - t2 return torch.sum(diff * diff) / diff.numel() . loss = mse(preds, targets) print(loss) . tensor(53075.1758, grad_fn=&lt;DivBackward0&gt;) . loss.backward() . print(w) print(w.grad) # derivative of the loss w.r.t element in w . tensor([[ 0.0728, -2.0486, 0.2053], [ 1.4556, -1.4721, -1.4280]], requires_grad=True) tensor([[-19571.1211, -23133.6465, -13756.3496], [-14156.5244, -17938.3672, -10636.8340]]) . print(b) print(b.grad) . tensor([-2.6483, -2.7893], requires_grad=True) tensor([-236.8975, -175.6347]) . Grad of loss w.r.t each element in tensor indicates the rate of change of loss or slope of the loss function . we can substract from each weight element a small quantity proportional to the derivative of the loss w.r.t that element to reduce the loss slightly . print(w) w.grad . tensor([[ 0.0728, -2.0486, 0.2053], [ 1.4556, -1.4721, -1.4280]], requires_grad=True) . tensor([[-19571.1211, -23133.6465, -13756.3496], [-14156.5244, -17938.3672, -10636.8340]]) . print(w) w.grad * 1e-5 # new weights to near w . tensor([[ 0.0728, -2.0486, 0.2053], [ 1.4556, -1.4721, -1.4280]], requires_grad=True) . tensor([[-0.1957, -0.2313, -0.1376], [-0.1416, -0.1794, -0.1064]]) . with torch.no_grad(): w -= w.grad * 1e-5 # 1e-5 is the step ie small coz loss is large.....Learning Rate b -= b.grad * 1e-5 . torch.no_grad() to indicate to Pytorch that we shouldn&#39;t take track, calculate, or modify gradients while updating the weights and biases . w, b . (tensor([[ 0.2685, -1.8173, 0.3429], [ 1.5971, -1.2927, -1.3216]], requires_grad=True), tensor([-2.6459, -2.7876], requires_grad=True)) . preds = model(inputs) loss = mse(preds, targets) print(loss) . tensor(37202.4609, grad_fn=&lt;DivBackward0&gt;) . Now reset the gradients to 0 . w.grad.zero_() b.grad.zero_() print(w.grad) print(b.grad) . tensor([[0., 0., 0.], [0., 0., 0.]]) tensor([0., 0.]) . Train the Model using Gradient descent . preds = model(inputs) print(preds) . tensor([[ -90.0597, -29.6393], [-116.1892, -55.7924], [-202.9139, -113.7154], [ -40.7170, 55.6320], [-134.5765, -109.2006]], grad_fn=&lt;AddBackward0&gt;) . loss = mse(preds, targets) print(loss) . tensor(37202.4609, grad_fn=&lt;DivBackward0&gt;) . loss.backward() print(w.grad) print(b.grad) . tensor([[-15880.6006, -19155.8594, -11304.5137], [-11370.2803, -14927.9023, -8782.6719]]) tensor([-193.0913, -142.5432]) . update the weights and biases using gradientdescent . with torch.no_grad(): w -= w.grad * 1e-5 b -= b.grad * 1e-5 w.grad.zero_() b.grad.zero_() . print(w) print(b) . tensor([[ 0.4273, -1.6257, 0.4559], [ 1.7108, -1.1434, -1.2338]], requires_grad=True) tensor([-2.6440, -2.7862], requires_grad=True) . preds = model(inputs) loss = mse(preds, targets) print(loss) . tensor(26488.4434, grad_fn=&lt;DivBackward0&gt;) . Train on multiple Epochs . for i in range(100): preds = model(inputs) loss = mse(preds, targets) loss.backward() with torch.no_grad(): w -= w.grad * 1e-5 b -= b.grad * 1e-5 w.grad.zero_() b.grad.zero_() . preds = model(inputs) loss = mse(preds, targets) print(loss) . tensor(1333.2324, grad_fn=&lt;DivBackward0&gt;) . print(preds) print(targets) . tensor([[ 65.7250, 83.3404], [ 92.7282, 99.8337], [ 80.9948, 113.9022], [ 73.8770, 114.4924], [ 88.8938, 71.9206]], grad_fn=&lt;AddBackward0&gt;) tensor([[ 56., 70.], [ 81., 101.], [119., 133.], [ 22., 37.], [103., 119.]]) . Linear Regression using Pytorch built-ins . import torch.nn as nn . inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70], [74, 66, 43], [91, 87, 65], [88, 134, 59], [101, 44, 37], [68, 96, 71], [73, 66, 44], [92, 87, 64], [87, 135, 57], [103, 43 ,36], [68, 97, 70]], dtype=&#39;float32&#39;) targets = np.array([[56,70], [81, 101], [119, 133], [22, 37], [103, 119], [57,69], [80,102], [118, 132], [21, 38], [104, 118], [57, 69], [82, 100], [118, 134], [20, 38], [102, 120]], dtype=&#39;float32&#39;) inputs = torch.from_numpy(inputs) targets = torch.from_numpy(targets) . print(inputs) print(targets) . tensor([[ 73., 67., 43.], [ 91., 88., 64.], [ 87., 134., 58.], [102., 43., 37.], [ 69., 96., 70.], [ 74., 66., 43.], [ 91., 87., 65.], [ 88., 134., 59.], [101., 44., 37.], [ 68., 96., 71.], [ 73., 66., 44.], [ 92., 87., 64.], [ 87., 135., 57.], [103., 43., 36.], [ 68., 97., 70.]]) tensor([[ 56., 70.], [ 81., 101.], [119., 133.], [ 22., 37.], [103., 119.], [ 57., 69.], [ 80., 102.], [118., 132.], [ 21., 38.], [104., 118.], [ 57., 69.], [ 82., 100.], [118., 134.], [ 20., 38.], [102., 120.]]) . Dataset and DataLoader . creating a TensorDataset, which allows access to rows from inputs and targets as tuples and provide standard APIs for working many different types pf datasets in Pytorch . from torch.utils.data import TensorDataset . train_ds = TensorDataset(inputs, targets) train_ds[0:3] # 0 to 3-1 . (tensor([[ 73., 67., 43.], [ 91., 88., 64.], [ 87., 134., 58.]]), tensor([[ 56., 70.], [ 81., 101.], [119., 133.]])) . from torch.utils.data import DataLoader . batch_size = 5 train_dl = DataLoader(train_ds, batch_size, shuffle=True) . inputs . tensor([[ 73., 67., 43.], [ 91., 88., 64.], [ 87., 134., 58.], [102., 43., 37.], [ 69., 96., 70.], [ 74., 66., 43.], [ 91., 87., 65.], [ 88., 134., 59.], [101., 44., 37.], [ 68., 96., 71.], [ 73., 66., 44.], [ 92., 87., 64.], [ 87., 135., 57.], [103., 43., 36.], [ 68., 97., 70.]]) . for xb, yb in train_dl: print(xb) print(yb) break . tensor([[102., 43., 37.], [ 91., 87., 65.], [ 69., 96., 70.], [ 88., 134., 59.], [ 74., 66., 43.]]) tensor([[ 22., 37.], [ 80., 102.], [103., 119.], [118., 132.], [ 57., 69.]]) . nn.Linear . Instead of initialising the weights and biases manually, we can define the model using the nn.Linear . model = nn.Linear(3, 2) print(model.weight) print(model.bias) . Parameter containing: tensor([[-0.1637, 0.0519, -0.1459], [-0.2050, 0.2159, -0.0023]], requires_grad=True) Parameter containing: tensor([-0.1157, -0.1562], requires_grad=True) . list(model.parameters()) . [Parameter containing: tensor([[-0.1637, 0.0519, -0.1459], [-0.2050, 0.2159, -0.0023]], requires_grad=True), Parameter containing: tensor([-0.1157, -0.1562], requires_grad=True)] . preds = model(inputs) . preds . tensor([[-14.8669, -0.7540], [-19.7889, 0.0420], [-15.8733, 10.8090], [-19.9838, -11.8685], [-16.6477, 6.2663], [-15.0825, -1.1750], [-19.9867, -0.1762], [-16.1830, 10.6017], [-19.7683, -11.4475], [-16.6298, 6.4690], [-15.0646, -0.9722], [-20.0045, -0.3789], [-15.6756, 11.0272], [-20.0016, -12.0712], [-16.4321, 6.6873]], grad_fn=&lt;AddmmBackward&gt;) . Loss Function . import torch.nn.functional as F . loss_fn = F.mse_loss . loss = loss_fn(model(inputs), targets) print(loss) . tensor(9453.6309, grad_fn=&lt;MseLossBackward&gt;) . Optimizer . we will use stochastic gradient descent -&gt; optim.SGD . opt = torch.optim.SGD(model.parameters(), lr=1e-5) #lr is the learning rate . Train the Model . def fit(num_epochs, model, loss_fn, opt, train_dl): for epoch in range(num_epochs): for xb, xy in train_dl: pred = model(xb) # Generate Predictions loss = loss_fn(pred, yb) # calculate loss loss.backward() # compute gradient opt.step() # update parameters using gradient opt.zero_grad() # reset the gradient to zero if (epoch+1) % 10 == 0: print(&#39;Epoch [{}/{}], Loss: {:.4f}&#39;.format(epoch+1, num_epochs, loss.item())) . fit(100, model, loss_fn, opt, train_dl) . Epoch [10/100], Loss: 1473.9495 Epoch [20/100], Loss: 1101.0323 Epoch [30/100], Loss: 1247.5220 Epoch [40/100], Loss: 1066.2527 Epoch [50/100], Loss: 1192.7886 Epoch [60/100], Loss: 1239.0150 Epoch [70/100], Loss: 916.5994 Epoch [80/100], Loss: 986.2520 Epoch [90/100], Loss: 1190.9945 Epoch [100/100], Loss: 1572.8744 . preds = model(inputs) . preds . tensor([[ 62.2054, 75.3431], [ 81.8397, 99.3478], [ 75.8782, 91.9953], [ 78.3076, 94.4091], [ 70.4413, 85.9155], [ 62.8510, 76.1146], [ 82.2799, 99.9035], [ 76.9230, 93.2708], [ 77.6620, 93.6375], [ 70.2358, 85.6997], [ 62.6455, 75.8988], [ 82.4854, 100.1193], [ 75.4381, 91.4396], [ 78.5131, 94.6249], [ 69.7957, 85.1440]], grad_fn=&lt;AddmmBackward&gt;) . targets . tensor([[ 56., 70.], [ 81., 101.], [119., 133.], [ 22., 37.], [103., 119.], [ 57., 69.], [ 80., 102.], [118., 132.], [ 21., 38.], [104., 118.], [ 57., 69.], [ 82., 100.], [118., 134.], [ 20., 38.], [102., 120.]]) . Random input Batch . model(torch.tensor([[75, 63, 44.]])) # we&#39;ll get a batch of output . tensor([[63.9573, 77.4678]], grad_fn=&lt;AddmmBackward&gt;) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/05/28/Torch-LR-GD.html",
            "relUrl": "/2021/05/28/Torch-LR-GD.html",
            "date": " • May 28, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Decision Trees and Random Forest",
            "content": "import opendatasets as od import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import seaborn as sns import os %matplotlib inline pd.set_option(&#39;display.max_columns&#39;, None) pd.set_option(&#39;display.max_rows&#39;, 150) sns.set_style(&#39;darkgrid&#39;) matplotlib.rcParams[&#39;font.size&#39;] = 14 matplotlib.rcParams[&#39;figure.figsize&#39;] = (10, 6) matplotlib.rcParams[&#39;figure.facecolor&#39;] = &#39;#00000000&#39; . os.listdir(&#39;weather-dataset-rattle-package&#39;) . [&#39;weatherAUS.csv&#39;] . raw_df = pd.read_csv(&#39;weather-dataset-rattle-package/weatherAUS.csv&#39;) . raw_df.head(10) . Date Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am WindDir3pm WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow . 0 2008-12-01 | Albury | 13.4 | 22.9 | 0.6 | NaN | NaN | W | 44.0 | W | WNW | 20.0 | 24.0 | 71.0 | 22.0 | 1007.7 | 1007.1 | 8.0 | NaN | 16.9 | 21.8 | No | No | . 1 2008-12-02 | Albury | 7.4 | 25.1 | 0.0 | NaN | NaN | WNW | 44.0 | NNW | WSW | 4.0 | 22.0 | 44.0 | 25.0 | 1010.6 | 1007.8 | NaN | NaN | 17.2 | 24.3 | No | No | . 2 2008-12-03 | Albury | 12.9 | 25.7 | 0.0 | NaN | NaN | WSW | 46.0 | W | WSW | 19.0 | 26.0 | 38.0 | 30.0 | 1007.6 | 1008.7 | NaN | 2.0 | 21.0 | 23.2 | No | No | . 3 2008-12-04 | Albury | 9.2 | 28.0 | 0.0 | NaN | NaN | NE | 24.0 | SE | E | 11.0 | 9.0 | 45.0 | 16.0 | 1017.6 | 1012.8 | NaN | NaN | 18.1 | 26.5 | No | No | . 4 2008-12-05 | Albury | 17.5 | 32.3 | 1.0 | NaN | NaN | W | 41.0 | ENE | NW | 7.0 | 20.0 | 82.0 | 33.0 | 1010.8 | 1006.0 | 7.0 | 8.0 | 17.8 | 29.7 | No | No | . 5 2008-12-06 | Albury | 14.6 | 29.7 | 0.2 | NaN | NaN | WNW | 56.0 | W | W | 19.0 | 24.0 | 55.0 | 23.0 | 1009.2 | 1005.4 | NaN | NaN | 20.6 | 28.9 | No | No | . 6 2008-12-07 | Albury | 14.3 | 25.0 | 0.0 | NaN | NaN | W | 50.0 | SW | W | 20.0 | 24.0 | 49.0 | 19.0 | 1009.6 | 1008.2 | 1.0 | NaN | 18.1 | 24.6 | No | No | . 7 2008-12-08 | Albury | 7.7 | 26.7 | 0.0 | NaN | NaN | W | 35.0 | SSE | W | 6.0 | 17.0 | 48.0 | 19.0 | 1013.4 | 1010.1 | NaN | NaN | 16.3 | 25.5 | No | No | . 8 2008-12-09 | Albury | 9.7 | 31.9 | 0.0 | NaN | NaN | NNW | 80.0 | SE | NW | 7.0 | 28.0 | 42.0 | 9.0 | 1008.9 | 1003.6 | NaN | NaN | 18.3 | 30.2 | No | Yes | . 9 2008-12-10 | Albury | 13.1 | 30.1 | 1.4 | NaN | NaN | W | 28.0 | S | SSE | 15.0 | 11.0 | 58.0 | 27.0 | 1007.0 | 1005.7 | NaN | NaN | 20.1 | 28.2 | Yes | No | . raw_df.shape . (145460, 23) . raw_df.info() # to check column types of dataset . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 145460 entries, 0 to 145459 Data columns (total 23 columns): # Column Non-Null Count Dtype -- -- 0 Date 145460 non-null object 1 Location 145460 non-null object 2 MinTemp 143975 non-null float64 3 MaxTemp 144199 non-null float64 4 Rainfall 142199 non-null float64 5 Evaporation 82670 non-null float64 6 Sunshine 75625 non-null float64 7 WindGustDir 135134 non-null object 8 WindGustSpeed 135197 non-null float64 9 WindDir9am 134894 non-null object 10 WindDir3pm 141232 non-null object 11 WindSpeed9am 143693 non-null float64 12 WindSpeed3pm 142398 non-null float64 13 Humidity9am 142806 non-null float64 14 Humidity3pm 140953 non-null float64 15 Pressure9am 130395 non-null float64 16 Pressure3pm 130432 non-null float64 17 Cloud9am 89572 non-null float64 18 Cloud3pm 86102 non-null float64 19 Temp9am 143693 non-null float64 20 Temp3pm 141851 non-null float64 21 RainToday 142199 non-null object 22 RainTomorrow 142193 non-null object dtypes: float64(16), object(7) memory usage: 25.5+ MB . raw_df.dropna(subset=[&#39;RainTomorrow&#39;], inplace=True) . raw_df.head(2) . Date Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am WindDir3pm WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow . 0 2008-12-01 | Albury | 13.4 | 22.9 | 0.6 | NaN | NaN | W | 44.0 | W | WNW | 20.0 | 24.0 | 71.0 | 22.0 | 1007.7 | 1007.1 | 8.0 | NaN | 16.9 | 21.8 | No | No | . 1 2008-12-02 | Albury | 7.4 | 25.1 | 0.0 | NaN | NaN | WNW | 44.0 | NNW | WSW | 4.0 | 22.0 | 44.0 | 25.0 | 1010.6 | 1007.8 | NaN | NaN | 17.2 | 24.3 | No | No | . raw_df.shape # shape has become 142193 . (142193, 23) . Training Validation and Test Sets . plt.title(&quot;no.of Rows per Year&quot;) sns.countplot(x=pd.to_datetime(raw_df.Date).dt.year); . year = pd.to_datetime(raw_df.Date).dt.year train_df = raw_df[year&lt;2015] val_df = raw_df[year==2015] test_df = raw_df[year&gt;2015] print(train_df.shape, val_df.shape, test_df.shape) . (98988, 23) (17231, 23) (25974, 23) . Input and Target Columns . input_cols = list(train_df.columns)[1:-1] target_cols = &#39;RainTomorrow&#39; . target_cols . &#39;RainTomorrow&#39; . input_cols . [&#39;Location&#39;, &#39;MinTemp&#39;, &#39;MaxTemp&#39;, &#39;Rainfall&#39;, &#39;Evaporation&#39;, &#39;Sunshine&#39;, &#39;WindGustDir&#39;, &#39;WindGustSpeed&#39;, &#39;WindDir9am&#39;, &#39;WindDir3pm&#39;, &#39;WindSpeed9am&#39;, &#39;WindSpeed3pm&#39;, &#39;Humidity9am&#39;, &#39;Humidity3pm&#39;, &#39;Pressure9am&#39;, &#39;Pressure3pm&#39;, &#39;Cloud9am&#39;, &#39;Cloud3pm&#39;, &#39;Temp9am&#39;, &#39;Temp3pm&#39;, &#39;RainToday&#39;] . train_inputs = train_df[input_cols].copy() train_targets = train_df[target_cols].copy() val_inputs = val_df[input_cols].copy() val_targets = val_df[target_cols].copy() test_inputs = test_df[input_cols].copy() test_targets = test_df[target_cols].copy() . numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist() categorical_cols = train_inputs.select_dtypes(&#39;object&#39;).columns.tolist() . print(numeric_cols) . [&#39;MinTemp&#39;, &#39;MaxTemp&#39;, &#39;Rainfall&#39;, &#39;Evaporation&#39;, &#39;Sunshine&#39;, &#39;WindGustSpeed&#39;, &#39;WindSpeed9am&#39;, &#39;WindSpeed3pm&#39;, &#39;Humidity9am&#39;, &#39;Humidity3pm&#39;, &#39;Pressure9am&#39;, &#39;Pressure3pm&#39;, &#39;Cloud9am&#39;, &#39;Cloud3pm&#39;, &#39;Temp9am&#39;, &#39;Temp3pm&#39;] . print(categorical_cols) . [&#39;Location&#39;, &#39;WindGustDir&#39;, &#39;WindDir9am&#39;, &#39;WindDir3pm&#39;, &#39;RainToday&#39;] . Imputing Missing Numeric Values . train_inputs[numeric_cols].isna().sum().sort_values(ascending=False) . Sunshine 40696 Evaporation 37110 Cloud3pm 36766 Cloud9am 35764 Pressure9am 9345 Pressure3pm 9309 WindGustSpeed 6902 Humidity9am 1265 Humidity3pm 1186 WindSpeed3pm 1140 WindSpeed9am 1133 Rainfall 1000 Temp9am 783 Temp3pm 663 MinTemp 434 MaxTemp 198 dtype: int64 . from sklearn.impute import SimpleImputer . imputer = SimpleImputer(strategy = &#39;mean&#39;).fit(raw_df[numeric_cols]) # imputer will figureout the avg for each of cols . train_inputs[numeric_cols] = imputer.transform(train_inputs[numeric_cols]) # fill empty data val_inputs[numeric_cols] = imputer.transform(val_inputs[numeric_cols]) test_inputs[numeric_cols] = imputer.transform(test_inputs[numeric_cols]) . train_inputs[numeric_cols].isna().sum() . MinTemp 0 MaxTemp 0 Rainfall 0 Evaporation 0 Sunshine 0 WindGustSpeed 0 WindSpeed9am 0 WindSpeed3pm 0 Humidity9am 0 Humidity3pm 0 Pressure9am 0 Pressure3pm 0 Cloud9am 0 Cloud3pm 0 Temp9am 0 Temp3pm 0 dtype: int64 . Scaling Numeric Features . from sklearn.preprocessing import MinMaxScaler . val_inputs.describe().loc[[&#39;min&#39;, &#39;max&#39;]] . MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustSpeed WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm . min -8.2 | -3.2 | 0.0 | 0.0 | 0.0 | 7.0 | 0.0 | 0.0 | 4.0 | 0.0 | 988.1 | 982.2 | 0.0 | 0.0 | -6.2 | -4.0 | . max 31.9 | 45.4 | 247.2 | 70.4 | 14.5 | 135.0 | 87.0 | 74.0 | 100.0 | 100.0 | 1039.3 | 1037.3 | 8.0 | 8.0 | 37.5 | 42.8 | . scaler = MinMaxScaler().fit(raw_df[numeric_cols]) . train_inputs[numeric_cols] = scaler.transform(train_inputs[numeric_cols]) val_inputs[numeric_cols] = scaler.transform(val_inputs[numeric_cols]) test_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols]) . val_inputs.describe().loc[[&#39;min&#39;, &#39;max&#39;]] . MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustSpeed WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm . min 0.007075 | 0.030246 | 0.000000 | 0.000000 | 0.0 | 0.007752 | 0.000000 | 0.000000 | 0.04 | 0.0 | 0.125620 | 0.0816 | 0.000000 | 0.000000 | 0.021097 | 0.026871 | . max 0.952830 | 0.948960 | 0.666307 | 0.485517 | 1.0 | 1.000000 | 0.669231 | 0.850575 | 1.00 | 1.0 | 0.971901 | 0.9632 | 0.888889 | 0.888889 | 0.943038 | 0.925144 | . Encoding Categorical Data . from sklearn.preprocessing import OneHotEncoder . train_df[categorical_cols].fillna(&#39;Unkown&#39;) val_df[categorical_cols].fillna(&#39;Unkown&#39;) test_df[categorical_cols].fillna(&#39;Unknown&#39;) . Location WindGustDir WindDir9am WindDir3pm RainToday . 2498 Albury | ENE | Unknown | ESE | No | . 2499 Albury | SSE | SSE | SE | No | . 2500 Albury | ENE | ESE | ENE | Yes | . 2501 Albury | SSE | SE | SSE | Yes | . 2502 Albury | ENE | SE | SSE | Yes | . ... ... | ... | ... | ... | ... | . 145454 Uluru | E | ESE | E | No | . 145455 Uluru | E | SE | ENE | No | . 145456 Uluru | NNW | SE | N | No | . 145457 Uluru | N | SE | WNW | No | . 145458 Uluru | SE | SSE | N | No | . 25974 rows × 5 columns . encoder = OneHotEncoder(sparse=False, handle_unknown=&#39;ignore&#39;).fit(raw_df[categorical_cols]) . encoded_cols = list(encoder.get_feature_names(categorical_cols)) . train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols]) val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols]) test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols]) . print(encoded_cols) . [&#39;Location_Adelaide&#39;, &#39;Location_Albany&#39;, &#39;Location_Albury&#39;, &#39;Location_AliceSprings&#39;, &#39;Location_BadgerysCreek&#39;, &#39;Location_Ballarat&#39;, &#39;Location_Bendigo&#39;, &#39;Location_Brisbane&#39;, &#39;Location_Cairns&#39;, &#39;Location_Canberra&#39;, &#39;Location_Cobar&#39;, &#39;Location_CoffsHarbour&#39;, &#39;Location_Dartmoor&#39;, &#39;Location_Darwin&#39;, &#39;Location_GoldCoast&#39;, &#39;Location_Hobart&#39;, &#39;Location_Katherine&#39;, &#39;Location_Launceston&#39;, &#39;Location_Melbourne&#39;, &#39;Location_MelbourneAirport&#39;, &#39;Location_Mildura&#39;, &#39;Location_Moree&#39;, &#39;Location_MountGambier&#39;, &#39;Location_MountGinini&#39;, &#39;Location_Newcastle&#39;, &#39;Location_Nhil&#39;, &#39;Location_NorahHead&#39;, &#39;Location_NorfolkIsland&#39;, &#39;Location_Nuriootpa&#39;, &#39;Location_PearceRAAF&#39;, &#39;Location_Penrith&#39;, &#39;Location_Perth&#39;, &#39;Location_PerthAirport&#39;, &#39;Location_Portland&#39;, &#39;Location_Richmond&#39;, &#39;Location_Sale&#39;, &#39;Location_SalmonGums&#39;, &#39;Location_Sydney&#39;, &#39;Location_SydneyAirport&#39;, &#39;Location_Townsville&#39;, &#39;Location_Tuggeranong&#39;, &#39;Location_Uluru&#39;, &#39;Location_WaggaWagga&#39;, &#39;Location_Walpole&#39;, &#39;Location_Watsonia&#39;, &#39;Location_Williamtown&#39;, &#39;Location_Witchcliffe&#39;, &#39;Location_Wollongong&#39;, &#39;Location_Woomera&#39;, &#39;WindGustDir_E&#39;, &#39;WindGustDir_ENE&#39;, &#39;WindGustDir_ESE&#39;, &#39;WindGustDir_N&#39;, &#39;WindGustDir_NE&#39;, &#39;WindGustDir_NNE&#39;, &#39;WindGustDir_NNW&#39;, &#39;WindGustDir_NW&#39;, &#39;WindGustDir_S&#39;, &#39;WindGustDir_SE&#39;, &#39;WindGustDir_SSE&#39;, &#39;WindGustDir_SSW&#39;, &#39;WindGustDir_SW&#39;, &#39;WindGustDir_W&#39;, &#39;WindGustDir_WNW&#39;, &#39;WindGustDir_WSW&#39;, &#39;WindGustDir_nan&#39;, &#39;WindDir9am_E&#39;, &#39;WindDir9am_ENE&#39;, &#39;WindDir9am_ESE&#39;, &#39;WindDir9am_N&#39;, &#39;WindDir9am_NE&#39;, &#39;WindDir9am_NNE&#39;, &#39;WindDir9am_NNW&#39;, &#39;WindDir9am_NW&#39;, &#39;WindDir9am_S&#39;, &#39;WindDir9am_SE&#39;, &#39;WindDir9am_SSE&#39;, &#39;WindDir9am_SSW&#39;, &#39;WindDir9am_SW&#39;, &#39;WindDir9am_W&#39;, &#39;WindDir9am_WNW&#39;, &#39;WindDir9am_WSW&#39;, &#39;WindDir9am_nan&#39;, &#39;WindDir3pm_E&#39;, &#39;WindDir3pm_ENE&#39;, &#39;WindDir3pm_ESE&#39;, &#39;WindDir3pm_N&#39;, &#39;WindDir3pm_NE&#39;, &#39;WindDir3pm_NNE&#39;, &#39;WindDir3pm_NNW&#39;, &#39;WindDir3pm_NW&#39;, &#39;WindDir3pm_S&#39;, &#39;WindDir3pm_SE&#39;, &#39;WindDir3pm_SSE&#39;, &#39;WindDir3pm_SSW&#39;, &#39;WindDir3pm_SW&#39;, &#39;WindDir3pm_W&#39;, &#39;WindDir3pm_WNW&#39;, &#39;WindDir3pm_WSW&#39;, &#39;WindDir3pm_nan&#39;, &#39;RainToday_No&#39;, &#39;RainToday_Yes&#39;, &#39;RainToday_nan&#39;] . train_inputs.head(10) . Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am WindDir3pm WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm RainToday Location_Adelaide Location_Albany Location_Albury Location_AliceSprings Location_BadgerysCreek Location_Ballarat Location_Bendigo Location_Brisbane Location_Cairns Location_Canberra Location_Cobar Location_CoffsHarbour Location_Dartmoor Location_Darwin Location_GoldCoast Location_Hobart Location_Katherine Location_Launceston Location_Melbourne Location_MelbourneAirport Location_Mildura Location_Moree Location_MountGambier Location_MountGinini Location_Newcastle Location_Nhil Location_NorahHead Location_NorfolkIsland Location_Nuriootpa Location_PearceRAAF Location_Penrith Location_Perth Location_PerthAirport Location_Portland Location_Richmond Location_Sale Location_SalmonGums Location_Sydney Location_SydneyAirport Location_Townsville Location_Tuggeranong Location_Uluru Location_WaggaWagga Location_Walpole Location_Watsonia Location_Williamtown Location_Witchcliffe Location_Wollongong Location_Woomera WindGustDir_E WindGustDir_ENE WindGustDir_ESE WindGustDir_N WindGustDir_NE WindGustDir_NNE WindGustDir_NNW WindGustDir_NW WindGustDir_S WindGustDir_SE WindGustDir_SSE WindGustDir_SSW WindGustDir_SW WindGustDir_W WindGustDir_WNW WindGustDir_WSW WindGustDir_nan WindDir9am_E WindDir9am_ENE WindDir9am_ESE WindDir9am_N WindDir9am_NE WindDir9am_NNE WindDir9am_NNW WindDir9am_NW WindDir9am_S WindDir9am_SE WindDir9am_SSE WindDir9am_SSW WindDir9am_SW WindDir9am_W WindDir9am_WNW WindDir9am_WSW WindDir9am_nan WindDir3pm_E WindDir3pm_ENE WindDir3pm_ESE WindDir3pm_N WindDir3pm_NE WindDir3pm_NNE WindDir3pm_NNW WindDir3pm_NW WindDir3pm_S WindDir3pm_SE WindDir3pm_SSE WindDir3pm_SSW WindDir3pm_SW WindDir3pm_W WindDir3pm_WNW WindDir3pm_WSW WindDir3pm_nan RainToday_No RainToday_Yes RainToday_nan . 0 Albury | 0.516509 | 0.523629 | 0.001617 | 0.037723 | 0.525852 | W | 0.294574 | W | WNW | 0.153846 | 0.275862 | 0.71 | 0.22 | 0.449587 | 0.4800 | 0.888889 | 0.500352 | 0.508439 | 0.522073 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 1 Albury | 0.375000 | 0.565217 | 0.000000 | 0.037723 | 0.525852 | WNW | 0.294574 | NNW | WSW | 0.030769 | 0.252874 | 0.44 | 0.25 | 0.497521 | 0.4912 | 0.493021 | 0.500352 | 0.514768 | 0.570058 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2 Albury | 0.504717 | 0.576560 | 0.000000 | 0.037723 | 0.525852 | WSW | 0.310078 | W | WSW | 0.146154 | 0.298851 | 0.38 | 0.30 | 0.447934 | 0.5056 | 0.493021 | 0.222222 | 0.594937 | 0.548944 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 3 Albury | 0.417453 | 0.620038 | 0.000000 | 0.037723 | 0.525852 | NE | 0.139535 | SE | E | 0.084615 | 0.103448 | 0.45 | 0.16 | 0.613223 | 0.5712 | 0.493021 | 0.500352 | 0.533755 | 0.612284 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 4 Albury | 0.613208 | 0.701323 | 0.002695 | 0.037723 | 0.525852 | W | 0.271318 | ENE | NW | 0.053846 | 0.229885 | 0.82 | 0.33 | 0.500826 | 0.4624 | 0.777778 | 0.888889 | 0.527426 | 0.673704 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 5 Albury | 0.544811 | 0.652174 | 0.000539 | 0.037723 | 0.525852 | WNW | 0.387597 | W | W | 0.146154 | 0.275862 | 0.55 | 0.23 | 0.474380 | 0.4528 | 0.493021 | 0.500352 | 0.586498 | 0.658349 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 6 Albury | 0.537736 | 0.563327 | 0.000000 | 0.037723 | 0.525852 | W | 0.341085 | SW | W | 0.153846 | 0.275862 | 0.49 | 0.19 | 0.480992 | 0.4976 | 0.111111 | 0.500352 | 0.533755 | 0.575816 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 7 Albury | 0.382075 | 0.595463 | 0.000000 | 0.037723 | 0.525852 | W | 0.224806 | SSE | W | 0.046154 | 0.195402 | 0.48 | 0.19 | 0.543802 | 0.5280 | 0.493021 | 0.500352 | 0.495781 | 0.593090 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 8 Albury | 0.429245 | 0.693762 | 0.000000 | 0.037723 | 0.525852 | NNW | 0.573643 | SE | NW | 0.053846 | 0.321839 | 0.42 | 0.09 | 0.469421 | 0.4240 | 0.493021 | 0.500352 | 0.537975 | 0.683301 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 9 Albury | 0.509434 | 0.659735 | 0.003774 | 0.037723 | 0.525852 | W | 0.170543 | S | SSE | 0.115385 | 0.126437 | 0.58 | 0.27 | 0.438017 | 0.4576 | 0.493021 | 0.500352 | 0.575949 | 0.644914 | Yes | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | . X_train = train_inputs[numeric_cols + encoded_cols] X_val = val_inputs[numeric_cols + encoded_cols] X_test = test_inputs[numeric_cols + encoded_cols] . X_test.head(10) . MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustSpeed WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm Location_Adelaide Location_Albany Location_Albury Location_AliceSprings Location_BadgerysCreek Location_Ballarat Location_Bendigo Location_Brisbane Location_Cairns Location_Canberra Location_Cobar Location_CoffsHarbour Location_Dartmoor Location_Darwin Location_GoldCoast Location_Hobart Location_Katherine Location_Launceston Location_Melbourne Location_MelbourneAirport Location_Mildura Location_Moree Location_MountGambier Location_MountGinini Location_Newcastle Location_Nhil Location_NorahHead Location_NorfolkIsland Location_Nuriootpa Location_PearceRAAF Location_Penrith Location_Perth Location_PerthAirport Location_Portland Location_Richmond Location_Sale Location_SalmonGums Location_Sydney Location_SydneyAirport Location_Townsville Location_Tuggeranong Location_Uluru Location_WaggaWagga Location_Walpole Location_Watsonia Location_Williamtown Location_Witchcliffe Location_Wollongong Location_Woomera WindGustDir_E WindGustDir_ENE WindGustDir_ESE WindGustDir_N WindGustDir_NE WindGustDir_NNE WindGustDir_NNW WindGustDir_NW WindGustDir_S WindGustDir_SE WindGustDir_SSE WindGustDir_SSW WindGustDir_SW WindGustDir_W WindGustDir_WNW WindGustDir_WSW WindGustDir_nan WindDir9am_E WindDir9am_ENE WindDir9am_ESE WindDir9am_N WindDir9am_NE WindDir9am_NNE WindDir9am_NNW WindDir9am_NW WindDir9am_S WindDir9am_SE WindDir9am_SSE WindDir9am_SSW WindDir9am_SW WindDir9am_W WindDir9am_WNW WindDir9am_WSW WindDir9am_nan WindDir3pm_E WindDir3pm_ENE WindDir3pm_ESE WindDir3pm_N WindDir3pm_NE WindDir3pm_NNE WindDir3pm_NNW WindDir3pm_NW WindDir3pm_S WindDir3pm_SE WindDir3pm_SSE WindDir3pm_SSW WindDir3pm_SW WindDir3pm_W WindDir3pm_WNW WindDir3pm_WSW WindDir3pm_nan RainToday_No RainToday_Yes RainToday_nan . 2498 0.681604 | 0.801512 | 0.000000 | 0.037723 | 0.525852 | 0.372093 | 0.000000 | 0.080460 | 0.46 | 0.17 | 0.543802 | 0.5136 | 0.777778 | 0.333333 | 0.702532 | 0.808061 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2499 0.693396 | 0.725898 | 0.001078 | 0.037723 | 0.525852 | 0.341085 | 0.069231 | 0.195402 | 0.54 | 0.30 | 0.505785 | 0.5008 | 0.888889 | 0.888889 | 0.675105 | 0.712092 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2500 0.634434 | 0.527410 | 0.005930 | 0.037723 | 0.525852 | 0.325581 | 0.084615 | 0.448276 | 0.62 | 0.67 | 0.553719 | 0.6032 | 0.888889 | 0.888889 | 0.611814 | 0.477927 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | . 2501 0.608491 | 0.538752 | 0.042049 | 0.037723 | 0.525852 | 0.255814 | 0.069231 | 0.195402 | 0.74 | 0.65 | 0.618182 | 0.6304 | 0.888889 | 0.888889 | 0.556962 | 0.518234 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | . 2502 0.566038 | 0.523629 | 0.018329 | 0.037723 | 0.525852 | 0.193798 | 0.046154 | 0.103448 | 0.92 | 0.63 | 0.591736 | 0.5888 | 0.888889 | 0.888889 | 0.514768 | 0.529750 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | . 2503 0.601415 | 0.621928 | 0.000539 | 0.037723 | 0.525852 | 0.255814 | 0.069231 | 0.126437 | 0.76 | 0.52 | 0.563636 | 0.5680 | 0.888889 | 0.888889 | 0.580169 | 0.596929 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2504 0.587264 | 0.620038 | 0.000000 | 0.037723 | 0.525852 | 0.224806 | 0.153846 | 0.229885 | 0.46 | 0.31 | 0.609917 | 0.6176 | 0.493021 | 0.222222 | 0.592827 | 0.614203 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2505 0.537736 | 0.689981 | 0.000000 | 0.037723 | 0.525852 | 0.139535 | 0.084615 | 0.068966 | 0.63 | 0.24 | 0.646281 | 0.6416 | 0.493021 | 0.888889 | 0.561181 | 0.654511 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2506 0.594340 | 0.752363 | 0.000000 | 0.037723 | 0.525852 | 0.170543 | 0.084615 | 0.103448 | 0.52 | 0.24 | 0.629752 | 0.6144 | 0.493021 | 0.333333 | 0.662447 | 0.738964 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2507 0.620283 | 0.790170 | 0.000000 | 0.037723 | 0.525852 | 0.271318 | 0.069231 | 0.195402 | 0.54 | 0.17 | 0.596694 | 0.5680 | 0.493021 | 0.500352 | 0.704641 | 0.798464 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . Training and Visualizing Decision Trees . . useful blog :Visualize a Decision Tree in 4 Ways with Scikit-Learn and Python . Training . from sklearn.tree import DecisionTreeClassifier . model = DecisionTreeClassifier(random_state=42) # random state is provided to get same value each time . %%time model.fit(X_train, train_targets) . CPU times: user 2.93 s, sys: 12.6 ms, total: 2.94 s Wall time: 2.95 s . DecisionTreeClassifier(random_state=42) . Evaluation . from sklearn.metrics import accuracy_score, confusion_matrix . train_preds = model.predict(X_train) . train_preds . array([&#39;No&#39;, &#39;No&#39;, &#39;No&#39;, ..., &#39;No&#39;, &#39;No&#39;, &#39;No&#39;], dtype=object) . pd.value_counts(train_preds) . No 76707 Yes 22281 dtype: int64 . Decision tree also returns probabilities of each prediction . train_probs = model.predict_proba(X_train) . train_probs . array([[1., 0.], [1., 0.], [1., 0.], ..., [1., 0.], [1., 0.], [1., 0.]]) . train_targets . 0 No 1 No 2 No 3 No 4 No .. 144548 No 144549 No 144550 No 144551 No 144552 No Name: RainTomorrow, Length: 98988, dtype: object . accuracy_score(train_preds, train_targets) . 0.9999797955307714 . model.score(X_val, val_targets) # direct prediction on val inputs and compare accuracy #only ~79% . 0.7921188555510418 . val_targets.value_counts() / len(val_targets) . No 0.788289 Yes 0.211711 Name: RainTomorrow, dtype: float64 . It appears that the model has learned the training examples perfect, and doesn&#39;t generalize well to previously unseen examples. This phenomenon is called &quot;overfitting&quot;, and reducing overfitting is one of the most important parts of any machine learning project. . Visualizing Tree . from sklearn.tree import plot_tree, export_text . plt.figure(figsize=(80, 40)) plot_tree(model, feature_names=X_train.columns, max_depth=2, filled=True) . [Text(2232.0, 1902.6000000000001, &#39;Humidity3pm &lt;= 0.715 ngini = 0.349 nsamples = 98988 nvalue = [76705, 22283]&#39;), Text(1116.0, 1359.0, &#39;Rainfall &lt;= 0.004 ngini = 0.248 nsamples = 82418 nvalue = [70439, 11979]&#39;), Text(558.0, 815.4000000000001, &#39;Sunshine &lt;= 0.525 ngini = 0.198 nsamples = 69252 nvalue = [61538, 7714]&#39;), Text(279.0, 271.79999999999995, &#39; n (...) n&#39;), Text(837.0, 271.79999999999995, &#39; n (...) n&#39;), Text(1674.0, 815.4000000000001, &#39;Humidity3pm &lt;= 0.512 ngini = 0.438 nsamples = 13166 nvalue = [8901, 4265]&#39;), Text(1395.0, 271.79999999999995, &#39; n (...) n&#39;), Text(1953.0, 271.79999999999995, &#39; n (...) n&#39;), Text(3348.0, 1359.0, &#39;Humidity3pm &lt;= 0.825 ngini = 0.47 nsamples = 16570 nvalue = [6266, 10304]&#39;), Text(2790.0, 815.4000000000001, &#39;WindGustSpeed &lt;= 0.279 ngini = 0.499 nsamples = 9136 nvalue = [4804, 4332]&#39;), Text(2511.0, 271.79999999999995, &#39; n (...) n&#39;), Text(3069.0, 271.79999999999995, &#39; n (...) n&#39;), Text(3906.0, 815.4000000000001, &#39;Rainfall &lt;= 0.01 ngini = 0.316 nsamples = 7434 nvalue = [1462, 5972]&#39;), Text(3627.0, 271.79999999999995, &#39; n (...) n&#39;), Text(4185.0, 271.79999999999995, &#39; n (...) n&#39;)] . How a Decision Tree is Created . Note the gini value in each box. This is the loss function used by the decision tree to decide which column should be used for splitting the data, and at what point the column should be split. A lower Gini index indicates a better split. A perfect split (only one class on each side) has a Gini index of 0. . For a mathematical discussion of the Gini Index, watch this video: It has the following formula: . . Conceptually speaking, while training the models evaluates all possible splits across all possible columns and picks the best one. Then, it recursively performs an optimal split for the two portions. In practice, however, it&#39;s very inefficient to check all possible splits, so the model uses a heuristic (predefined strategy) combined with some randomization. . Let&#39;s check the depth of the tree that was created. . model.tree_.max_depth . 48 . tree_text = export_text(model, max_depth=10, feature_names=list(X_train.columns)) print(tree_text[:5000]) . | Humidity3pm &lt;= 0.72 | | Rainfall &lt;= 0.00 | | | Sunshine &lt;= 0.52 | | | | Pressure3pm &lt;= 0.58 | | | | | WindGustSpeed &lt;= 0.36 | | | | | | Humidity3pm &lt;= 0.28 | | | | | | | WindDir9am_NE &lt;= 0.50 | | | | | | | | Location_Watsonia &lt;= 0.50 | | | | | | | | | Cloud9am &lt;= 0.83 | | | | | | | | | | WindSpeed3pm &lt;= 0.07 | | | | | | | | | | | Pressure3pm &lt;= 0.46 | | | | | | | | | | | | class: Yes | | | | | | | | | | | Pressure3pm &gt; 0.46 | | | | | | | | | | | | class: No | | | | | | | | | | WindSpeed3pm &gt; 0.07 | | | | | | | | | | | MinTemp &lt;= 0.32 | | | | | | | | | | | | truncated branch of depth 2 | | | | | | | | | | | MinTemp &gt; 0.32 | | | | | | | | | | | | truncated branch of depth 7 | | | | | | | | | Cloud9am &gt; 0.83 | | | | | | | | | | Cloud3pm &lt;= 0.42 | | | | | | | | | | | class: Yes | | | | | | | | | | Cloud3pm &gt; 0.42 | | | | | | | | | | | Rainfall &lt;= 0.00 | | | | | | | | | | | | truncated branch of depth 2 | | | | | | | | | | | Rainfall &gt; 0.00 | | | | | | | | | | | | class: Yes | | | | | | | | Location_Watsonia &gt; 0.50 | | | | | | | | | class: Yes | | | | | | | WindDir9am_NE &gt; 0.50 | | | | | | | | WindGustSpeed &lt;= 0.25 | | | | | | | | | class: No | | | | | | | | WindGustSpeed &gt; 0.25 | | | | | | | | | Pressure9am &lt;= 0.54 | | | | | | | | | | Evaporation &lt;= 0.09 | | | | | | | | | | | Location_AliceSprings &lt;= 0.50 | | | | | | | | | | | | truncated branch of depth 4 | | | | | | | | | | | Location_AliceSprings &gt; 0.50 | | | | | | | | | | | | class: Yes | | | | | | | | | | Evaporation &gt; 0.09 | | | | | | | | | | | WindGustDir_ENE &lt;= 0.50 | | | | | | | | | | | | class: Yes | | | | | | | | | | | WindGustDir_ENE &gt; 0.50 | | | | | | | | | | | | class: No | | | | | | | | | Pressure9am &gt; 0.54 | | | | | | | | | | Humidity3pm &lt;= 0.20 | | | | | | | | | | | class: Yes | | | | | | | | | | Humidity3pm &gt; 0.20 | | | | | | | | | | | Evaporation &lt;= 0.02 | | | | | | | | | | | | class: Yes | | | | | | | | | | | Evaporation &gt; 0.02 | | | | | | | | | | | | class: No | | | | | | Humidity3pm &gt; 0.28 | | | | | | | Sunshine &lt;= 0.05 | | | | | | | | WindGustSpeed &lt;= 0.25 | | | | | | | | | Evaporation &lt;= 0.01 | | | | | | | | | | WindGustSpeed &lt;= 0.23 | | | | | | | | | | | class: Yes | | | | | | | | | | WindGustSpeed &gt; 0.23 | | | | | | | | | | | class: No | | | | | | | | | Evaporation &gt; 0.01 | | | | | | | | | | Evaporation &lt;= 0.07 | | | | | | | | | | | Temp3pm &lt;= 0.34 | | | | | | | | | | | | class: Yes | | | | | | | | | | | Temp3pm &gt; 0.34 | | | | | | | | | | | | truncated branch of depth 11 | | | | | | | | | | Evaporation &gt; 0.07 | | | | | | | | | | | WindSpeed9am &lt;= 0.12 | | | | | | | | | | | | class: Yes | | | | | | | | | | | WindSpeed9am &gt; 0.12 | | | | | | | | | | | | class: No | | | | | | | | WindGustSpeed &gt; 0.25 | | | | | | | | | Pressure9am &lt;= 0.56 | | | | | | | | | | MinTemp &lt;= 0.40 | | | | | | | | | | | WindDir9am_WNW &lt;= 0.50 | | | | | | | | | | | | class: Yes | | | | | | | | | | | WindDir9am_WNW &gt; 0.50 | | | | | | | | | | | | class: No | | | | | | | | | | MinTemp &gt; 0.40 | | | | | | | | | | | Humidity3pm &lt;= 0.66 | | | | | | | | | | | | truncated branch of depth 7 | | | | | | | | | | | Humidity3pm &gt; 0.66 | | | | | | | | | | | | truncated branch of depth 4 | | | | | | | | | Pressure9am &gt; 0.56 | | | | | . Feature Importance . X_train.columns . Index([&#39;MinTemp&#39;, &#39;MaxTemp&#39;, &#39;Rainfall&#39;, &#39;Evaporation&#39;, &#39;Sunshine&#39;, &#39;WindGustSpeed&#39;, &#39;WindSpeed9am&#39;, &#39;WindSpeed3pm&#39;, &#39;Humidity9am&#39;, &#39;Humidity3pm&#39;, ... &#39;WindDir3pm_SSE&#39;, &#39;WindDir3pm_SSW&#39;, &#39;WindDir3pm_SW&#39;, &#39;WindDir3pm_W&#39;, &#39;WindDir3pm_WNW&#39;, &#39;WindDir3pm_WSW&#39;, &#39;WindDir3pm_nan&#39;, &#39;RainToday_No&#39;, &#39;RainToday_Yes&#39;, &#39;RainToday_nan&#39;], dtype=&#39;object&#39;, length=119) . model.feature_importances_ . array([3.48942086e-02, 3.23605486e-02, 5.91385668e-02, 2.49619907e-02, 4.94652143e-02, 5.63334673e-02, 2.80205998e-02, 2.98128801e-02, 4.02182908e-02, 2.61441297e-01, 3.44145027e-02, 6.20573699e-02, 1.36406176e-02, 1.69229866e-02, 3.50001550e-02, 3.04064076e-02, 2.24086587e-03, 2.08018104e-03, 1.27475954e-03, 7.26936324e-04, 1.39779517e-03, 1.15264873e-03, 6.92808159e-04, 1.80675598e-03, 1.08370901e-03, 1.19773895e-03, 8.87119103e-04, 2.15764220e-03, 1.67094731e-03, 7.98919493e-05, 1.10558668e-03, 1.42008656e-03, 4.10087635e-04, 1.09028115e-03, 1.44164766e-03, 9.08284767e-04, 1.05770304e-03, 6.18133455e-04, 1.80387272e-03, 2.10403527e-03, 2.74413333e-04, 7.31599405e-04, 1.35408990e-03, 1.54759332e-03, 1.30917564e-03, 1.07134670e-03, 8.36408023e-04, 1.62662229e-03, 1.00326116e-03, 2.16053455e-03, 8.46802258e-04, 1.88919081e-03, 9.29325203e-04, 1.29545157e-03, 1.27604831e-03, 5.12736888e-04, 1.38458902e-03, 3.97103931e-04, 1.03734689e-03, 1.44437047e-03, 1.75870184e-03, 1.42487857e-03, 2.78109569e-03, 2.00782698e-03, 2.80617652e-04, 1.61509734e-03, 1.64361598e-03, 2.36124112e-03, 3.05457932e-03, 2.33239534e-03, 2.78643875e-03, 2.16695261e-03, 3.41491352e-03, 2.30573542e-03, 2.28270604e-03, 2.34408118e-03, 2.26557332e-03, 2.54592702e-03, 2.75264499e-03, 2.83905192e-03, 2.49480561e-03, 1.54840338e-03, 2.50305095e-03, 2.53945388e-03, 2.28130055e-03, 3.80572180e-03, 2.58535069e-03, 3.10172224e-03, 2.54236791e-03, 2.50297796e-03, 2.06400988e-03, 2.52931192e-03, 2.07840517e-03, 1.77387278e-03, 1.78920555e-03, 2.77709687e-03, 2.42564566e-03, 2.26471887e-03, 1.73346117e-03, 2.23926957e-03, 2.47865244e-03, 2.31917387e-03, 3.21211861e-03, 2.92382975e-03, 2.24399274e-03, 3.68774754e-03, 3.87595982e-03, 3.20326068e-03, 2.53323550e-03, 2.40444844e-03, 2.26790411e-03, 2.19744009e-03, 2.28064147e-03, 2.88545323e-03, 2.05278867e-03, 1.12604304e-03, 2.86325849e-04, 1.32322128e-03, 1.72690480e-03]) . importance_df = pd.DataFrame({ &#39;feature&#39;: X_train.columns, &#39;importance&#39;: model.feature_importances_ }).sort_values(&#39;importance&#39;, ascending=False) . importance_df.head(10) . feature importance . 9 Humidity3pm | 0.261441 | . 11 Pressure3pm | 0.062057 | . 2 Rainfall | 0.059139 | . 5 WindGustSpeed | 0.056333 | . 4 Sunshine | 0.049465 | . 8 Humidity9am | 0.040218 | . 14 Temp9am | 0.035000 | . 0 MinTemp | 0.034894 | . 10 Pressure9am | 0.034415 | . 1 MaxTemp | 0.032361 | . plt.title(&#39;Feature Importance&#39;) sns.barplot(data=importance_df.head(10), x=&#39;importance&#39;, y=&#39;feature&#39;); . Hyperparameter Tuning and Overfitting . ?DecisionTreeClassifier . As we saw in the previous section, our decision tree classifier memorized all training examples, leading to a 100% training accuracy, while the validation accuracy was only marginally better than a dumb baseline model. This phenomenon is called overfitting, and in this section, we&#39;ll look at some strategies for reducing overfitting. The process of reducing overfitting is known as regularlization. . The DecisionTreeClassifier accepts several arguments, some of which can be modified to reduce overfitting. . These arguments are called hyperparameters because they must be configured manually (as opposed to the parameters within the model which are learned from the data. We&#39;ll explore a couple of hyperparameters: . max_depth | max_leaf_nodes | . max_depth . By reducing the maximum depth of the decision tree, we can prevent the tree from memorizing all training examples, which may lead to better generalization . model = DecisionTreeClassifier(max_depth=3, random_state=42) . model.fit(X_train, train_targets) . DecisionTreeClassifier(max_depth=3, random_state=42) . model.score(X_train, train_targets) . 0.8291308037337859 . model.score(X_val, val_targets) . 0.8334397307178921 . model.classes_ . array([&#39;No&#39;, &#39;Yes&#39;], dtype=object) . Great, while the training accuracy of the model has gone down, the validation accuracy of the model has increased significantly. . plt.figure(figsize=(80, 40)) plot_tree(model, feature_names=X_train.columns, filled=True, rounded=True, class_names=model.classes_) . [Text(2232.0, 1902.6000000000001, &#39;Humidity3pm &lt;= 0.715 ngini = 0.349 nsamples = 98988 nvalue = [76705, 22283] nclass = No&#39;), Text(1116.0, 1359.0, &#39;Rainfall &lt;= 0.004 ngini = 0.248 nsamples = 82418 nvalue = [70439, 11979] nclass = No&#39;), Text(558.0, 815.4000000000001, &#39;Sunshine &lt;= 0.525 ngini = 0.198 nsamples = 69252 nvalue = [61538, 7714] nclass = No&#39;), Text(279.0, 271.79999999999995, &#39;gini = 0.363 nsamples = 12620 nvalue = [9618, 3002] nclass = No&#39;), Text(837.0, 271.79999999999995, &#39;gini = 0.153 nsamples = 56632 nvalue = [51920, 4712] nclass = No&#39;), Text(1674.0, 815.4000000000001, &#39;Humidity3pm &lt;= 0.512 ngini = 0.438 nsamples = 13166 nvalue = [8901, 4265] nclass = No&#39;), Text(1395.0, 271.79999999999995, &#39;gini = 0.293 nsamples = 4299 nvalue = [3531, 768] nclass = No&#39;), Text(1953.0, 271.79999999999995, &#39;gini = 0.478 nsamples = 8867 nvalue = [5370, 3497] nclass = No&#39;), Text(3348.0, 1359.0, &#39;Humidity3pm &lt;= 0.825 ngini = 0.47 nsamples = 16570 nvalue = [6266, 10304] nclass = Yes&#39;), Text(2790.0, 815.4000000000001, &#39;WindGustSpeed &lt;= 0.279 ngini = 0.499 nsamples = 9136 nvalue = [4804, 4332] nclass = No&#39;), Text(2511.0, 271.79999999999995, &#39;gini = 0.472 nsamples = 5583 nvalue = [3457, 2126] nclass = No&#39;), Text(3069.0, 271.79999999999995, &#39;gini = 0.471 nsamples = 3553 nvalue = [1347, 2206] nclass = Yes&#39;), Text(3906.0, 815.4000000000001, &#39;Rainfall &lt;= 0.01 ngini = 0.316 nsamples = 7434 nvalue = [1462, 5972] nclass = Yes&#39;), Text(3627.0, 271.79999999999995, &#39;gini = 0.391 nsamples = 4360 nvalue = [1161, 3199] nclass = Yes&#39;), Text(4185.0, 271.79999999999995, &#39;gini = 0.177 nsamples = 3074 nvalue = [301, 2773] nclass = Yes&#39;)] . print(export_text(model, feature_names=list(X_train.columns))) . | Humidity3pm &lt;= 0.72 | | Rainfall &lt;= 0.00 | | | Sunshine &lt;= 0.52 | | | | class: No | | | Sunshine &gt; 0.52 | | | | class: No | | Rainfall &gt; 0.00 | | | Humidity3pm &lt;= 0.51 | | | | class: No | | | Humidity3pm &gt; 0.51 | | | | class: No | Humidity3pm &gt; 0.72 | | Humidity3pm &lt;= 0.82 | | | WindGustSpeed &lt;= 0.28 | | | | class: No | | | WindGustSpeed &gt; 0.28 | | | | class: Yes | | Humidity3pm &gt; 0.82 | | | Rainfall &lt;= 0.01 | | | | class: Yes | | | Rainfall &gt; 0.01 | | | | class: Yes . def max_depth_error(md): model = DecisionTreeClassifier(max_depth=md, random_state=42) model.fit(X_train, train_targets) train_error = 1 - model.score(X_train, train_targets) val_error = 1 - model.score(X_val, val_targets) return {&#39;Max Depth&#39;: md, &#39;Training Error&#39;: train_error, &#39;Validation Error&#39;: val_error} . %%time errors_df = pd.DataFrame([max_depth_error(md) for md in range(1, 21)]) . CPU times: user 26.4 s, sys: 225 ms, total: 26.6 s Wall time: 26.7 s . errors_df . Max Depth Training Error Validation Error . 0 1 | 0.184315 | 0.177935 | . 1 2 | 0.179547 | 0.172712 | . 2 3 | 0.170869 | 0.166560 | . 3 4 | 0.165707 | 0.164355 | . 4 5 | 0.160676 | 0.159074 | . 5 6 | 0.156271 | 0.157275 | . 6 7 | 0.153312 | 0.154605 | . 7 8 | 0.147806 | 0.158029 | . 8 9 | 0.140906 | 0.156578 | . 9 10 | 0.132945 | 0.157333 | . 10 11 | 0.123227 | 0.159248 | . 11 12 | 0.113489 | 0.160815 | . 12 13 | 0.101750 | 0.163833 | . 13 14 | 0.089981 | 0.167373 | . 14 15 | 0.078999 | 0.171261 | . 15 16 | 0.068180 | 0.174279 | . 16 17 | 0.058138 | 0.176890 | . 17 18 | 0.048733 | 0.181243 | . 18 19 | 0.040025 | 0.187569 | . 19 20 | 0.032539 | 0.190297 | . plt.figure() plt.plot(errors_df[&#39;Max Depth&#39;], errors_df[&#39;Training Error&#39;]) plt.plot(errors_df[&#39;Max Depth&#39;], errors_df[&#39;Validation Error&#39;]) plt.title(&quot;Training vs Validation Error&quot;) plt.xticks(range(0,21,2)) plt.xlabel(&#39;Max. Depth&#39;) plt.ylabel(&#39;Prediction Error ie 1-Accuracy&#39;) plt.legend([&#39;Training&#39;, &#39;Validation&#39;]) . &lt;matplotlib.legend.Legend at 0x7f1fcb8fd3a0&gt; . . So for us max depth of 7 results in lowest validation error . model = DecisionTreeClassifier(max_depth=7, random_state=42).fit(X_train, train_targets) model.score(X_val, val_targets), model.score(X_train, train_targets) . (0.8453949277465034, 0.8466884874934335) . max_leaf_nodes . Another way to control the size of complexity of a decision tree is to limit the number of leaf nodes. This allows branches of the tree to have varying depths. . model = DecisionTreeClassifier(max_leaf_nodes = 128, random_state = 42) . model.fit(X_train, train_targets) . DecisionTreeClassifier(max_leaf_nodes=128, random_state=42) . model.score(X_train, train_targets) . 0.8480421869317493 . model.score(X_val, val_targets) . 0.8442342290058615 . model.tree_.max_depth . 12 . Notice that the model was able to achieve a greater depth of 12 for certain paths while keeping other paths shorter. . model_text = export_text(model, feature_names = list(X_train.columns)) print(model_text[:3000]) . | Humidity3pm &lt;= 0.72 | | Rainfall &lt;= 0.00 | | | Sunshine &lt;= 0.52 | | | | Pressure3pm &lt;= 0.58 | | | | | WindGustSpeed &lt;= 0.36 | | | | | | Humidity3pm &lt;= 0.28 | | | | | | | class: No | | | | | | Humidity3pm &gt; 0.28 | | | | | | | Sunshine &lt;= 0.05 | | | | | | | | class: Yes | | | | | | | Sunshine &gt; 0.05 | | | | | | | | Pressure3pm &lt;= 0.43 | | | | | | | | | class: Yes | | | | | | | | Pressure3pm &gt; 0.43 | | | | | | | | | Humidity3pm &lt;= 0.57 | | | | | | | | | | WindDir9am_NE &lt;= 0.50 | | | | | | | | | | | WindDir9am_NNE &lt;= 0.50 | | | | | | | | | | | | class: No | | | | | | | | | | | WindDir9am_NNE &gt; 0.50 | | | | | | | | | | | | class: No | | | | | | | | | | WindDir9am_NE &gt; 0.50 | | | | | | | | | | | class: Yes | | | | | | | | | Humidity3pm &gt; 0.57 | | | | | | | | | | MaxTemp &lt;= 0.53 | | | | | | | | | | | class: No | | | | | | | | | | MaxTemp &gt; 0.53 | | | | | | | | | | | Temp3pm &lt;= 0.67 | | | | | | | | | | | | class: No | | | | | | | | | | | Temp3pm &gt; 0.67 | | | | | | | | | | | | class: No | | | | | WindGustSpeed &gt; 0.36 | | | | | | Humidity3pm &lt;= 0.45 | | | | | | | Sunshine &lt;= 0.39 | | | | | | | | class: No | | | | | | | Sunshine &gt; 0.39 | | | | | | | | class: No | | | | | | Humidity3pm &gt; 0.45 | | | | | | | Pressure3pm &lt;= 0.49 | | | | | | | | class: Yes | | | | | | | Pressure3pm &gt; 0.49 | | | | | | | | class: Yes | | | | Pressure3pm &gt; 0.58 | | | | | Pressure3pm &lt;= 0.70 | | | | | | Sunshine &lt;= 0.32 | | | | | | | WindDir9am_N &lt;= 0.50 | | | | | | | | Humidity3pm &lt;= 0.67 | | | | | | | | | class: No | | | | | | | | Humidity3pm &gt; 0.67 | | | | | | | | | class: No | | | | | | | WindDir9am_N &gt; 0.50 | | | | | | | | class: No | | | | | | Sunshine &gt; 0.32 | | | | | | | WindGustSpeed &lt;= 0.33 | | | | | | | | class: No | | | | | | | WindGustSpeed &gt; 0.33 | | | | | | | | class: No | | | | | Pressure3pm &gt; 0.70 | | | | | | Location_CoffsHarbour &lt;= 0.50 | | | | | | | class: No | | | | | | Location_CoffsHarbour &gt; 0.50 | | | | | | | class: No | | | Sunshine &gt; 0.52 | | | . Random Forest with Ausstralia Rain Dataset . While tuning the hyperparameters of a single decision tree may lead to some improvements, a much more effective strategy is to combine the results of several decision trees trained with slightly different parameters. This is called a random forest model. . The key idea here is that each decision tree in the forest will make different kinds of errors, and upon averaging, many of their errors will cancel out. . A random forest works by averaging/combining the results of several decision trees: . . . We&#39;ll use the RandomForestClassifier class from sklearn.ensemble. . from sklearn.ensemble import RandomForestClassifier . model = RandomForestClassifier(n_jobs = -1, random_state=42) # n_jobs= -1 to train in parallel . %%time model.fit(X_train, train_targets) . CPU times: user 38.8 s, sys: 156 ms, total: 39 s Wall time: 4.13 s . RandomForestClassifier(n_jobs=-1, random_state=42) . model.score(X_train, train_targets) . 0.9999494888269285 . model.score(X_val, val_targets) . 0.8566537055307295 . Once again, the training accuracy is almost 100%, but this time the validation accuracy is much better. In fact, it is better than the best single decision tree we had trained so far. Do you see the power of random forests? . This general technique of combining the results of many models is called &quot;ensembling&quot;, it works because most errors of individual models cancel out on averaging. Here&#39;s what it looks like visually: . . We can also look at the probabilities for the predictions. The probability of a class is simply the fraction of trees which that predicted the given class. . train_probs = model.predict_proba(X_train) . train_preds . array([&#39;No&#39;, &#39;No&#39;, &#39;No&#39;, ..., &#39;No&#39;, &#39;No&#39;, &#39;No&#39;], dtype=object) . train_probs . array([[0.93, 0.07], [1. , 0. ], [0.99, 0.01], ..., [0.99, 0.01], [1. , 0. ], [0.96, 0.04]]) . model.estimators_[0] . DecisionTreeClassifier(max_features=&#39;auto&#39;, random_state=1608637542) . len(model.estimators_) . 100 . plt.figure(figsize=(80,40)) plot_tree(model.estimators_[0], max_depth=2, feature_names=X_train.columns, filled=True, class_names=model.classes_) . [Text(2232.0, 1902.6000000000001, &#39;Sunshine &lt;= 0.403 ngini = 0.347 nsamples = 62607 nvalue = [76887, 22101] nclass = No&#39;), Text(1116.0, 1359.0, &#39;Pressure9am &lt;= 0.609 ngini = 0.499 nsamples = 11288 nvalue = [9272, 8542] nclass = No&#39;), Text(558.0, 815.4000000000001, &#39;Cloud9am &lt;= 0.833 ngini = 0.475 nsamples = 6067 nvalue = [3702, 5808] nclass = Yes&#39;), Text(279.0, 271.79999999999995, &#39; n (...) n&#39;), Text(837.0, 271.79999999999995, &#39; n (...) n&#39;), Text(1674.0, 815.4000000000001, &#39;WindGustDir_NNE &lt;= 0.5 ngini = 0.442 nsamples = 5221 nvalue = [5570, 2734] nclass = No&#39;), Text(1395.0, 271.79999999999995, &#39; n (...) n&#39;), Text(1953.0, 271.79999999999995, &#39; n (...) n&#39;), Text(3348.0, 1359.0, &#39;RainToday_Yes &lt;= 0.5 ngini = 0.278 nsamples = 51319 nvalue = [67615, 13559] nclass = No&#39;), Text(2790.0, 815.4000000000001, &#39;Pressure9am &lt;= 0.521 ngini = 0.207 nsamples = 41960 nvalue = [58514, 7796] nclass = No&#39;), Text(2511.0, 271.79999999999995, &#39; n (...) n&#39;), Text(3069.0, 271.79999999999995, &#39; n (...) n&#39;), Text(3906.0, 815.4000000000001, &#39;Pressure9am &lt;= 0.614 ngini = 0.475 nsamples = 9359 nvalue = [9101, 5763] nclass = No&#39;), Text(3627.0, 271.79999999999995, &#39; n (...) n&#39;), Text(4185.0, 271.79999999999995, &#39; n (...) n&#39;)] . plt.figure(figsize=(80,40)) plot_tree(model.estimators_[40], max_depth=2, feature_names=X_train.columns, filled=True, class_names=model.classes_) . [Text(2232.0, 1902.6000000000001, &#39;Cloud9am &lt;= 0.722 ngini = 0.35 nsamples = 62556 nvalue = [76638, 22350] nclass = No&#39;), Text(1116.0, 1359.0, &#39;RainToday_No &lt;= 0.5 ngini = 0.291 nsamples = 48000 nvalue = [62502, 13407] nclass = No&#39;), Text(558.0, 815.4000000000001, &#39;Evaporation &lt;= 0.038 ngini = 0.483 nsamples = 9005 nvalue = [8394, 5804] nclass = No&#39;), Text(279.0, 271.79999999999995, &#39; n (...) n&#39;), Text(837.0, 271.79999999999995, &#39; n (...) n&#39;), Text(1674.0, 815.4000000000001, &#39;Pressure9am &lt;= 0.517 ngini = 0.216 nsamples = 38995 nvalue = [54108, 7603] nclass = No&#39;), Text(1395.0, 271.79999999999995, &#39; n (...) n&#39;), Text(1953.0, 271.79999999999995, &#39; n (...) n&#39;), Text(3348.0, 1359.0, &#39;Humidity3pm &lt;= 0.755 ngini = 0.475 nsamples = 14556 nvalue = [14136, 8943] nclass = No&#39;), Text(2790.0, 815.4000000000001, &#39;Rainfall &lt;= 0.004 ngini = 0.388 nsamples = 10894 nvalue = [12707, 4538] nclass = No&#39;), Text(2511.0, 271.79999999999995, &#39; n (...) n&#39;), Text(3069.0, 271.79999999999995, &#39; n (...) n&#39;), Text(3906.0, 815.4000000000001, &#39;Temp9am &lt;= 0.37 ngini = 0.37 nsamples = 3662 nvalue = [1429, 4405] nclass = Yes&#39;), Text(3627.0, 271.79999999999995, &#39; n (...) n&#39;), Text(4185.0, 271.79999999999995, &#39; n (...) n&#39;)] . Random forest also assign importance to each feature, by combining the importance values from individual trees . imortance_df = pd.DataFrame({ &#39;feature&#39;: X_train.columns, &#39;importance&#39;: model.feature_importances_ }).sort_values(&#39;importance&#39;, ascending=False) . importance_df.head(10) . feature importance . 9 Humidity3pm | 0.261441 | . 11 Pressure3pm | 0.062057 | . 2 Rainfall | 0.059139 | . 5 WindGustSpeed | 0.056333 | . 4 Sunshine | 0.049465 | . 8 Humidity9am | 0.040218 | . 14 Temp9am | 0.035000 | . 0 MinTemp | 0.034894 | . 10 Pressure9am | 0.034415 | . 1 MaxTemp | 0.032361 | . plt.title(&#39;importance of features&#39;) sns.barplot(data=importance_df.head(10), x=&#39;importance&#39;, y=&#39;feature&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;importance of features&#39;}, xlabel=&#39;importance&#39;, ylabel=&#39;feature&#39;&gt; . Hyperparameter Tuning with Random Forests . Docs of RF Hyperparameters . ?RandomForestClassifier . base_model = RandomForestClassifier(random_state=42, n_jobs=-1).fit(X_train, train_targets) . base_train_acc = base_model.score(X_train, train_targets) base_train_acc . 0.9999494888269285 . base_val_acc = base_model.score(X_val, val_targets) base_val_acc . 0.8566537055307295 . base_accs = base_train_acc, base_val_acc . n_estimators . This argument controls the number of decision trees in the random forest. The default value is 100. For larger datasets, it helps to have a greater number of estimators. As a general rule, try to have as few estimators as needed. . 10 estimators . model = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=10) . model.fit(X_train, train_targets) . RandomForestClassifier(n_estimators=10, n_jobs=-1, random_state=42) . model.score(X_train, train_targets), model.score(X_val, val_targets) . (0.986958015112943, 0.8485868492832686) . base_accs . (0.9999494888269285, 0.8566537055307295) . 500 estimators . model = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=500) . model.fit(X_train, train_targets) . RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42) . model.score(X_train, train_targets), model.score(X_val, val_targets) . (0.9999797955307714, 0.8577563693343393) . base_accs . (0.9999494888269285, 0.8566537055307295) . max_depth and max_leaf_nodes . These arguments are passed directly to each decision tree, and control the maximum depth and max. no leaf nodes of each tree respectively. By default, no maximum depth is specified, which is why each tree has a training accuracy of 100%. You can specify a max_depth to reduce overfitting. . . Let&#39;s define a helper function test_params to make it easy to test hyperparameters. . def test_params(**params): model = RandomForestClassifier(random_state=42, n_jobs=-1, **params).fit(X_train, train_targets) return model.score(X_train, train_targets), model.score(X_val, val_targets) . test_params(max_depth=5, max_leaf_nodes=1024, n_estimators=1000) . (0.8231704853113508, 0.8279264116998433) . test_params(max_depth=26) . (0.9814826039519942, 0.8572340549010504) . test_params(max_leaf_nodes=2**5) . (0.8314341132258456, 0.833904010214149) . test_params(max_leaf_nodes=2**20) . (0.9999494888269285, 0.8556671116011839) . base_accs . (0.9999494888269285, 0.8566537055307295) . max_features . Instead of picking all features (columns) for every split, we can specify that only a fraction of features be chosen randomly to figure out a split. . . Notice that the default value auto causes only $ sqrt{n}$ out of total features ( $n$ ) to be chosen randomly at each split. This is the reason each decision tree in the forest is different. While it may seem counterintuitive, choosing all features for every split of every tree will lead to identical trees, so the random forest will not generalize well. . test_params(max_features=&#39;log2&#39;) . (0.9999595910615429, 0.8558992513493123) . test_params(max_features=3) . (0.9999494888269285, 0.8543323080494458) . test_params(max_features=20) . (0.9999595910615429, 0.8565956705936975) . base_accs . (0.9999494888269285, 0.8566537055307295) . min_samples_split and min_sample_leaf . By default, the decision tree classifier tries to split every node that has 2 or more. You can increase the values of these arguments to change this behavior and reduce overfitting, especially for very large datasets. . test_params(min_samples_split=3, min_samples_leaf=2) . (0.9625005051117307, 0.8565956705936975) . test_params(min_samples_split=100, min_samples_leaf=60) . (0.8495676243585081, 0.8451047530613429) . base_accs . (0.9999494888269285, 0.8566537055307295) . min_impurity_decrease . This argument is used to control the threshold for splitting nodes. A node will be split if this split induces a decrease of the impurity (Gini index) greater than or equal to this value. It&#39;s default value is 0, and you can increase it to reduce overfitting. . test_params(min_impurity_decrease=1e-7) . (0.9996060128500425, 0.8561313910974406) . test_params(min_impurity_decrease=1e-2) . (0.774891906089627, 0.7882885497069235) . base_accs . (0.9999494888269285, 0.8566537055307295) . bootstrap, max_samples . By default, a random forest doesn&#39;t use the entire dataset for training each decision tree. Instead it applies a technique called bootstrapping. For each tree, rows from the dataset are picked one by one randomly, with replacement i.e. some rows may not show up at all, while some rows may show up multiple times. . . Bootstrapping helps the random forest generalize better, because each decision tree only sees a fraction of th training set, and some rows randomly get higher weightage than others. . test_params(bootstrap=False) . (0.9999797955307714, 0.8567697754047937) . base_accs . (0.9999494888269285, 0.8566537055307295) . When bootstrapping is enabled, you can also control the number or fraction of rows to be considered for each bootstrap using max_samples. This can further generalize the model. . . test_params(max_samples=0.9) . (0.9997676486038711, 0.8565376356566653) . base_accs . (0.9999494888269285, 0.8566537055307295) . Learn more about bootstrapping here: https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710 . class_weight . model.classes_ . array([&#39;No&#39;, &#39;Yes&#39;], dtype=object) . test_params(class_weight=&#39;balanced&#39;) . (0.9999494888269285, 0.8543903429864779) . test_params(class_weight={&#39;No&#39;: 1, &#39;Yes&#39;: 2}) . (0.9999595910615429, 0.8558412164122802) . base_accs . (0.9999494888269285, 0.8566537055307295) . Putting it together . model = RandomForestClassifier(n_jobs=-1, random_state=42, n_estimators=500, max_features=7, max_depth=30, class_weight={&#39;No&#39;: 1, &#39;Yes&#39;: 1.5}) . model.fit(X_train, train_targets) . RandomForestClassifier(class_weight={&#39;No&#39;: 1, &#39;Yes&#39;: 1.5}, max_depth=30, max_features=7, n_estimators=500, n_jobs=-1, random_state=42) . model.score(X_train, train_targets), model.score(X_val, val_targets) . (0.9920192346547057, 0.8563054959085369) . base_accs . (0.9999494888269285, 0.8566537055307295) . model.score(X_test, test_targets) . 0.8451913451913452 . Making Predictions on New Inputs . def predict_input(model, single_input): input_df = pd.DataFrame([single_input]) input_df[numeric_cols] = imputer.transform(input_df[numeric_cols]) input_df[numeric_cols] = scaler.transform(input_df[numeric_cols]) input_df[encoded_cols] = encoder.transform(input_df[categorical_cols]) X_input = input_df[numeric_cols + encoded_cols] pred = model.predict(X_input)[0] prob = model.predict_proba(X_input)[0][list(model.classes_).index(pred)] return pred, prob . new_input = {&#39;Date&#39;: &#39;2021-06-19&#39;, &#39;Location&#39;: &#39;Launceston&#39;, &#39;MinTemp&#39;: 23.2, &#39;MaxTemp&#39;: 33.2, &#39;Rainfall&#39;: 10.2, &#39;Evaporation&#39;: 4.2, &#39;Sunshine&#39;: np.nan, &#39;WindGustDir&#39;: &#39;NNW&#39;, &#39;WindGustSpeed&#39;: 52.0, &#39;WindDir9am&#39;: &#39;NW&#39;, &#39;WindDir3pm&#39;: &#39;NNE&#39;, &#39;WindSpeed9am&#39;: 13.0, &#39;WindSpeed3pm&#39;: 20.0, &#39;Humidity9am&#39;: 89.0, &#39;Humidity3pm&#39;: 58.0, &#39;Pressure9am&#39;: 1004.8, &#39;Pressure3pm&#39;: 1001.5, &#39;Cloud9am&#39;: 8.0, &#39;Cloud3pm&#39;: 5.0, &#39;Temp9am&#39;: 25.7, &#39;Temp3pm&#39;: 33.0, &#39;RainToday&#39;: &#39;Yes&#39;} . predict_input(model, new_input) . (&#39;Yes&#39;, 0.7608595348304202) . raw_df.Location.unique() . array([&#39;Albury&#39;, &#39;BadgerysCreek&#39;, &#39;Cobar&#39;, &#39;CoffsHarbour&#39;, &#39;Moree&#39;, &#39;Newcastle&#39;, &#39;NorahHead&#39;, &#39;NorfolkIsland&#39;, &#39;Penrith&#39;, &#39;Richmond&#39;, &#39;Sydney&#39;, &#39;SydneyAirport&#39;, &#39;WaggaWagga&#39;, &#39;Williamtown&#39;, &#39;Wollongong&#39;, &#39;Canberra&#39;, &#39;Tuggeranong&#39;, &#39;MountGinini&#39;, &#39;Ballarat&#39;, &#39;Bendigo&#39;, &#39;Sale&#39;, &#39;MelbourneAirport&#39;, &#39;Melbourne&#39;, &#39;Mildura&#39;, &#39;Nhil&#39;, &#39;Portland&#39;, &#39;Watsonia&#39;, &#39;Dartmoor&#39;, &#39;Brisbane&#39;, &#39;Cairns&#39;, &#39;GoldCoast&#39;, &#39;Townsville&#39;, &#39;Adelaide&#39;, &#39;MountGambier&#39;, &#39;Nuriootpa&#39;, &#39;Woomera&#39;, &#39;Albany&#39;, &#39;Witchcliffe&#39;, &#39;PearceRAAF&#39;, &#39;PerthAirport&#39;, &#39;Perth&#39;, &#39;SalmonGums&#39;, &#39;Walpole&#39;, &#39;Hobart&#39;, &#39;Launceston&#39;, &#39;AliceSprings&#39;, &#39;Darwin&#39;, &#39;Katherine&#39;, &#39;Uluru&#39;], dtype=object) . Saving and Loading Trained Models . import joblib . aussie_rain = { &#39;model&#39;: model, &#39;imputer&#39;: imputer, &#39;scaler&#39;: scaler, &#39;encoder&#39;: encoder, &#39;input_cols&#39;: input_cols, &#39;target_col&#39;: target_cols, &#39;numeric_cols&#39;: numeric_cols, &#39;categorical_cols&#39;: categorical_cols, &#39;encoded_cols&#39;: encoded_cols } . joblib.dump(aussie_rain, &#39;aussie_rain.joblib&#39;) . [&#39;aussie_rain.joblib&#39;] . aussie_rain2 = joblib.load(&#39;aussie_rain.joblib&#39;) . test_preds2 = aussie_rain2[&#39;model&#39;].predict(X_test) accuracy_score(test_targets, test_preds2) . 0.8451913451913452 .",
            "url": "https://mr-siddy.github.io/ML-blog/ml/2021/05/17/Decision-Trees-Random-Forests.html",
            "relUrl": "/ml/2021/05/17/Decision-Trees-Random-Forests.html",
            "date": " • May 17, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Decision Trees using Ausstralia Rain Dataset",
            "content": "import opendatasets as od import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import seaborn as sns import os %matplotlib inline pd.set_option(&#39;display.max_columns&#39;, None) pd.set_option(&#39;display.max_rows&#39;, 150) sns.set_style(&#39;darkgrid&#39;) matplotlib.rcParams[&#39;font.size&#39;] = 14 matplotlib.rcParams[&#39;figure.figsize&#39;] = (10, 6) matplotlib.rcParams[&#39;figure.facecolor&#39;] = &#39;#00000000&#39; . os.listdir(&#39;weather-dataset-rattle-package&#39;) . [&#39;weatherAUS.csv&#39;] . raw_df = pd.read_csv(&#39;weather-dataset-rattle-package/weatherAUS.csv&#39;) . raw_df.head(10) . Date Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am WindDir3pm WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow . 0 2008-12-01 | Albury | 13.4 | 22.9 | 0.6 | NaN | NaN | W | 44.0 | W | WNW | 20.0 | 24.0 | 71.0 | 22.0 | 1007.7 | 1007.1 | 8.0 | NaN | 16.9 | 21.8 | No | No | . 1 2008-12-02 | Albury | 7.4 | 25.1 | 0.0 | NaN | NaN | WNW | 44.0 | NNW | WSW | 4.0 | 22.0 | 44.0 | 25.0 | 1010.6 | 1007.8 | NaN | NaN | 17.2 | 24.3 | No | No | . 2 2008-12-03 | Albury | 12.9 | 25.7 | 0.0 | NaN | NaN | WSW | 46.0 | W | WSW | 19.0 | 26.0 | 38.0 | 30.0 | 1007.6 | 1008.7 | NaN | 2.0 | 21.0 | 23.2 | No | No | . 3 2008-12-04 | Albury | 9.2 | 28.0 | 0.0 | NaN | NaN | NE | 24.0 | SE | E | 11.0 | 9.0 | 45.0 | 16.0 | 1017.6 | 1012.8 | NaN | NaN | 18.1 | 26.5 | No | No | . 4 2008-12-05 | Albury | 17.5 | 32.3 | 1.0 | NaN | NaN | W | 41.0 | ENE | NW | 7.0 | 20.0 | 82.0 | 33.0 | 1010.8 | 1006.0 | 7.0 | 8.0 | 17.8 | 29.7 | No | No | . 5 2008-12-06 | Albury | 14.6 | 29.7 | 0.2 | NaN | NaN | WNW | 56.0 | W | W | 19.0 | 24.0 | 55.0 | 23.0 | 1009.2 | 1005.4 | NaN | NaN | 20.6 | 28.9 | No | No | . 6 2008-12-07 | Albury | 14.3 | 25.0 | 0.0 | NaN | NaN | W | 50.0 | SW | W | 20.0 | 24.0 | 49.0 | 19.0 | 1009.6 | 1008.2 | 1.0 | NaN | 18.1 | 24.6 | No | No | . 7 2008-12-08 | Albury | 7.7 | 26.7 | 0.0 | NaN | NaN | W | 35.0 | SSE | W | 6.0 | 17.0 | 48.0 | 19.0 | 1013.4 | 1010.1 | NaN | NaN | 16.3 | 25.5 | No | No | . 8 2008-12-09 | Albury | 9.7 | 31.9 | 0.0 | NaN | NaN | NNW | 80.0 | SE | NW | 7.0 | 28.0 | 42.0 | 9.0 | 1008.9 | 1003.6 | NaN | NaN | 18.3 | 30.2 | No | Yes | . 9 2008-12-10 | Albury | 13.1 | 30.1 | 1.4 | NaN | NaN | W | 28.0 | S | SSE | 15.0 | 11.0 | 58.0 | 27.0 | 1007.0 | 1005.7 | NaN | NaN | 20.1 | 28.2 | Yes | No | . raw_df.shape . (145460, 23) . raw_df.info() # to check column types of dataset . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 145460 entries, 0 to 145459 Data columns (total 23 columns): # Column Non-Null Count Dtype -- -- 0 Date 145460 non-null object 1 Location 145460 non-null object 2 MinTemp 143975 non-null float64 3 MaxTemp 144199 non-null float64 4 Rainfall 142199 non-null float64 5 Evaporation 82670 non-null float64 6 Sunshine 75625 non-null float64 7 WindGustDir 135134 non-null object 8 WindGustSpeed 135197 non-null float64 9 WindDir9am 134894 non-null object 10 WindDir3pm 141232 non-null object 11 WindSpeed9am 143693 non-null float64 12 WindSpeed3pm 142398 non-null float64 13 Humidity9am 142806 non-null float64 14 Humidity3pm 140953 non-null float64 15 Pressure9am 130395 non-null float64 16 Pressure3pm 130432 non-null float64 17 Cloud9am 89572 non-null float64 18 Cloud3pm 86102 non-null float64 19 Temp9am 143693 non-null float64 20 Temp3pm 141851 non-null float64 21 RainToday 142199 non-null object 22 RainTomorrow 142193 non-null object dtypes: float64(16), object(7) memory usage: 25.5+ MB . raw_df.dropna(subset=[&#39;RainTomorrow&#39;], inplace=True) . raw_df.head(2) . Date Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am WindDir3pm WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow . 0 2008-12-01 | Albury | 13.4 | 22.9 | 0.6 | NaN | NaN | W | 44.0 | W | WNW | 20.0 | 24.0 | 71.0 | 22.0 | 1007.7 | 1007.1 | 8.0 | NaN | 16.9 | 21.8 | No | No | . 1 2008-12-02 | Albury | 7.4 | 25.1 | 0.0 | NaN | NaN | WNW | 44.0 | NNW | WSW | 4.0 | 22.0 | 44.0 | 25.0 | 1010.6 | 1007.8 | NaN | NaN | 17.2 | 24.3 | No | No | . raw_df.shape # shape has become 142193 . (142193, 23) . Training Validation and Test Sets . plt.title(&quot;no.of Rows per Year&quot;) sns.countplot(x=pd.to_datetime(raw_df.Date).dt.year); . year = pd.to_datetime(raw_df.Date).dt.year train_df = raw_df[year&lt;2015] val_df = raw_df[year==2015] test_df = raw_df[year&gt;2015] print(train_df.shape, val_df.shape, test_df.shape) . (98988, 23) (17231, 23) (25974, 23) . Input and Target Columns . input_cols = list(train_df.columns)[1:-1] target_cols = &#39;RainTomorrow&#39; . target_cols . &#39;RainTomorrow&#39; . input_cols . [&#39;Location&#39;, &#39;MinTemp&#39;, &#39;MaxTemp&#39;, &#39;Rainfall&#39;, &#39;Evaporation&#39;, &#39;Sunshine&#39;, &#39;WindGustDir&#39;, &#39;WindGustSpeed&#39;, &#39;WindDir9am&#39;, &#39;WindDir3pm&#39;, &#39;WindSpeed9am&#39;, &#39;WindSpeed3pm&#39;, &#39;Humidity9am&#39;, &#39;Humidity3pm&#39;, &#39;Pressure9am&#39;, &#39;Pressure3pm&#39;, &#39;Cloud9am&#39;, &#39;Cloud3pm&#39;, &#39;Temp9am&#39;, &#39;Temp3pm&#39;, &#39;RainToday&#39;] . train_inputs = train_df[input_cols].copy() train_targets = train_df[target_cols].copy() val_inputs = val_df[input_cols].copy() val_targets = val_df[target_cols].copy() test_inputs = test_df[input_cols].copy() test_targets = test_df[target_cols].copy() . numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist() categorical_cols = train_inputs.select_dtypes(&#39;object&#39;).columns.tolist() . print(numeric_cols) . [&#39;MinTemp&#39;, &#39;MaxTemp&#39;, &#39;Rainfall&#39;, &#39;Evaporation&#39;, &#39;Sunshine&#39;, &#39;WindGustSpeed&#39;, &#39;WindSpeed9am&#39;, &#39;WindSpeed3pm&#39;, &#39;Humidity9am&#39;, &#39;Humidity3pm&#39;, &#39;Pressure9am&#39;, &#39;Pressure3pm&#39;, &#39;Cloud9am&#39;, &#39;Cloud3pm&#39;, &#39;Temp9am&#39;, &#39;Temp3pm&#39;] . print(categorical_cols) . [&#39;Location&#39;, &#39;WindGustDir&#39;, &#39;WindDir9am&#39;, &#39;WindDir3pm&#39;, &#39;RainToday&#39;] . Imputing Missing Numeric Values . train_inputs[numeric_cols].isna().sum().sort_values(ascending=False) . Sunshine 40696 Evaporation 37110 Cloud3pm 36766 Cloud9am 35764 Pressure9am 9345 Pressure3pm 9309 WindGustSpeed 6902 Humidity9am 1265 Humidity3pm 1186 WindSpeed3pm 1140 WindSpeed9am 1133 Rainfall 1000 Temp9am 783 Temp3pm 663 MinTemp 434 MaxTemp 198 dtype: int64 . from sklearn.impute import SimpleImputer . imputer = SimpleImputer(strategy = &#39;mean&#39;).fit(raw_df[numeric_cols]) # imputer will figureout the avg for each of cols . train_inputs[numeric_cols] = imputer.transform(train_inputs[numeric_cols]) # fill empty data val_inputs[numeric_cols] = imputer.transform(val_inputs[numeric_cols]) test_inputs[numeric_cols] = imputer.transform(test_inputs[numeric_cols]) . train_inputs[numeric_cols].isna().sum() . MinTemp 0 MaxTemp 0 Rainfall 0 Evaporation 0 Sunshine 0 WindGustSpeed 0 WindSpeed9am 0 WindSpeed3pm 0 Humidity9am 0 Humidity3pm 0 Pressure9am 0 Pressure3pm 0 Cloud9am 0 Cloud3pm 0 Temp9am 0 Temp3pm 0 dtype: int64 . Scaling Numeric Features . from sklearn.preprocessing import MinMaxScaler . val_inputs.describe().loc[[&#39;min&#39;, &#39;max&#39;]] . MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustSpeed WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm . min -8.2 | -3.2 | 0.0 | 0.0 | 0.0 | 7.0 | 0.0 | 0.0 | 4.0 | 0.0 | 988.1 | 982.2 | 0.0 | 0.0 | -6.2 | -4.0 | . max 31.9 | 45.4 | 247.2 | 70.4 | 14.5 | 135.0 | 87.0 | 74.0 | 100.0 | 100.0 | 1039.3 | 1037.3 | 8.0 | 8.0 | 37.5 | 42.8 | . scaler = MinMaxScaler().fit(raw_df[numeric_cols]) . train_inputs[numeric_cols] = scaler.transform(train_inputs[numeric_cols]) val_inputs[numeric_cols] = scaler.transform(val_inputs[numeric_cols]) test_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols]) . val_inputs.describe().loc[[&#39;min&#39;, &#39;max&#39;]] . MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustSpeed WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm . min 0.007075 | 0.030246 | 0.000000 | 0.000000 | 0.0 | 0.007752 | 0.000000 | 0.000000 | 0.04 | 0.0 | 0.125620 | 0.0816 | 0.000000 | 0.000000 | 0.021097 | 0.026871 | . max 0.952830 | 0.948960 | 0.666307 | 0.485517 | 1.0 | 1.000000 | 0.669231 | 0.850575 | 1.00 | 1.0 | 0.971901 | 0.9632 | 0.888889 | 0.888889 | 0.943038 | 0.925144 | . Encoding Categorical Data . from sklearn.preprocessing import OneHotEncoder . train_df[categorical_cols].fillna(&#39;Unkown&#39;) val_df[categorical_cols].fillna(&#39;Unkown&#39;) test_df[categorical_cols].fillna(&#39;Unknown&#39;) . Location WindGustDir WindDir9am WindDir3pm RainToday . 2498 Albury | ENE | Unknown | ESE | No | . 2499 Albury | SSE | SSE | SE | No | . 2500 Albury | ENE | ESE | ENE | Yes | . 2501 Albury | SSE | SE | SSE | Yes | . 2502 Albury | ENE | SE | SSE | Yes | . ... ... | ... | ... | ... | ... | . 145454 Uluru | E | ESE | E | No | . 145455 Uluru | E | SE | ENE | No | . 145456 Uluru | NNW | SE | N | No | . 145457 Uluru | N | SE | WNW | No | . 145458 Uluru | SE | SSE | N | No | . 25974 rows × 5 columns . encoder = OneHotEncoder(sparse=False, handle_unknown=&#39;ignore&#39;).fit(raw_df[categorical_cols]) . encoded_cols = list(encoder.get_feature_names(categorical_cols)) . train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols]) val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols]) test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols]) . print(encoded_cols) . [&#39;Location_Adelaide&#39;, &#39;Location_Albany&#39;, &#39;Location_Albury&#39;, &#39;Location_AliceSprings&#39;, &#39;Location_BadgerysCreek&#39;, &#39;Location_Ballarat&#39;, &#39;Location_Bendigo&#39;, &#39;Location_Brisbane&#39;, &#39;Location_Cairns&#39;, &#39;Location_Canberra&#39;, &#39;Location_Cobar&#39;, &#39;Location_CoffsHarbour&#39;, &#39;Location_Dartmoor&#39;, &#39;Location_Darwin&#39;, &#39;Location_GoldCoast&#39;, &#39;Location_Hobart&#39;, &#39;Location_Katherine&#39;, &#39;Location_Launceston&#39;, &#39;Location_Melbourne&#39;, &#39;Location_MelbourneAirport&#39;, &#39;Location_Mildura&#39;, &#39;Location_Moree&#39;, &#39;Location_MountGambier&#39;, &#39;Location_MountGinini&#39;, &#39;Location_Newcastle&#39;, &#39;Location_Nhil&#39;, &#39;Location_NorahHead&#39;, &#39;Location_NorfolkIsland&#39;, &#39;Location_Nuriootpa&#39;, &#39;Location_PearceRAAF&#39;, &#39;Location_Penrith&#39;, &#39;Location_Perth&#39;, &#39;Location_PerthAirport&#39;, &#39;Location_Portland&#39;, &#39;Location_Richmond&#39;, &#39;Location_Sale&#39;, &#39;Location_SalmonGums&#39;, &#39;Location_Sydney&#39;, &#39;Location_SydneyAirport&#39;, &#39;Location_Townsville&#39;, &#39;Location_Tuggeranong&#39;, &#39;Location_Uluru&#39;, &#39;Location_WaggaWagga&#39;, &#39;Location_Walpole&#39;, &#39;Location_Watsonia&#39;, &#39;Location_Williamtown&#39;, &#39;Location_Witchcliffe&#39;, &#39;Location_Wollongong&#39;, &#39;Location_Woomera&#39;, &#39;WindGustDir_E&#39;, &#39;WindGustDir_ENE&#39;, &#39;WindGustDir_ESE&#39;, &#39;WindGustDir_N&#39;, &#39;WindGustDir_NE&#39;, &#39;WindGustDir_NNE&#39;, &#39;WindGustDir_NNW&#39;, &#39;WindGustDir_NW&#39;, &#39;WindGustDir_S&#39;, &#39;WindGustDir_SE&#39;, &#39;WindGustDir_SSE&#39;, &#39;WindGustDir_SSW&#39;, &#39;WindGustDir_SW&#39;, &#39;WindGustDir_W&#39;, &#39;WindGustDir_WNW&#39;, &#39;WindGustDir_WSW&#39;, &#39;WindGustDir_nan&#39;, &#39;WindDir9am_E&#39;, &#39;WindDir9am_ENE&#39;, &#39;WindDir9am_ESE&#39;, &#39;WindDir9am_N&#39;, &#39;WindDir9am_NE&#39;, &#39;WindDir9am_NNE&#39;, &#39;WindDir9am_NNW&#39;, &#39;WindDir9am_NW&#39;, &#39;WindDir9am_S&#39;, &#39;WindDir9am_SE&#39;, &#39;WindDir9am_SSE&#39;, &#39;WindDir9am_SSW&#39;, &#39;WindDir9am_SW&#39;, &#39;WindDir9am_W&#39;, &#39;WindDir9am_WNW&#39;, &#39;WindDir9am_WSW&#39;, &#39;WindDir9am_nan&#39;, &#39;WindDir3pm_E&#39;, &#39;WindDir3pm_ENE&#39;, &#39;WindDir3pm_ESE&#39;, &#39;WindDir3pm_N&#39;, &#39;WindDir3pm_NE&#39;, &#39;WindDir3pm_NNE&#39;, &#39;WindDir3pm_NNW&#39;, &#39;WindDir3pm_NW&#39;, &#39;WindDir3pm_S&#39;, &#39;WindDir3pm_SE&#39;, &#39;WindDir3pm_SSE&#39;, &#39;WindDir3pm_SSW&#39;, &#39;WindDir3pm_SW&#39;, &#39;WindDir3pm_W&#39;, &#39;WindDir3pm_WNW&#39;, &#39;WindDir3pm_WSW&#39;, &#39;WindDir3pm_nan&#39;, &#39;RainToday_No&#39;, &#39;RainToday_Yes&#39;, &#39;RainToday_nan&#39;] . train_inputs.head(10) . Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am WindDir3pm WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm RainToday Location_Adelaide Location_Albany Location_Albury Location_AliceSprings Location_BadgerysCreek Location_Ballarat Location_Bendigo Location_Brisbane Location_Cairns Location_Canberra Location_Cobar Location_CoffsHarbour Location_Dartmoor Location_Darwin Location_GoldCoast Location_Hobart Location_Katherine Location_Launceston Location_Melbourne Location_MelbourneAirport Location_Mildura Location_Moree Location_MountGambier Location_MountGinini Location_Newcastle Location_Nhil Location_NorahHead Location_NorfolkIsland Location_Nuriootpa Location_PearceRAAF Location_Penrith Location_Perth Location_PerthAirport Location_Portland Location_Richmond Location_Sale Location_SalmonGums Location_Sydney Location_SydneyAirport Location_Townsville Location_Tuggeranong Location_Uluru Location_WaggaWagga Location_Walpole Location_Watsonia Location_Williamtown Location_Witchcliffe Location_Wollongong Location_Woomera WindGustDir_E WindGustDir_ENE WindGustDir_ESE WindGustDir_N WindGustDir_NE WindGustDir_NNE WindGustDir_NNW WindGustDir_NW WindGustDir_S WindGustDir_SE WindGustDir_SSE WindGustDir_SSW WindGustDir_SW WindGustDir_W WindGustDir_WNW WindGustDir_WSW WindGustDir_nan WindDir9am_E WindDir9am_ENE WindDir9am_ESE WindDir9am_N WindDir9am_NE WindDir9am_NNE WindDir9am_NNW WindDir9am_NW WindDir9am_S WindDir9am_SE WindDir9am_SSE WindDir9am_SSW WindDir9am_SW WindDir9am_W WindDir9am_WNW WindDir9am_WSW WindDir9am_nan WindDir3pm_E WindDir3pm_ENE WindDir3pm_ESE WindDir3pm_N WindDir3pm_NE WindDir3pm_NNE WindDir3pm_NNW WindDir3pm_NW WindDir3pm_S WindDir3pm_SE WindDir3pm_SSE WindDir3pm_SSW WindDir3pm_SW WindDir3pm_W WindDir3pm_WNW WindDir3pm_WSW WindDir3pm_nan RainToday_No RainToday_Yes RainToday_nan . 0 Albury | 0.516509 | 0.523629 | 0.001617 | 0.037723 | 0.525852 | W | 0.294574 | W | WNW | 0.153846 | 0.275862 | 0.71 | 0.22 | 0.449587 | 0.4800 | 0.888889 | 0.500352 | 0.508439 | 0.522073 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 1 Albury | 0.375000 | 0.565217 | 0.000000 | 0.037723 | 0.525852 | WNW | 0.294574 | NNW | WSW | 0.030769 | 0.252874 | 0.44 | 0.25 | 0.497521 | 0.4912 | 0.493021 | 0.500352 | 0.514768 | 0.570058 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2 Albury | 0.504717 | 0.576560 | 0.000000 | 0.037723 | 0.525852 | WSW | 0.310078 | W | WSW | 0.146154 | 0.298851 | 0.38 | 0.30 | 0.447934 | 0.5056 | 0.493021 | 0.222222 | 0.594937 | 0.548944 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 3 Albury | 0.417453 | 0.620038 | 0.000000 | 0.037723 | 0.525852 | NE | 0.139535 | SE | E | 0.084615 | 0.103448 | 0.45 | 0.16 | 0.613223 | 0.5712 | 0.493021 | 0.500352 | 0.533755 | 0.612284 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 4 Albury | 0.613208 | 0.701323 | 0.002695 | 0.037723 | 0.525852 | W | 0.271318 | ENE | NW | 0.053846 | 0.229885 | 0.82 | 0.33 | 0.500826 | 0.4624 | 0.777778 | 0.888889 | 0.527426 | 0.673704 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 5 Albury | 0.544811 | 0.652174 | 0.000539 | 0.037723 | 0.525852 | WNW | 0.387597 | W | W | 0.146154 | 0.275862 | 0.55 | 0.23 | 0.474380 | 0.4528 | 0.493021 | 0.500352 | 0.586498 | 0.658349 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 6 Albury | 0.537736 | 0.563327 | 0.000000 | 0.037723 | 0.525852 | W | 0.341085 | SW | W | 0.153846 | 0.275862 | 0.49 | 0.19 | 0.480992 | 0.4976 | 0.111111 | 0.500352 | 0.533755 | 0.575816 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 7 Albury | 0.382075 | 0.595463 | 0.000000 | 0.037723 | 0.525852 | W | 0.224806 | SSE | W | 0.046154 | 0.195402 | 0.48 | 0.19 | 0.543802 | 0.5280 | 0.493021 | 0.500352 | 0.495781 | 0.593090 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 8 Albury | 0.429245 | 0.693762 | 0.000000 | 0.037723 | 0.525852 | NNW | 0.573643 | SE | NW | 0.053846 | 0.321839 | 0.42 | 0.09 | 0.469421 | 0.4240 | 0.493021 | 0.500352 | 0.537975 | 0.683301 | No | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 9 Albury | 0.509434 | 0.659735 | 0.003774 | 0.037723 | 0.525852 | W | 0.170543 | S | SSE | 0.115385 | 0.126437 | 0.58 | 0.27 | 0.438017 | 0.4576 | 0.493021 | 0.500352 | 0.575949 | 0.644914 | Yes | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | . X_train = train_inputs[numeric_cols + encoded_cols] X_val = val_inputs[numeric_cols + encoded_cols] X_test = test_inputs[numeric_cols + encoded_cols] . X_test.head(10) . MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustSpeed WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm Location_Adelaide Location_Albany Location_Albury Location_AliceSprings Location_BadgerysCreek Location_Ballarat Location_Bendigo Location_Brisbane Location_Cairns Location_Canberra Location_Cobar Location_CoffsHarbour Location_Dartmoor Location_Darwin Location_GoldCoast Location_Hobart Location_Katherine Location_Launceston Location_Melbourne Location_MelbourneAirport Location_Mildura Location_Moree Location_MountGambier Location_MountGinini Location_Newcastle Location_Nhil Location_NorahHead Location_NorfolkIsland Location_Nuriootpa Location_PearceRAAF Location_Penrith Location_Perth Location_PerthAirport Location_Portland Location_Richmond Location_Sale Location_SalmonGums Location_Sydney Location_SydneyAirport Location_Townsville Location_Tuggeranong Location_Uluru Location_WaggaWagga Location_Walpole Location_Watsonia Location_Williamtown Location_Witchcliffe Location_Wollongong Location_Woomera WindGustDir_E WindGustDir_ENE WindGustDir_ESE WindGustDir_N WindGustDir_NE WindGustDir_NNE WindGustDir_NNW WindGustDir_NW WindGustDir_S WindGustDir_SE WindGustDir_SSE WindGustDir_SSW WindGustDir_SW WindGustDir_W WindGustDir_WNW WindGustDir_WSW WindGustDir_nan WindDir9am_E WindDir9am_ENE WindDir9am_ESE WindDir9am_N WindDir9am_NE WindDir9am_NNE WindDir9am_NNW WindDir9am_NW WindDir9am_S WindDir9am_SE WindDir9am_SSE WindDir9am_SSW WindDir9am_SW WindDir9am_W WindDir9am_WNW WindDir9am_WSW WindDir9am_nan WindDir3pm_E WindDir3pm_ENE WindDir3pm_ESE WindDir3pm_N WindDir3pm_NE WindDir3pm_NNE WindDir3pm_NNW WindDir3pm_NW WindDir3pm_S WindDir3pm_SE WindDir3pm_SSE WindDir3pm_SSW WindDir3pm_SW WindDir3pm_W WindDir3pm_WNW WindDir3pm_WSW WindDir3pm_nan RainToday_No RainToday_Yes RainToday_nan . 2498 0.681604 | 0.801512 | 0.000000 | 0.037723 | 0.525852 | 0.372093 | 0.000000 | 0.080460 | 0.46 | 0.17 | 0.543802 | 0.5136 | 0.777778 | 0.333333 | 0.702532 | 0.808061 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2499 0.693396 | 0.725898 | 0.001078 | 0.037723 | 0.525852 | 0.341085 | 0.069231 | 0.195402 | 0.54 | 0.30 | 0.505785 | 0.5008 | 0.888889 | 0.888889 | 0.675105 | 0.712092 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2500 0.634434 | 0.527410 | 0.005930 | 0.037723 | 0.525852 | 0.325581 | 0.084615 | 0.448276 | 0.62 | 0.67 | 0.553719 | 0.6032 | 0.888889 | 0.888889 | 0.611814 | 0.477927 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | . 2501 0.608491 | 0.538752 | 0.042049 | 0.037723 | 0.525852 | 0.255814 | 0.069231 | 0.195402 | 0.74 | 0.65 | 0.618182 | 0.6304 | 0.888889 | 0.888889 | 0.556962 | 0.518234 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | . 2502 0.566038 | 0.523629 | 0.018329 | 0.037723 | 0.525852 | 0.193798 | 0.046154 | 0.103448 | 0.92 | 0.63 | 0.591736 | 0.5888 | 0.888889 | 0.888889 | 0.514768 | 0.529750 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | . 2503 0.601415 | 0.621928 | 0.000539 | 0.037723 | 0.525852 | 0.255814 | 0.069231 | 0.126437 | 0.76 | 0.52 | 0.563636 | 0.5680 | 0.888889 | 0.888889 | 0.580169 | 0.596929 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2504 0.587264 | 0.620038 | 0.000000 | 0.037723 | 0.525852 | 0.224806 | 0.153846 | 0.229885 | 0.46 | 0.31 | 0.609917 | 0.6176 | 0.493021 | 0.222222 | 0.592827 | 0.614203 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2505 0.537736 | 0.689981 | 0.000000 | 0.037723 | 0.525852 | 0.139535 | 0.084615 | 0.068966 | 0.63 | 0.24 | 0.646281 | 0.6416 | 0.493021 | 0.888889 | 0.561181 | 0.654511 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2506 0.594340 | 0.752363 | 0.000000 | 0.037723 | 0.525852 | 0.170543 | 0.084615 | 0.103448 | 0.52 | 0.24 | 0.629752 | 0.6144 | 0.493021 | 0.333333 | 0.662447 | 0.738964 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . 2507 0.620283 | 0.790170 | 0.000000 | 0.037723 | 0.525852 | 0.271318 | 0.069231 | 0.195402 | 0.54 | 0.17 | 0.596694 | 0.5680 | 0.493021 | 0.500352 | 0.704641 | 0.798464 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | . Training and Visualizing Decision Trees . . useful blog :Visualize a Decision Tree in 4 Ways with Scikit-Learn and Python . Training . from sklearn.tree import DecisionTreeClassifier . model = DecisionTreeClassifier(random_state=42) # random state is provided to get same value each time . %%time model.fit(X_train, train_targets) . CPU times: user 1.9 s, sys: 9.74 ms, total: 1.91 s Wall time: 1.92 s . DecisionTreeClassifier(random_state=42) . Evaluation . from sklearn.metrics import accuracy_score, confusion_matrix . train_preds = model.predict(X_train) . train_preds . array([&#39;No&#39;, &#39;No&#39;, &#39;No&#39;, ..., &#39;No&#39;, &#39;No&#39;, &#39;No&#39;], dtype=object) . pd.value_counts(train_preds) . No 76707 Yes 22281 dtype: int64 . Decision tree also returns probabilities of each prediction . train_probs = model.predict_proba(X_train) . train_probs . array([[1., 0.], [1., 0.], [1., 0.], ..., [1., 0.], [1., 0.], [1., 0.]]) . train_targets . 0 No 1 No 2 No 3 No 4 No .. 144548 No 144549 No 144550 No 144551 No 144552 No Name: RainTomorrow, Length: 98988, dtype: object . accuracy_score(train_preds, train_targets) . 0.9999797955307714 . model.score(X_val, val_targets) # direct prediction on val inputs and compare accuracy #only ~79% . 0.7921188555510418 . val_targets.value_counts() / len(val_targets) . No 0.788289 Yes 0.211711 Name: RainTomorrow, dtype: float64 . It appears that the model has learned the training examples perfect, and doesn&#39;t generalize well to previously unseen examples. This phenomenon is called &quot;overfitting&quot;, and reducing overfitting is one of the most important parts of any machine learning project. . Visualizing Tree . from sklearn.tree import plot_tree, export_text . plt.figure(figsize=(80, 40)) plot_tree(model, feature_names=X_train.columns, max_depth=2, filled=True) . [Text(2232.0, 1902.6000000000001, &#39;Humidity3pm &lt;= 0.715 ngini = 0.349 nsamples = 98988 nvalue = [76705, 22283]&#39;), Text(1116.0, 1359.0, &#39;Rainfall &lt;= 0.004 ngini = 0.248 nsamples = 82418 nvalue = [70439, 11979]&#39;), Text(558.0, 815.4000000000001, &#39;Sunshine &lt;= 0.525 ngini = 0.198 nsamples = 69252 nvalue = [61538, 7714]&#39;), Text(279.0, 271.79999999999995, &#39; n (...) n&#39;), Text(837.0, 271.79999999999995, &#39; n (...) n&#39;), Text(1674.0, 815.4000000000001, &#39;Humidity3pm &lt;= 0.512 ngini = 0.438 nsamples = 13166 nvalue = [8901, 4265]&#39;), Text(1395.0, 271.79999999999995, &#39; n (...) n&#39;), Text(1953.0, 271.79999999999995, &#39; n (...) n&#39;), Text(3348.0, 1359.0, &#39;Humidity3pm &lt;= 0.825 ngini = 0.47 nsamples = 16570 nvalue = [6266, 10304]&#39;), Text(2790.0, 815.4000000000001, &#39;WindGustSpeed &lt;= 0.279 ngini = 0.499 nsamples = 9136 nvalue = [4804, 4332]&#39;), Text(2511.0, 271.79999999999995, &#39; n (...) n&#39;), Text(3069.0, 271.79999999999995, &#39; n (...) n&#39;), Text(3906.0, 815.4000000000001, &#39;Rainfall &lt;= 0.01 ngini = 0.316 nsamples = 7434 nvalue = [1462, 5972]&#39;), Text(3627.0, 271.79999999999995, &#39; n (...) n&#39;), Text(4185.0, 271.79999999999995, &#39; n (...) n&#39;)] . How a Decision Tree is Created . Note the gini value in each box. This is the loss function used by the decision tree to decide which column should be used for splitting the data, and at what point the column should be split. A lower Gini index indicates a better split. A perfect split (only one class on each side) has a Gini index of 0. . For a mathematical discussion of the Gini Index, watch this video: It has the following formula: . . Conceptually speaking, while training the models evaluates all possible splits across all possible columns and picks the best one. Then, it recursively performs an optimal split for the two portions. In practice, however, it&#39;s very inefficient to check all possible splits, so the model uses a heuristic (predefined strategy) combined with some randomization. . Let&#39;s check the depth of the tree that was created. . model.tree_.max_depth . 48 . tree_text = export_text(model, max_depth=10, feature_names=list(X_train.columns)) print(tree_text[:5000]) . | Humidity3pm &lt;= 0.72 | | Rainfall &lt;= 0.00 | | | Sunshine &lt;= 0.52 | | | | Pressure3pm &lt;= 0.58 | | | | | WindGustSpeed &lt;= 0.36 | | | | | | Humidity3pm &lt;= 0.28 | | | | | | | WindDir9am_NE &lt;= 0.50 | | | | | | | | Location_Watsonia &lt;= 0.50 | | | | | | | | | Cloud9am &lt;= 0.83 | | | | | | | | | | WindSpeed3pm &lt;= 0.07 | | | | | | | | | | | Pressure3pm &lt;= 0.46 | | | | | | | | | | | | class: Yes | | | | | | | | | | | Pressure3pm &gt; 0.46 | | | | | | | | | | | | class: No | | | | | | | | | | WindSpeed3pm &gt; 0.07 | | | | | | | | | | | MinTemp &lt;= 0.32 | | | | | | | | | | | | truncated branch of depth 2 | | | | | | | | | | | MinTemp &gt; 0.32 | | | | | | | | | | | | truncated branch of depth 7 | | | | | | | | | Cloud9am &gt; 0.83 | | | | | | | | | | Cloud3pm &lt;= 0.42 | | | | | | | | | | | class: Yes | | | | | | | | | | Cloud3pm &gt; 0.42 | | | | | | | | | | | Rainfall &lt;= 0.00 | | | | | | | | | | | | truncated branch of depth 2 | | | | | | | | | | | Rainfall &gt; 0.00 | | | | | | | | | | | | class: Yes | | | | | | | | Location_Watsonia &gt; 0.50 | | | | | | | | | class: Yes | | | | | | | WindDir9am_NE &gt; 0.50 | | | | | | | | WindGustSpeed &lt;= 0.25 | | | | | | | | | class: No | | | | | | | | WindGustSpeed &gt; 0.25 | | | | | | | | | Pressure9am &lt;= 0.54 | | | | | | | | | | Evaporation &lt;= 0.09 | | | | | | | | | | | Location_AliceSprings &lt;= 0.50 | | | | | | | | | | | | truncated branch of depth 4 | | | | | | | | | | | Location_AliceSprings &gt; 0.50 | | | | | | | | | | | | class: Yes | | | | | | | | | | Evaporation &gt; 0.09 | | | | | | | | | | | WindGustDir_ENE &lt;= 0.50 | | | | | | | | | | | | class: Yes | | | | | | | | | | | WindGustDir_ENE &gt; 0.50 | | | | | | | | | | | | class: No | | | | | | | | | Pressure9am &gt; 0.54 | | | | | | | | | | Humidity3pm &lt;= 0.20 | | | | | | | | | | | class: Yes | | | | | | | | | | Humidity3pm &gt; 0.20 | | | | | | | | | | | Evaporation &lt;= 0.02 | | | | | | | | | | | | class: Yes | | | | | | | | | | | Evaporation &gt; 0.02 | | | | | | | | | | | | class: No | | | | | | Humidity3pm &gt; 0.28 | | | | | | | Sunshine &lt;= 0.05 | | | | | | | | WindGustSpeed &lt;= 0.25 | | | | | | | | | Evaporation &lt;= 0.01 | | | | | | | | | | WindGustSpeed &lt;= 0.23 | | | | | | | | | | | class: Yes | | | | | | | | | | WindGustSpeed &gt; 0.23 | | | | | | | | | | | class: No | | | | | | | | | Evaporation &gt; 0.01 | | | | | | | | | | Evaporation &lt;= 0.07 | | | | | | | | | | | Temp3pm &lt;= 0.34 | | | | | | | | | | | | class: Yes | | | | | | | | | | | Temp3pm &gt; 0.34 | | | | | | | | | | | | truncated branch of depth 11 | | | | | | | | | | Evaporation &gt; 0.07 | | | | | | | | | | | WindSpeed9am &lt;= 0.12 | | | | | | | | | | | | class: Yes | | | | | | | | | | | WindSpeed9am &gt; 0.12 | | | | | | | | | | | | class: No | | | | | | | | WindGustSpeed &gt; 0.25 | | | | | | | | | Pressure9am &lt;= 0.56 | | | | | | | | | | MinTemp &lt;= 0.40 | | | | | | | | | | | WindDir9am_WNW &lt;= 0.50 | | | | | | | | | | | | class: Yes | | | | | | | | | | | WindDir9am_WNW &gt; 0.50 | | | | | | | | | | | | class: No | | | | | | | | | | MinTemp &gt; 0.40 | | | | | | | | | | | Humidity3pm &lt;= 0.66 | | | | | | | | | | | | truncated branch of depth 7 | | | | | | | | | | | Humidity3pm &gt; 0.66 | | | | | | | | | | | | truncated branch of depth 4 | | | | | | | | | Pressure9am &gt; 0.56 | | | | | . Feature Importance . X_train.columns . Index([&#39;MinTemp&#39;, &#39;MaxTemp&#39;, &#39;Rainfall&#39;, &#39;Evaporation&#39;, &#39;Sunshine&#39;, &#39;WindGustSpeed&#39;, &#39;WindSpeed9am&#39;, &#39;WindSpeed3pm&#39;, &#39;Humidity9am&#39;, &#39;Humidity3pm&#39;, ... &#39;WindDir3pm_SSE&#39;, &#39;WindDir3pm_SSW&#39;, &#39;WindDir3pm_SW&#39;, &#39;WindDir3pm_W&#39;, &#39;WindDir3pm_WNW&#39;, &#39;WindDir3pm_WSW&#39;, &#39;WindDir3pm_nan&#39;, &#39;RainToday_No&#39;, &#39;RainToday_Yes&#39;, &#39;RainToday_nan&#39;], dtype=&#39;object&#39;, length=119) . model.feature_importances_ . array([3.48942086e-02, 3.23605486e-02, 5.91385668e-02, 2.49619907e-02, 4.94652143e-02, 5.63334673e-02, 2.80205998e-02, 2.98128801e-02, 4.02182908e-02, 2.61441297e-01, 3.44145027e-02, 6.20573699e-02, 1.36406176e-02, 1.69229866e-02, 3.50001550e-02, 3.04064076e-02, 2.24086587e-03, 2.08018104e-03, 1.27475954e-03, 7.26936324e-04, 1.39779517e-03, 1.15264873e-03, 6.92808159e-04, 1.80675598e-03, 1.08370901e-03, 1.19773895e-03, 8.87119103e-04, 2.15764220e-03, 1.67094731e-03, 7.98919493e-05, 1.10558668e-03, 1.42008656e-03, 4.10087635e-04, 1.09028115e-03, 1.44164766e-03, 9.08284767e-04, 1.05770304e-03, 6.18133455e-04, 1.80387272e-03, 2.10403527e-03, 2.74413333e-04, 7.31599405e-04, 1.35408990e-03, 1.54759332e-03, 1.30917564e-03, 1.07134670e-03, 8.36408023e-04, 1.62662229e-03, 1.00326116e-03, 2.16053455e-03, 8.46802258e-04, 1.88919081e-03, 9.29325203e-04, 1.29545157e-03, 1.27604831e-03, 5.12736888e-04, 1.38458902e-03, 3.97103931e-04, 1.03734689e-03, 1.44437047e-03, 1.75870184e-03, 1.42487857e-03, 2.78109569e-03, 2.00782698e-03, 2.80617652e-04, 1.61509734e-03, 1.64361598e-03, 2.36124112e-03, 3.05457932e-03, 2.33239534e-03, 2.78643875e-03, 2.16695261e-03, 3.41491352e-03, 2.30573542e-03, 2.28270604e-03, 2.34408118e-03, 2.26557332e-03, 2.54592702e-03, 2.75264499e-03, 2.83905192e-03, 2.49480561e-03, 1.54840338e-03, 2.50305095e-03, 2.53945388e-03, 2.28130055e-03, 3.80572180e-03, 2.58535069e-03, 3.10172224e-03, 2.54236791e-03, 2.50297796e-03, 2.06400988e-03, 2.52931192e-03, 2.07840517e-03, 1.77387278e-03, 1.78920555e-03, 2.77709687e-03, 2.42564566e-03, 2.26471887e-03, 1.73346117e-03, 2.23926957e-03, 2.47865244e-03, 2.31917387e-03, 3.21211861e-03, 2.92382975e-03, 2.24399274e-03, 3.68774754e-03, 3.87595982e-03, 3.20326068e-03, 2.53323550e-03, 2.40444844e-03, 2.26790411e-03, 2.19744009e-03, 2.28064147e-03, 2.88545323e-03, 2.05278867e-03, 1.12604304e-03, 2.86325849e-04, 1.32322128e-03, 1.72690480e-03]) . importance_df = pd.DataFrame({ &#39;feature&#39;: X_train.columns, &#39;importance&#39;: model.feature_importances_ }).sort_values(&#39;importance&#39;, ascending=False) . importance_df.head(10) . feature importance . 9 Humidity3pm | 0.261441 | . 11 Pressure3pm | 0.062057 | . 2 Rainfall | 0.059139 | . 5 WindGustSpeed | 0.056333 | . 4 Sunshine | 0.049465 | . 8 Humidity9am | 0.040218 | . 14 Temp9am | 0.035000 | . 0 MinTemp | 0.034894 | . 10 Pressure9am | 0.034415 | . 1 MaxTemp | 0.032361 | . plt.title(&#39;Feature Importance&#39;) sns.barplot(data=importance_df.head(10), x=&#39;importance&#39;, y=&#39;feature&#39;); . Hyperparameter Tuning and Overfitting . ?DecisionTreeClassifier . As we saw in the previous section, our decision tree classifier memorized all training examples, leading to a 100% training accuracy, while the validation accuracy was only marginally better than a dumb baseline model. This phenomenon is called overfitting, and in this section, we&#39;ll look at some strategies for reducing overfitting. The process of reducing overfitting is known as regularlization. . The DecisionTreeClassifier accepts several arguments, some of which can be modified to reduce overfitting. . These arguments are called hyperparameters because they must be configured manually (as opposed to the parameters within the model which are learned from the data. We&#39;ll explore a couple of hyperparameters: . max_depth | max_leaf_nodes | . max_depth . By reducing the maximum depth of the decision tree, we can prevent the tree from memorizing all training examples, which may lead to better generalization . model = DecisionTreeClassifier(max_depth=3, random_state=42) . model.fit(X_train, train_targets) . DecisionTreeClassifier(max_depth=3, random_state=42) . model.score(X_train, train_targets) . 0.8291308037337859 . model.score(X_val, val_targets) . 0.8334397307178921 . model.classes_ . array([&#39;No&#39;, &#39;Yes&#39;], dtype=object) . Great, while the training accuracy of the model has gone down, the validation accuracy of the model has increased significantly. . plt.figure(figsize=(80, 40)) plot_tree(model, feature_names=X_train.columns, filled=True, rounded=True, class_names=model.classes_) . [Text(2232.0, 1902.6000000000001, &#39;Humidity3pm &lt;= 0.715 ngini = 0.349 nsamples = 98988 nvalue = [76705, 22283] nclass = No&#39;), Text(1116.0, 1359.0, &#39;Rainfall &lt;= 0.004 ngini = 0.248 nsamples = 82418 nvalue = [70439, 11979] nclass = No&#39;), Text(558.0, 815.4000000000001, &#39;Sunshine &lt;= 0.525 ngini = 0.198 nsamples = 69252 nvalue = [61538, 7714] nclass = No&#39;), Text(279.0, 271.79999999999995, &#39;gini = 0.363 nsamples = 12620 nvalue = [9618, 3002] nclass = No&#39;), Text(837.0, 271.79999999999995, &#39;gini = 0.153 nsamples = 56632 nvalue = [51920, 4712] nclass = No&#39;), Text(1674.0, 815.4000000000001, &#39;Humidity3pm &lt;= 0.512 ngini = 0.438 nsamples = 13166 nvalue = [8901, 4265] nclass = No&#39;), Text(1395.0, 271.79999999999995, &#39;gini = 0.293 nsamples = 4299 nvalue = [3531, 768] nclass = No&#39;), Text(1953.0, 271.79999999999995, &#39;gini = 0.478 nsamples = 8867 nvalue = [5370, 3497] nclass = No&#39;), Text(3348.0, 1359.0, &#39;Humidity3pm &lt;= 0.825 ngini = 0.47 nsamples = 16570 nvalue = [6266, 10304] nclass = Yes&#39;), Text(2790.0, 815.4000000000001, &#39;WindGustSpeed &lt;= 0.279 ngini = 0.499 nsamples = 9136 nvalue = [4804, 4332] nclass = No&#39;), Text(2511.0, 271.79999999999995, &#39;gini = 0.472 nsamples = 5583 nvalue = [3457, 2126] nclass = No&#39;), Text(3069.0, 271.79999999999995, &#39;gini = 0.471 nsamples = 3553 nvalue = [1347, 2206] nclass = Yes&#39;), Text(3906.0, 815.4000000000001, &#39;Rainfall &lt;= 0.01 ngini = 0.316 nsamples = 7434 nvalue = [1462, 5972] nclass = Yes&#39;), Text(3627.0, 271.79999999999995, &#39;gini = 0.391 nsamples = 4360 nvalue = [1161, 3199] nclass = Yes&#39;), Text(4185.0, 271.79999999999995, &#39;gini = 0.177 nsamples = 3074 nvalue = [301, 2773] nclass = Yes&#39;)] . print(export_text(model, feature_names=list(X_train.columns))) . | Humidity3pm &lt;= 0.72 | | Rainfall &lt;= 0.00 | | | Sunshine &lt;= 0.52 | | | | class: No | | | Sunshine &gt; 0.52 | | | | class: No | | Rainfall &gt; 0.00 | | | Humidity3pm &lt;= 0.51 | | | | class: No | | | Humidity3pm &gt; 0.51 | | | | class: No | Humidity3pm &gt; 0.72 | | Humidity3pm &lt;= 0.82 | | | WindGustSpeed &lt;= 0.28 | | | | class: No | | | WindGustSpeed &gt; 0.28 | | | | class: Yes | | Humidity3pm &gt; 0.82 | | | Rainfall &lt;= 0.01 | | | | class: Yes | | | Rainfall &gt; 0.01 | | | | class: Yes . def max_depth_error(md): model = DecisionTreeClassifier(max_depth=md, random_state=42) model.fit(X_train, train_targets) train_error = 1 - model.score(X_train, train_targets) val_error = 1 - model.score(X_val, val_targets) return {&#39;Max Depth&#39;: md, &#39;Training Error&#39;: train_error, &#39;Validation Error&#39;: val_error} . %%time errors_df = pd.DataFrame([max_depth_error(md) for md in range(1, 21)]) . CPU times: user 30.3 s, sys: 203 ms, total: 30.5 s Wall time: 30.5 s . errors_df . Max Depth Training Error Validation Error . 0 1 | 0.184315 | 0.177935 | . 1 2 | 0.179547 | 0.172712 | . 2 3 | 0.170869 | 0.166560 | . 3 4 | 0.165707 | 0.164355 | . 4 5 | 0.160676 | 0.159074 | . 5 6 | 0.156271 | 0.157275 | . 6 7 | 0.153312 | 0.154605 | . 7 8 | 0.147806 | 0.158029 | . 8 9 | 0.140906 | 0.156578 | . 9 10 | 0.132945 | 0.157333 | . 10 11 | 0.123227 | 0.159248 | . 11 12 | 0.113489 | 0.160815 | . 12 13 | 0.101750 | 0.163833 | . 13 14 | 0.089981 | 0.167373 | . 14 15 | 0.078999 | 0.171261 | . 15 16 | 0.068180 | 0.174279 | . 16 17 | 0.058138 | 0.176890 | . 17 18 | 0.048733 | 0.181243 | . 18 19 | 0.040025 | 0.187569 | . 19 20 | 0.032539 | 0.190297 | . plt.figure() plt.plot(errors_df[&#39;Max Depth&#39;], errors_df[&#39;Training Error&#39;]) plt.plot(errors_df[&#39;Max Depth&#39;], errors_df[&#39;Validation Error&#39;]) plt.title(&quot;Training vs Validation Error&quot;) plt.xticks(range(0,21,2)) plt.xlabel(&#39;Max. Depth&#39;) plt.ylabel(&#39;Prediction Error ie 1-Accuracy&#39;) plt.legend([&#39;Training&#39;, &#39;Validation&#39;]) . &lt;matplotlib.legend.Legend at 0x7f037824ffa0&gt; . . So for us max depth of 7 results in lowest validation error . model = DecisionTreeClassifier(max_depth=7, random_state=42).fit(X_train, train_targets) model.score(X_val, val_targets), model.score(X_train, train_targets) . (0.8453949277465034, 0.8466884874934335) . max_leaf_nodes . Another way to control the size of complexity of a decision tree is to limit the number of leaf nodes. This allows branches of the tree to have varying depths. . model = DecisionTreeClassifier(max_leaf_nodes = 128, random_state = 42) . model.fit(X_train, train_targets) . DecisionTreeClassifier(max_leaf_nodes=128, random_state=42) . model.score(X_train, train_targets) . 0.8480421869317493 . model.score(X_val, val_targets) . 0.8442342290058615 . model.tree_.max_depth . 12 . Notice that the model was able to achieve a greater depth of 12 for certain paths while keeping other paths shorter. . model_text = export_text(model, feature_names = list(X_train.columns)) print(model_text[:3000]) . | Humidity3pm &lt;= 0.72 | | Rainfall &lt;= 0.00 | | | Sunshine &lt;= 0.52 | | | | Pressure3pm &lt;= 0.58 | | | | | WindGustSpeed &lt;= 0.36 | | | | | | Humidity3pm &lt;= 0.28 | | | | | | | class: No | | | | | | Humidity3pm &gt; 0.28 | | | | | | | Sunshine &lt;= 0.05 | | | | | | | | class: Yes | | | | | | | Sunshine &gt; 0.05 | | | | | | | | Pressure3pm &lt;= 0.43 | | | | | | | | | class: Yes | | | | | | | | Pressure3pm &gt; 0.43 | | | | | | | | | Humidity3pm &lt;= 0.57 | | | | | | | | | | WindDir9am_NE &lt;= 0.50 | | | | | | | | | | | WindDir9am_NNE &lt;= 0.50 | | | | | | | | | | | | class: No | | | | | | | | | | | WindDir9am_NNE &gt; 0.50 | | | | | | | | | | | | class: No | | | | | | | | | | WindDir9am_NE &gt; 0.50 | | | | | | | | | | | class: Yes | | | | | | | | | Humidity3pm &gt; 0.57 | | | | | | | | | | MaxTemp &lt;= 0.53 | | | | | | | | | | | class: No | | | | | | | | | | MaxTemp &gt; 0.53 | | | | | | | | | | | Temp3pm &lt;= 0.67 | | | | | | | | | | | | class: No | | | | | | | | | | | Temp3pm &gt; 0.67 | | | | | | | | | | | | class: No | | | | | WindGustSpeed &gt; 0.36 | | | | | | Humidity3pm &lt;= 0.45 | | | | | | | Sunshine &lt;= 0.39 | | | | | | | | class: No | | | | | | | Sunshine &gt; 0.39 | | | | | | | | class: No | | | | | | Humidity3pm &gt; 0.45 | | | | | | | Pressure3pm &lt;= 0.49 | | | | | | | | class: Yes | | | | | | | Pressure3pm &gt; 0.49 | | | | | | | | class: Yes | | | | Pressure3pm &gt; 0.58 | | | | | Pressure3pm &lt;= 0.70 | | | | | | Sunshine &lt;= 0.32 | | | | | | | WindDir9am_N &lt;= 0.50 | | | | | | | | Humidity3pm &lt;= 0.67 | | | | | | | | | class: No | | | | | | | | Humidity3pm &gt; 0.67 | | | | | | | | | class: No | | | | | | | WindDir9am_N &gt; 0.50 | | | | | | | | class: No | | | | | | Sunshine &gt; 0.32 | | | | | | | WindGustSpeed &lt;= 0.33 | | | | | | | | class: No | | | | | | | WindGustSpeed &gt; 0.33 | | | | | | | | class: No | | | | | Pressure3pm &gt; 0.70 | | | | | | Location_CoffsHarbour &lt;= 0.50 | | | | | | | class: No | | | | | | Location_CoffsHarbour &gt; 0.50 | | | | | | | class: No | | | Sunshine &gt; 0.52 | | | .",
            "url": "https://mr-siddy.github.io/ML-blog/ml/2021/05/16/Decision-Trees-AussRain.html",
            "relUrl": "/ml/2021/05/16/Decision-Trees-AussRain.html",
            "date": " • May 16, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Biot Savard Law using Plotly",
            "content": "import plotly import numpy as np import matplotlib.pyplot as plt from scipy.integrate import quad import plotly.graph_objects as go from plotly.offline import plot from IPython.core.display import display,HTML import sympy as smp from sympy.vector import cross . phi = np.linspace(0, 2*np.pi, 100) def l(phi): return (1+3/4 * np.sin(3*phi)) * np.array([np.cos(phi), np.sin(phi), np.zeros(len(phi))]) . lx, ly, lz = l(phi) . plt.figure(figsize=(7,7)) plt.plot(lx,ly) plt.xlabel(&#39;$x/R$&#39;, fontsize=25) plt.ylabel(&#39;$y/R$&#39;, fontsize=25) plt.show() . solve integrand using sympy . t, x, y, z = smp.symbols(&#39;t, x, y, z&#39;) . l = (1+(3/4)*smp.sin(3*t))*smp.Matrix([smp.cos(t), smp.sin(t), 0]) r = smp.Matrix([x,y,z]) sep=r-l . l . $ displaystyle left[ begin{matrix} left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)} left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)} 0 end{matrix} right]$ r . $ displaystyle left[ begin{matrix}x y z end{matrix} right]$ sep . $ displaystyle left[ begin{matrix}x - left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)} y - left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)} z end{matrix} right]$ Define integrand . integrand = smp.diff(l,t).cross(sep) / sep.norm()**3 . Get x, y z components of the integrand . integrand . $ displaystyle left[ begin{matrix} frac{z left( left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)} + 2.25 sin{ left(t right)} cos{ left(3 t right)} right)}{ left( left|{z} right|^{2} + left|{x - left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)}} right|^{2} + left|{y - left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)}} right|^{2} right)^{ frac{3}{2}}} - frac{z left(- left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)} + 2.25 cos{ left(t right)} cos{ left(3 t right)} right)}{ left( left|{z} right|^{2} + left|{x - left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)}} right|^{2} + left|{y - left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)}} right|^{2} right)^{ frac{3}{2}}} frac{- left(x - left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)} right) left( left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)} + 2.25 sin{ left(t right)} cos{ left(3 t right)} right) + left(y - left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)} right) left(- left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)} + 2.25 cos{ left(t right)} cos{ left(3 t right)} right)}{ left( left|{z} right|^{2} + left|{x - left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)}} right|^{2} + left|{y - left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)}} right|^{2} right)^{ frac{3}{2}}} end{matrix} right]$ dBxdt = smp.lambdify([t,x,y,z], integrand[0]) dBydt = smp.lambdify([t,x,y,z], integrand[1]) dBzdt = smp.lambdify([t,x,y,z], integrand[2]) . dBxdt(np.pi, 1, 1, 1) . -0.0680413817439772 . quad(dBxdt, 0, 2*np.pi, args=(1,1,1)) # gives value and error . (0.367215052854198, 6.916483780662426e-09) . Get the magnetic field by performing the integral over each component . def B(x,y,z): return np.array([quad(dBxdt, 0, 2*np.pi, args=(x,y,z))[0], quad(dBydt, 0, 2*np.pi, args=(x,y,z))[0], quad(dBzdt, 0, 2*np.pi, args=(x,y,z))[0]]) . B(0.5,0.5,0) . array([ 0. , 0. , 10.87779227]) . B(0.5,0.5,1) . array([0.19069963, 0.52786431, 1.52524645]) . Set up a meshgrid to solve for the field in some 3D volume . x = np.linspace(-2,2,20) xv,yv,zv = np.meshgrid(x,x,x) . B_field = np.vectorize(B, signature=&#39;(),(),()-&gt;(n)&#39;)(xv,yv,zv) Bx = B_field[:,:,:,0] By = B_field[:,:,:,1] Bz = B_field[:,:,:,2] . use plotly for 3D intractive plot . xv.ravel() . array([-2., -2., -2., ..., 2., 2., 2.]) . data = go.Cone(x=xv.ravel(), y=yv.ravel(), z=zv.ravel(), u=Bx.ravel(), v=By.ravel(), w=Bz.ravel(), colorscale=&#39;Inferno&#39;, colorbar=dict(title=&#39;$x^2$&#39;), sizemode=&quot;absolute&quot;, sizeref=20) layout = go.Layout(title=r&#39;Biot Savard Law &#39;, scene=dict(xaxis_title=r&#39;x&#39;, yaxis_title=r&#39;y&#39;, zaxis_title=r&#39;z&#39;, aspectratio=dict(x=1, y=1, z=1), camera_eye=dict(x=1.2, y=1.2, z=1.2))) fig = go.Figure(data = data, layout=layout) fig.add_scatter3d(x=lx, y=ly, z=lz, mode=&#39;lines&#39;, line = dict(color=&#39;green&#39;, width=10)) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/05/14/Biot-Savart-Law.html",
            "relUrl": "/2021/05/14/Biot-Savart-Law.html",
            "date": " • May 14, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Car Price Prediction using Random Forest Regressor",
            "content": "Problem Statement: . A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts. Which variables are significant in predicting the price of a car . Dataset link :- . https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho . Data Preprocessing . import pandas as pd . df = pd.read_csv(&#39;car data.csv&#39;) . df.head() . Car_Name Year Selling_Price Present_Price Kms_Driven Fuel_Type Seller_Type Transmission Owner . 0 ritz | 2014 | 3.35 | 5.59 | 27000 | Petrol | Dealer | Manual | 0 | . 1 sx4 | 2013 | 4.75 | 9.54 | 43000 | Diesel | Dealer | Manual | 0 | . 2 ciaz | 2017 | 7.25 | 9.85 | 6900 | Petrol | Dealer | Manual | 0 | . 3 wagon r | 2011 | 2.85 | 4.15 | 5200 | Petrol | Dealer | Manual | 0 | . 4 swift | 2014 | 4.60 | 6.87 | 42450 | Diesel | Dealer | Manual | 0 | . df.shape . (301, 9) . print(df[&quot;Seller_Type&quot;].unique()) print(df[&quot;Transmission&quot;].unique()) print(df[&quot;Owner&quot;].unique()) . [&#39;Dealer&#39; &#39;Individual&#39;] [&#39;Manual&#39; &#39;Automatic&#39;] [0 1 3] . df.isnull().sum() . Car_Name 0 Year 0 Selling_Price 0 Present_Price 0 Kms_Driven 0 Fuel_Type 0 Seller_Type 0 Transmission 0 Owner 0 dtype: int64 . df.describe() . Year Selling_Price Present_Price Kms_Driven Owner . count 301.000000 | 301.000000 | 301.000000 | 301.000000 | 301.000000 | . mean 2013.627907 | 4.661296 | 7.628472 | 36947.205980 | 0.043189 | . std 2.891554 | 5.082812 | 8.644115 | 38886.883882 | 0.247915 | . min 2003.000000 | 0.100000 | 0.320000 | 500.000000 | 0.000000 | . 25% 2012.000000 | 0.900000 | 1.200000 | 15000.000000 | 0.000000 | . 50% 2014.000000 | 3.600000 | 6.400000 | 32000.000000 | 0.000000 | . 75% 2016.000000 | 6.000000 | 9.900000 | 48767.000000 | 0.000000 | . max 2018.000000 | 35.000000 | 92.600000 | 500000.000000 | 3.000000 | . final_dataset=df[[&#39;Year&#39;, &#39;Selling_Price&#39;, &#39;Present_Price&#39;, &#39;Kms_Driven&#39;, &#39;Fuel_Type&#39;, &#39;Seller_Type&#39;, &#39;Transmission&#39;, &#39;Owner&#39;]] . final_dataset[&#39;Current_Year&#39;]=2021 . final_dataset[&#39;no_year&#39;]=final_dataset[&#39;Current_Year&#39;]-final_dataset[&#39;Year&#39;] . final_dataset.head() . Year Selling_Price Present_Price Kms_Driven Fuel_Type Seller_Type Transmission Owner Current_Year no_year . 0 2014 | 3.35 | 5.59 | 27000 | Petrol | Dealer | Manual | 0 | 2021 | 7 | . 1 2013 | 4.75 | 9.54 | 43000 | Diesel | Dealer | Manual | 0 | 2021 | 8 | . 2 2017 | 7.25 | 9.85 | 6900 | Petrol | Dealer | Manual | 0 | 2021 | 4 | . 3 2011 | 2.85 | 4.15 | 5200 | Petrol | Dealer | Manual | 0 | 2021 | 10 | . 4 2014 | 4.60 | 6.87 | 42450 | Diesel | Dealer | Manual | 0 | 2021 | 7 | . final_dataset.drop([&#39;Year&#39;],axis=1,inplace=True) . final_dataset.drop([&#39;Current_Year&#39;],axis=1,inplace=True) . final_dataset=pd.get_dummies(final_dataset,drop_first=True) . final_dataset.head() . Selling_Price Present_Price Kms_Driven Owner no_year Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . 0 3.35 | 5.59 | 27000 | 0 | 7 | 0 | 1 | 0 | 1 | . 1 4.75 | 9.54 | 43000 | 0 | 8 | 1 | 0 | 0 | 1 | . 2 7.25 | 9.85 | 6900 | 0 | 4 | 0 | 1 | 0 | 1 | . 3 2.85 | 4.15 | 5200 | 0 | 10 | 0 | 1 | 0 | 1 | . 4 4.60 | 6.87 | 42450 | 0 | 7 | 1 | 0 | 0 | 1 | . final_dataset.corr() . Selling_Price Present_Price Kms_Driven Owner no_year Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . Selling_Price 1.000000 | 0.878983 | 0.029187 | -0.088344 | -0.236141 | 0.552339 | -0.540571 | -0.550724 | -0.367128 | . Present_Price 0.878983 | 1.000000 | 0.203647 | 0.008057 | 0.047584 | 0.473306 | -0.465244 | -0.512030 | -0.348715 | . Kms_Driven 0.029187 | 0.203647 | 1.000000 | 0.089216 | 0.524342 | 0.172515 | -0.172874 | -0.101419 | -0.162510 | . Owner -0.088344 | 0.008057 | 0.089216 | 1.000000 | 0.182104 | -0.053469 | 0.055687 | 0.124269 | -0.050316 | . no_year -0.236141 | 0.047584 | 0.524342 | 0.182104 | 1.000000 | -0.064315 | 0.059959 | 0.039896 | -0.000394 | . Fuel_Type_Diesel 0.552339 | 0.473306 | 0.172515 | -0.053469 | -0.064315 | 1.000000 | -0.979648 | -0.350467 | -0.098643 | . Fuel_Type_Petrol -0.540571 | -0.465244 | -0.172874 | 0.055687 | 0.059959 | -0.979648 | 1.000000 | 0.358321 | 0.091013 | . Seller_Type_Individual -0.550724 | -0.512030 | -0.101419 | 0.124269 | 0.039896 | -0.350467 | 0.358321 | 1.000000 | 0.063240 | . Transmission_Manual -0.367128 | -0.348715 | -0.162510 | -0.050316 | -0.000394 | -0.098643 | 0.091013 | 0.063240 | 1.000000 | . Data-Visualisation . import seaborn as sns . sns.pairplot(final_dataset) . &lt;seaborn.axisgrid.PairGrid at 0x21f6bf7bb48&gt; . import matplotlib.pyplot as plt %matplotlib inline . corrmat = final_dataset.corr() top_corr_features=corrmat.index plt.figure(figsize=(20,20)) #plot heat map g = sns.heatmap(final_dataset[top_corr_features].corr(),annot=True, cmap=&#39;RdYlGn&#39;) . final_dataset.head() . Selling_Price Present_Price Kms_Driven Owner no_year Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . 0 3.35 | 5.59 | 27000 | 0 | 7 | 0 | 1 | 0 | 1 | . 1 4.75 | 9.54 | 43000 | 0 | 8 | 1 | 0 | 0 | 1 | . 2 7.25 | 9.85 | 6900 | 0 | 4 | 0 | 1 | 0 | 1 | . 3 2.85 | 4.15 | 5200 | 0 | 10 | 0 | 1 | 0 | 1 | . 4 4.60 | 6.87 | 42450 | 0 | 7 | 1 | 0 | 0 | 1 | . X=final_dataset.iloc[:,1:] y=final_dataset.iloc[:,0] . X.head() . Present_Price Kms_Driven Owner no_year Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . 0 5.59 | 27000 | 0 | 7 | 0 | 1 | 0 | 1 | . 1 9.54 | 43000 | 0 | 8 | 1 | 0 | 0 | 1 | . 2 9.85 | 6900 | 0 | 4 | 0 | 1 | 0 | 1 | . 3 4.15 | 5200 | 0 | 10 | 0 | 1 | 0 | 1 | . 4 6.87 | 42450 | 0 | 7 | 1 | 0 | 0 | 1 | . y.head() . 0 3.35 1 4.75 2 7.25 3 2.85 4 4.60 Name: Selling_Price, dtype: float64 . from sklearn.ensemble import ExtraTreesRegressor model=ExtraTreesRegressor() model.fit(X,y) . ExtraTreesRegressor() . print(model.feature_importances_) . [3.83348531e-01 4.08560046e-02 3.73461245e-04 7.61985654e-02 2.28609393e-01 1.08920232e-02 1.22151396e-01 1.37570626e-01] . feat_importances = pd.Series(model.feature_importances_, index=X.columns) feat_importances.nlargest(5).plot(kind=&#39;barh&#39;) plt.show() . Train Test Split . from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2) . X_train . Present_Price Kms_Driven Owner no_year Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . 174 0.72 | 38600 | 0 | 6 | 0 | 1 | 1 | 1 | . 144 0.99 | 25000 | 0 | 7 | 0 | 1 | 1 | 1 | . 212 13.60 | 22671 | 0 | 5 | 0 | 1 | 0 | 1 | . 196 0.52 | 500000 | 0 | 13 | 0 | 1 | 1 | 0 | . 243 7.60 | 7000 | 0 | 5 | 0 | 1 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 181 0.48 | 50000 | 0 | 5 | 0 | 1 | 1 | 1 | . 191 0.57 | 25000 | 1 | 9 | 0 | 1 | 1 | 1 | . 23 3.46 | 45280 | 0 | 7 | 0 | 1 | 0 | 1 | . 159 0.51 | 4000 | 0 | 4 | 0 | 1 | 1 | 0 | . 70 6.76 | 71000 | 0 | 7 | 1 | 0 | 0 | 1 | . 240 rows × 8 columns . X_train.shape . (240, 8) . ML model preparation . from sklearn.ensemble import RandomForestRegressor rf_random = RandomForestRegressor() . import numpy as np n_estimators=[int(x) for x in np.linspace(start = 100, stop = 1200, num =12 )] print(n_estimators) . [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200] . #Randomized Search CV # Number of trees in random forest n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)] # Number of features to consider at every split max_features = [&#39;auto&#39;, &#39;sqrt&#39;] # Maximum number of levels in tree max_depth = [int(x) for x in np.linspace(5, 30, num = 6)] # max_depth.append(None) # Minimum number of samples required to split a node min_samples_split = [2, 5, 10, 15, 100] # Minimum number of samples required at each leaf node min_samples_leaf = [1, 2, 5, 10] . random_grid = {&#39;n_estimators&#39;: n_estimators, &#39;max_features&#39;: max_features, &#39;max_depth&#39;: max_depth, &#39;min_samples_split&#39;: min_samples_split, &#39;min_samples_leaf&#39;: min_samples_leaf} print(random_grid) . {&#39;n_estimators&#39;: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200], &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;], &#39;max_depth&#39;: [5, 10, 15, 20, 25, 30], &#39;min_samples_split&#39;: [2, 5, 10, 15, 100], &#39;min_samples_leaf&#39;: [1, 2, 5, 10]} . # First create the base model to tune rf = RandomForestRegressor() . from sklearn.model_selection import RandomizedSearchCV . # Random search of parameters, using 3 fold cross validation, # search across 100 different combinations rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring=&#39;neg_mean_squared_error&#39;, n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1) . rf_random.fit(X_train,y_train) . Fitting 5 folds for each of 10 candidates, totalling 50 fits [CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time= 0.7s [CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time= 0.8s [CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time= 0.8s [CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time= 0.7s [CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time= 0.8s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time= 1.0s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time= 1.0s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time= 1.0s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time= 0.9s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time= 0.9s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time= 0.3s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time= 0.3s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time= 0.3s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time= 0.3s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time= 0.3s [CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time= 0.6s [CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 0.9s [CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 0.9s [CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 0.9s [CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 0.9s [CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 0.9s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time= 0.9s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time= 0.9s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time= 0.9s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time= 0.9s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time= 0.9s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time= 0.2s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time= 0.6s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time= 0.6s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time= 0.5s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time= 0.5s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time= 0.5s [CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time= 0.6s . RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=1, param_distributions={&#39;max_depth&#39;: [5, 10, 15, 20, 25, 30], &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;], &#39;min_samples_leaf&#39;: [1, 2, 5, 10], &#39;min_samples_split&#39;: [2, 5, 10, 15, 100], &#39;n_estimators&#39;: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200]}, random_state=42, scoring=&#39;neg_mean_squared_error&#39;, verbose=2) . Predictiong the Results . predictions = rf_random.predict(X_test) . predictions . array([ 0.42823578, 6.54336523, 4.51053508, 0.51499616, 5.25339526, 0.41187859, 3.59500812, 7.08705453, 1.1462021 , 21.3181618 , 0.24429459, 4.46844221, 10.77746653, 2.19624475, 2.87045466, 2.78251651, 5.56248702, 4.71332205, 0.64855331, 0.48048546, 1.16921385, 4.98697281, 0.86404259, 0.56195234, 0.26471753, 4.16548545, 4.66760447, 5.54659743, 6.3007795 , 0.54582494, 1.15449372, 7.65211645, 3.23066131, 4.9624319 , 0.54126424, 5.68049317, 3.02435804, 1.14775612, 8.57444514, 4.96809414, 0.63048064, 12.89946909, 13.48483701, 5.83942354, 0.57111473, 2.54267514, 7.08720208, 5.10635913, 5.07656358, 7.00626737, 5.68402798, 5.82973362, 3.03374272, 5.81018912, 21.1221698 , 2.4988902 , 2.56008469, 21.03405001, 5.37708202, 0.62723892, 0.4913718 ]) . sns.distplot(y_test-predictions) . C: Users mrsid anaconda3 envs carprediction lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;AxesSubplot:xlabel=&#39;Selling_Price&#39;, ylabel=&#39;Density&#39;&gt; . plt.scatter(y_test,predictions) . &lt;matplotlib.collections.PathCollection at 0x21f0eb52748&gt; . import pickle . file = open(&#39;random_forest_regression_model.pkl&#39;, &#39;wb&#39;) # dump information pickle.dump(rf_random, file) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/05/03/car-price-prediction.html",
            "relUrl": "/2021/05/03/car-price-prediction.html",
            "date": " • May 3, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Multicollinearity in Linear Regression",
            "content": "import pandas as pd . Using Advertising Dataset, link :- . https://www.kaggle.com/bumba5341/advertisingcsv . import statsmodels.api as sm df_adv = pd.read_csv(&#39;Advertising.csv&#39;, index_col=0) df_adv.head() . TV radio newspaper sales . 1 230.1 | 37.8 | 69.2 | 22.1 | . 2 44.5 | 39.3 | 45.1 | 10.4 | . 3 17.2 | 45.9 | 69.3 | 9.3 | . 4 151.5 | 41.3 | 58.5 | 18.5 | . 5 180.8 | 10.8 | 58.4 | 12.9 | . X = df_adv[[&#39;TV&#39;,&#39;radio&#39;,&#39;newspaper&#39;]] y = df_adv[&#39;sales&#39;] print(X,y) . TV radio newspaper 1 230.1 37.8 69.2 2 44.5 39.3 45.1 3 17.2 45.9 69.3 4 151.5 41.3 58.5 5 180.8 10.8 58.4 .. ... ... ... 196 38.2 3.7 13.8 197 94.2 4.9 8.1 198 177.0 9.3 6.4 199 283.6 42.0 66.2 200 232.1 8.6 8.7 [200 rows x 3 columns] 1 22.1 2 10.4 3 9.3 4 18.5 5 12.9 ... 196 7.6 197 9.7 198 12.8 199 25.5 200 13.4 Name: sales, Length: 200, dtype: float64 . fit a Oridinirary least square model with intercept on TV and Radio . X = sm.add_constant(X) . X . const TV radio newspaper . 1 1.0 | 230.1 | 37.8 | 69.2 | . 2 1.0 | 44.5 | 39.3 | 45.1 | . 3 1.0 | 17.2 | 45.9 | 69.3 | . 4 1.0 | 151.5 | 41.3 | 58.5 | . 5 1.0 | 180.8 | 10.8 | 58.4 | . ... ... | ... | ... | ... | . 196 1.0 | 38.2 | 3.7 | 13.8 | . 197 1.0 | 94.2 | 4.9 | 8.1 | . 198 1.0 | 177.0 | 9.3 | 6.4 | . 199 1.0 | 283.6 | 42.0 | 66.2 | . 200 1.0 | 232.1 | 8.6 | 8.7 | . 200 rows × 4 columns . model = sm.OLS(y, X).fit() . model.summary() # const indicates B0 value . OLS Regression Results Dep. Variable: sales | R-squared: 0.897 | . Model: OLS | Adj. R-squared: 0.896 | . Method: Least Squares | F-statistic: 570.3 | . Date: Tue, 27 Apr 2021 | Prob (F-statistic): 1.58e-96 | . Time: 19:40:31 | Log-Likelihood: -386.18 | . No. Observations: 200 | AIC: 780.4 | . Df Residuals: 196 | BIC: 793.6 | . Df Model: 3 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . const 2.9389 | 0.312 | 9.422 | 0.000 | 2.324 | 3.554 | . TV 0.0458 | 0.001 | 32.809 | 0.000 | 0.043 | 0.049 | . radio 0.1885 | 0.009 | 21.893 | 0.000 | 0.172 | 0.206 | . newspaper -0.0010 | 0.006 | -0.177 | 0.860 | -0.013 | 0.011 | . Omnibus: 60.414 | Durbin-Watson: 2.084 | . Prob(Omnibus): 0.000 | Jarque-Bera (JB): 151.241 | . Skew: -1.327 | Prob(JB): 1.44e-33 | . Kurtosis: 6.332 | Cond. No. 454. | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. import matplotlib.pyplot as plt import seaborn as sns X.iloc[:,1:].corr() . TV radio newspaper . TV 1.000000 | 0.054809 | 0.056648 | . radio 0.054809 | 1.000000 | 0.354104 | . newspaper 0.056648 | 0.354104 | 1.000000 | . plt.imshow(X,cmap=&#39;autumn&#39;) plt.show() . sns.heatmap(X,linewidth = 0.5 , cmap = &#39;coolwarm&#39;) plt.show() . Using Salary DataSet, link :- . https://github.com/mr-siddy/Machine-Learning/blob/master/Linear%20Regression/Salary_Data.csv . df_salary = pd.read_csv(&#39;Salary_Data.csv&#39;) df_salary.head() . YearsExperience Age Salary . 0 1.1 | 21.0 | 39343 | . 1 1.3 | 21.5 | 46205 | . 2 1.5 | 21.7 | 37731 | . 3 2.0 | 22.0 | 43525 | . 4 2.2 | 22.2 | 39891 | . X = df_salary[[&#39;YearsExperience&#39;,&#39;Age&#39;]] y = df_salary[&#39;Salary&#39;] . fit OLS model on y and X . X = sm.add_constant(X) model = sm.OLS(y,X).fit() . model.summary() # here observe R2, const, stderr and P&gt;|t| --&gt; high correlation . OLS Regression Results Dep. Variable: Salary | R-squared: 0.960 | . Model: OLS | Adj. R-squared: 0.957 | . Method: Least Squares | F-statistic: 323.9 | . Date: Tue, 27 Apr 2021 | Prob (F-statistic): 1.35e-19 | . Time: 19:58:08 | Log-Likelihood: -300.35 | . No. Observations: 30 | AIC: 606.7 | . Df Residuals: 27 | BIC: 610.9 | . Df Model: 2 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . const -6661.9872 | 2.28e+04 | -0.292 | 0.773 | -5.35e+04 | 4.02e+04 | . YearsExperience 6153.3533 | 2337.092 | 2.633 | 0.014 | 1358.037 | 1.09e+04 | . Age 1836.0136 | 1285.034 | 1.429 | 0.165 | -800.659 | 4472.686 | . Omnibus: 2.695 | Durbin-Watson: 1.711 | . Prob(Omnibus): 0.260 | Jarque-Bera (JB): 1.975 | . Skew: 0.456 | Prob(JB): 0.372 | . Kurtosis: 2.135 | Cond. No. 626. | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. X.iloc[:,1:].corr() . YearsExperience Age . YearsExperience 1.000000 | 0.987258 | . Age 0.987258 | 1.000000 | . sns.heatmap(X, cmap=&#39;summer&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x21443500808&gt; . How to Resolve . we check the P value and drop the feature which has higher p value . drop_age = X.drop(&#39;Age&#39;, axis=1) . model = sm.OLS(y,drop_age).fit() model.summary() . OLS Regression Results Dep. Variable: Salary | R-squared: 0.957 | . Model: OLS | Adj. R-squared: 0.955 | . Method: Least Squares | F-statistic: 622.5 | . Date: Tue, 27 Apr 2021 | Prob (F-statistic): 1.14e-20 | . Time: 20:07:01 | Log-Likelihood: -301.44 | . No. Observations: 30 | AIC: 606.9 | . Df Residuals: 28 | BIC: 609.7 | . Df Model: 1 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . const 2.579e+04 | 2273.053 | 11.347 | 0.000 | 2.11e+04 | 3.04e+04 | . YearsExperience 9449.9623 | 378.755 | 24.950 | 0.000 | 8674.119 | 1.02e+04 | . Omnibus: 2.140 | Durbin-Watson: 1.648 | . Prob(Omnibus): 0.343 | Jarque-Bera (JB): 1.569 | . Skew: 0.363 | Prob(JB): 0.456 | . Kurtosis: 2.147 | Cond. No. 13.2 | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/27/Multico-LR.html",
            "relUrl": "/2021/04/27/Multico-LR.html",
            "date": " • Apr 27, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Multiple Linear Regrassion using 50_Startups dataset",
            "content": "dataset link :- https://www.kaggle.com/farhanmd29/50-startups . Import Libraries and Creating Dataframe . import numpy as np import matplotlib.pyplot as plt import pandas as pd . dataset = pd.read_csv(&quot;50_Startups.csv&quot;,&quot;,&quot;) dataset.head() . R&amp;D Spend Administration Marketing Spend State Profit . 0 165349.20 | 136897.80 | 471784.10 | New York | 192261.83 | . 1 162597.70 | 151377.59 | 443898.53 | California | 191792.06 | . 2 153441.51 | 101145.55 | 407934.54 | Florida | 191050.39 | . 3 144372.41 | 118671.85 | 383199.62 | New York | 182901.99 | . 4 142107.34 | 91391.77 | 366168.42 | Florida | 166187.94 | . X = dataset.iloc[:, :-1] y = dataset.iloc[:,4] . print(X.head(),&quot; n&quot;) print(y.head()) . R&amp;D Spend Administration Marketing Spend State 0 165349.20 136897.80 471784.10 New York 1 162597.70 151377.59 443898.53 California 2 153441.51 101145.55 407934.54 Florida 3 144372.41 118671.85 383199.62 New York 4 142107.34 91391.77 366168.42 Florida 0 192261.83 1 191792.06 2 191050.39 3 182901.99 4 166187.94 Name: Profit, dtype: float64 . Data Preprocessing . states=pd.get_dummies(X[&#39;State&#39;],drop_first=True) #get_dummies helps to create dummy variables wrt no of categorial fratures # drop_first = True helps us to create dummy variable trap . X = X.drop(&#39;State&#39;,axis=1) . X . R&amp;D Spend Administration Marketing Spend . 0 165349.20 | 136897.80 | 471784.10 | . 1 162597.70 | 151377.59 | 443898.53 | . 2 153441.51 | 101145.55 | 407934.54 | . 3 144372.41 | 118671.85 | 383199.62 | . 4 142107.34 | 91391.77 | 366168.42 | . 5 131876.90 | 99814.71 | 362861.36 | . 6 134615.46 | 147198.87 | 127716.82 | . 7 130298.13 | 145530.06 | 323876.68 | . 8 120542.52 | 148718.95 | 311613.29 | . 9 123334.88 | 108679.17 | 304981.62 | . 10 101913.08 | 110594.11 | 229160.95 | . 11 100671.96 | 91790.61 | 249744.55 | . 12 93863.75 | 127320.38 | 249839.44 | . 13 91992.39 | 135495.07 | 252664.93 | . 14 119943.24 | 156547.42 | 256512.92 | . 15 114523.61 | 122616.84 | 261776.23 | . 16 78013.11 | 121597.55 | 264346.06 | . 17 94657.16 | 145077.58 | 282574.31 | . 18 91749.16 | 114175.79 | 294919.57 | . 19 86419.70 | 153514.11 | 0.00 | . 20 76253.86 | 113867.30 | 298664.47 | . 21 78389.47 | 153773.43 | 299737.29 | . 22 73994.56 | 122782.75 | 303319.26 | . 23 67532.53 | 105751.03 | 304768.73 | . 24 77044.01 | 99281.34 | 140574.81 | . 25 64664.71 | 139553.16 | 137962.62 | . 26 75328.87 | 144135.98 | 134050.07 | . 27 72107.60 | 127864.55 | 353183.81 | . 28 66051.52 | 182645.56 | 118148.20 | . 29 65605.48 | 153032.06 | 107138.38 | . 30 61994.48 | 115641.28 | 91131.24 | . 31 61136.38 | 152701.92 | 88218.23 | . 32 63408.86 | 129219.61 | 46085.25 | . 33 55493.95 | 103057.49 | 214634.81 | . 34 46426.07 | 157693.92 | 210797.67 | . 35 46014.02 | 85047.44 | 205517.64 | . 36 28663.76 | 127056.21 | 201126.82 | . 37 44069.95 | 51283.14 | 197029.42 | . 38 20229.59 | 65947.93 | 185265.10 | . 39 38558.51 | 82982.09 | 174999.30 | . 40 28754.33 | 118546.05 | 172795.67 | . 41 27892.92 | 84710.77 | 164470.71 | . 42 23640.93 | 96189.63 | 148001.11 | . 43 15505.73 | 127382.30 | 35534.17 | . 44 22177.74 | 154806.14 | 28334.72 | . 45 1000.23 | 124153.04 | 1903.93 | . 46 1315.46 | 115816.21 | 297114.46 | . 47 0.00 | 135426.92 | 0.00 | . 48 542.05 | 51743.15 | 0.00 | . 49 0.00 | 116983.80 | 45173.06 | . print(states.head()) . Florida New York 0 0 1 1 0 0 2 1 0 3 0 1 4 1 0 . X=pd.concat([X,states],axis=1) . print(X.head()) # Now we will apply y=b0+b1x1+b2x2+....... . R&amp;D Spend Administration Marketing Spend Florida New York 0 165349.20 136897.80 471784.10 0 1 1 162597.70 151377.59 443898.53 0 0 2 153441.51 101145.55 407934.54 1 0 3 144372.41 118671.85 383199.62 0 1 4 142107.34 91391.77 366168.42 1 0 . Train_test_split . from sklearn.model_selection import train_test_split . X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=0) . y_test . 28 103282.38 11 144259.40 10 146121.95 41 77798.83 2 191050.39 27 105008.31 38 81229.06 31 97483.56 22 110352.25 4 166187.94 Name: Profit, dtype: float64 . Applying Linear Regression . from sklearn.linear_model import LinearRegression . regressor = LinearRegression() regressor.fit(X_train, y_train) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . y_pred = regressor.predict(X_test) . y_pred . array([103015.20159796, 132582.27760816, 132447.73845174, 71976.09851258, 178537.48221055, 116161.24230165, 67851.69209676, 98791.73374687, 113969.43533012, 167921.0656955 ]) . from sklearn.metrics import r2_score # r2 = 1-(sum_of_residual/sum_of_mean) also model is good if r2 --&gt; 1 score= r2_score(y_test,y_pred) . score . 0.9347068473282423 .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/26/multiple-linear-regression.html",
            "relUrl": "/2021/04/26/multiple-linear-regression.html",
            "date": " • Apr 26, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Simple, Ridge and Lasso Linear Regression",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model import seaborn as sns . from sklearn.datasets import load_boston . df = load_boston() . df . {&#39;data&#39;: array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02, 4.9800e+00], [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02, 9.1400e+00], [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02, 4.0300e+00], ..., [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02, 5.6400e+00], [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02, 6.4800e+00], [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02, 7.8800e+00]]), &#39;target&#39;: array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. , 18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6, 15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2, 13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7, 21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9, 35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5, 19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. , 20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2, 23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8, 33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4, 21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. , 20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6, 23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4, 15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4, 17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7, 25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4, 23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. , 32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3, 34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4, 20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. , 26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3, 31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1, 22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6, 42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. , 36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4, 32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. , 20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1, 20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2, 22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1, 21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6, 19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7, 32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1, 18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8, 16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8, 13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3, 8.8, 7.2, 10.5, 7.4, 10.2, 11.5, 15.1, 23.2, 9.7, 13.8, 12.7, 13.1, 12.5, 8.5, 5. , 6.3, 5.6, 7.2, 12.1, 8.3, 8.5, 5. , 11.9, 27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3, 7. , 7.2, 7.5, 10.4, 8.8, 8.4, 16.7, 14.2, 20.8, 13.4, 11.7, 8.3, 10.2, 10.9, 11. , 9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4, 9.6, 8.7, 8.4, 12.8, 10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4, 15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7, 19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2, 29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8, 20.6, 21.2, 19.1, 20.6, 15.2, 7. , 8.1, 13.6, 20.1, 21.8, 24.5, 23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]), &#39;feature_names&#39;: array([&#39;CRIM&#39;, &#39;ZN&#39;, &#39;INDUS&#39;, &#39;CHAS&#39;, &#39;NOX&#39;, &#39;RM&#39;, &#39;AGE&#39;, &#39;DIS&#39;, &#39;RAD&#39;, &#39;TAX&#39;, &#39;PTRATIO&#39;, &#39;B&#39;, &#39;LSTAT&#39;], dtype=&#39;&lt;U7&#39;), &#39;DESCR&#39;: &#34;.. _boston_dataset: n nBoston house prices dataset n n n**Data Set Characteristics:** n n :Number of Instances: 506 n n :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. n n :Attribute Information (in order): n - CRIM per capita crime rate by town n - ZN proportion of residential land zoned for lots over 25,000 sq.ft. n - INDUS proportion of non-retail business acres per town n - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) n - NOX nitric oxides concentration (parts per 10 million) n - RM average number of rooms per dwelling n - AGE proportion of owner-occupied units built prior to 1940 n - DIS weighted distances to five Boston employment centres n - RAD index of accessibility to radial highways n - TAX full-value property-tax rate per $10,000 n - PTRATIO pupil-teacher ratio by town n - B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town n - LSTAT % lower status of the population n - MEDV Median value of owner-occupied homes in $1000&#39;s n n :Missing Attribute Values: None n n :Creator: Harrison, D. and Rubinfeld, D.L. n nThis is a copy of UCI ML housing dataset. nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/ n n nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. n nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. &#39;Hedonic nprices and the demand for clean air&#39;, J. Environ. Economics &amp; Management, nvol.5, 81-102, 1978. Used in Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics n...&#39;, Wiley, 1980. N.B. Various transformations are used in the table on npages 244-261 of the latter. n nThe Boston house-price data has been used in many machine learning papers that address regression nproblems. n n.. topic:: References n n - Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&#39;, Wiley, 1980. 244-261. n - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. n&#34;, &#39;filename&#39;: &#39;C: Users mrsid anaconda3 lib site-packages sklearn datasets data boston_house_prices.csv&#39;} . dataset= pd.DataFrame(df.data) print(dataset.head()) . 0 1 2 3 4 5 6 7 8 9 10 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 11 12 0 396.90 4.98 1 396.90 9.14 2 392.83 4.03 3 394.63 2.94 4 396.90 5.33 . dataset.head() . 0 1 2 3 4 5 6 7 8 9 10 11 12 . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | . dataset.columns=df.feature_names . dataset.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | . df.target.shape . (506,) . dataset[&quot;price&quot;]=df.target . dataset.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT price . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . X=dataset.iloc[:,:-1] # independent fratures y=dataset.iloc[:,-1] # dependent frature . Linear Regression . from sklearn.model_selection import cross_val_score from sklearn.linear_model import LinearRegression lin_regressor = LinearRegression() mse = cross_val_score(lin_regressor,X,y,scoring=&#39;neg_mean_squared_error&#39;,cv=5) #cv=crossvalidation mean_mse = np.mean(mse) print(mean_mse) # this value should be nearer to zero . -37.13180746769922 . Ridge Regression . from sklearn.linear_model import Ridge from sklearn.model_selection import GridSearchCV ridge=Ridge() parameters = {&#39;alpha&#39;:[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]} ridge_regressor=GridSearchCV(ridge,parameters,scoring = &#39;neg_mean_squared_error&#39;,cv=5) ridge_regressor.fit(X,y) . GridSearchCV(cv=5, error_score=nan, estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver=&#39;auto&#39;, tol=0.001), iid=&#39;deprecated&#39;, n_jobs=None, param_grid={&#39;alpha&#39;: [1e-15, 1e-10, 1e-08, 0.001, 0.01, 1, 5, 10, 20, 30, 35, 40, 45, 50, 55, 100]}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=&#39;neg_mean_squared_error&#39;, verbose=0) . print(ridge_regressor.best_params_) print(ridge_regressor.best_score_) . {&#39;alpha&#39;: 100} -29.905701947540365 . Lasso Regression . from sklearn.linear_model import Lasso from sklearn.model_selection import GridSearchCV lasso=Lasso() parameters = {&#39;alpha&#39;:[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]} Lasso_regressor=GridSearchCV(lasso,parameters,scoring = &#39;neg_mean_squared_error&#39;,cv=5) Lasso_regressor.fit(X,y) print(Lasso_regressor.best_params_) print(Lasso_regressor.best_score_) . {&#39;alpha&#39;: 1} -35.531580220694856 . C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4430.746729651311, tolerance: 3.9191485420792076 positive) C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4397.459304778431, tolerance: 3.3071316790123455 positive) C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3796.653037433508, tolerance: 2.813643886419753 positive) C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2564.292735790545, tolerance: 3.3071762123456794 positive) C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4294.252997826028, tolerance: 3.4809104444444445 positive) . from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0) . prediction_lasso=Lasso_regressor.predict(X_test) prediction_ridge=ridge_regressor.predict(X_test) . Plotting of results . import seaborn as sns sns.distplot(y_test-prediction_lasso) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x20e4370d588&gt; . sns.distplot(y_test-prediction_ridge) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x20e437ebf48&gt; . sns.distplot(y_test-mean_mse) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x20e438e2e48&gt; .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/26/Regression-on-Boston-Housing-Dataset.html",
            "relUrl": "/2021/04/26/Regression-on-Boston-Housing-Dataset.html",
            "date": " • Apr 26, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "EDA with Python and Applying Logistic Regression",
            "content": "Import Libraries . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline . The Data . train = pd.read_csv(&#39;titanic_train.csv&#39;) . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . Missing Data . train.isnull() # if True indicates a null value # but it is not a good way as data set can be vast . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 False | False | False | False | False | False | False | False | False | False | True | False | . 1 False | False | False | False | False | False | False | False | False | False | False | False | . 2 False | False | False | False | False | False | False | False | False | False | True | False | . 3 False | False | False | False | False | False | False | False | False | False | False | False | . 4 False | False | False | False | False | False | False | False | False | False | True | False | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 False | False | False | False | False | False | False | False | False | False | True | False | . 887 False | False | False | False | False | False | False | False | False | False | False | False | . 888 False | False | False | False | False | True | False | False | False | False | True | False | . 889 False | False | False | False | False | False | False | False | False | False | False | False | . 890 False | False | False | False | False | False | False | False | False | False | True | False | . 891 rows × 12 columns . sns.heatmap(train.isnull(),yticklabels = False, cbar =False, cmap=&#39;viridis&#39;) # so most of the null values are present in age and cabin . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a91cd84c8&gt; . sns.set_style(&#39;whitegrid&#39;) sns.countplot(x=&#39;Survived&#39;,data=train) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a91d397c8&gt; . sns.set_style(&#39;whitegrid&#39;) sns.countplot(x=&#39;Survived&#39;,hue=&#39;Sex&#39;,data=train, palette=&#39;RdBu_r&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92b67d88&gt; . sns.set_style(&#39;whitegrid&#39;) sns.countplot(x=&#39;Survived&#39;,hue=&#39;Pclass&#39;,data=train, palette=&#39;rainbow&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92c2ee88&gt; . sns.distplot(train[&#39;Age&#39;].dropna(), kde=False, color=&#39;darkred&#39;, bins=40) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92ca32c8&gt; . train[&#39;Age&#39;].hist(bins=30, color=&#39;blue&#39;,alpha=0.3) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92daeb88&gt; . sns.countplot(x=&#39;SibSp&#39;, data=train) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92e6f2c8&gt; . train[&#39;Fare&#39;].hist(bins=30,color=&#39;green&#39;,alpha=0.4) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92ed4e48&gt; . Data Cleaning . plt.figure(figsize=(12,7)) sns.boxplot(x=&#39;Pclass&#39;,y=&#39;Age&#39;,data=train,palette=&#39;winter&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92c2e5c8&gt; . we can see the wealthrer passengers in the higher classes tend to older, which makes sense, we&#39;ll use these average age values to impute based on pcalss for age . def impute_age(cols): Age = cols[0] Pclass = cols[1] if pd.isnull(Age): if Pclass == 1: return 37 elif Pclass == 2: return 29 else: return 24 else: return Age . train[&#39;Age&#39;] = train [[&#39;Age&#39;,&#39;Pclass&#39;]].apply(impute_age,axis=1 ) # now check heatmap again . sns.heatmap(train.isnull(),yticklabels = False, cbar =False, cmap=&#39;viridis&#39;) # so most of the null values are present in age and cabin . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a931ec508&gt; . we have to apply a lot of feature engineering to handle Cabin coz of a lot of Nan values hence we&#39;ll drop it for now . train.drop(&#39;Cabin&#39;,axis=1,inplace=True) . sns.heatmap(train.isnull(),yticklabels = False, cbar =False, cmap=&#39;viridis&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a93031ac8&gt; . train.dropna(inplace=True) . sns.heatmap(train.isnull(),yticklabels = False, cbar =False, cmap=&#39;viridis&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a930b6148&gt; . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | S | . Converting Categorical Features . we&#39;ll need to convert categorical features to dummy variables using pandas, otherwise our machine learning algorithm wont be able to directly take in those features as inputs . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 889 entries, 0 to 890 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 889 non-null int64 1 Survived 889 non-null int64 2 Pclass 889 non-null int64 3 Name 889 non-null object 4 Sex 889 non-null object 5 Age 889 non-null float64 6 SibSp 889 non-null int64 7 Parch 889 non-null int64 8 Ticket 889 non-null object 9 Fare 889 non-null float64 10 Embarked 889 non-null object dtypes: float64(2), int64(5), object(4) memory usage: 83.3+ KB . pd.get_dummies(train[&#39;Embarked&#39;],drop_first=True).head() . Q S . 0 0 | 1 | . 1 0 | 0 | . 2 0 | 1 | . 3 0 | 1 | . 4 0 | 1 | . sex = pd.get_dummies(train[&#39;Sex&#39;],drop_first=True) embark = pd.get_dummies(train[&#39;Embarked&#39;],drop_first=True) . train.drop([&#39;Sex&#39;,&#39;Embarked&#39;,&#39;Name&#39;,&#39;Ticket&#39;],axis=1,inplace=True) . train.head() . PassengerId Survived Pclass Age SibSp Parch Fare . 0 1 | 0 | 3 | 22.0 | 1 | 0 | 7.2500 | . 1 2 | 1 | 1 | 38.0 | 1 | 0 | 71.2833 | . 2 3 | 1 | 3 | 26.0 | 0 | 0 | 7.9250 | . 3 4 | 1 | 1 | 35.0 | 1 | 0 | 53.1000 | . 4 5 | 0 | 3 | 35.0 | 0 | 0 | 8.0500 | . train = pd.concat([train,sex,embark],axis=1) . train.head() . PassengerId Survived Pclass Age SibSp Parch Fare male Q S . 0 1 | 0 | 3 | 22.0 | 1 | 0 | 7.2500 | 1 | 0 | 1 | . 1 2 | 1 | 1 | 38.0 | 1 | 0 | 71.2833 | 0 | 0 | 0 | . 2 3 | 1 | 3 | 26.0 | 0 | 0 | 7.9250 | 0 | 0 | 1 | . 3 4 | 1 | 1 | 35.0 | 1 | 0 | 53.1000 | 0 | 0 | 1 | . 4 5 | 0 | 3 | 35.0 | 0 | 0 | 8.0500 | 1 | 0 | 1 | . Building a Logistic Regression Model . Train Test Split . train.drop(&#39;Survived&#39;,axis=1).head() . PassengerId Pclass Age SibSp Parch Fare male Q S . 0 1 | 3 | 22.0 | 1 | 0 | 7.2500 | 1 | 0 | 1 | . 1 2 | 1 | 38.0 | 1 | 0 | 71.2833 | 0 | 0 | 0 | . 2 3 | 3 | 26.0 | 0 | 0 | 7.9250 | 0 | 0 | 1 | . 3 4 | 1 | 35.0 | 1 | 0 | 53.1000 | 0 | 0 | 1 | . 4 5 | 3 | 35.0 | 0 | 0 | 8.0500 | 1 | 0 | 1 | . train[&#39;Survived&#39;].head() . 0 0 1 1 2 1 3 1 4 0 Name: Survived, dtype: int64 . from sklearn.model_selection import train_test_split . X_train,X_test,y_train,y_test = train_test_split(train.drop(&#39;Survived&#39;,axis=1),train[&#39;Survived&#39;],test_size=0.30,random_state=101) . Training and Predicting . from sklearn.linear_model import LogisticRegression . logmodel = LogisticRegression() logmodel.fit(X_train,y_train) . C: Users mrsid anaconda3 lib site-packages sklearn linear_model _logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . predictions = logmodel.predict(X_test) . from sklearn.metrics import confusion_matrix . accuracy = confusion_matrix(y_test,predictions) . accuracy . array([[149, 14], [ 39, 65]], dtype=int64) . from sklearn.metrics import accuracy_score . accuracy = accuracy_score(y_test,predictions) accuracy . 0.8014981273408239 . predictions . array([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1], dtype=int64) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/25/EDA.html",
            "relUrl": "/2021/04/25/EDA.html",
            "date": " • Apr 25, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Exploratory Data Analysis on Iris Dataset",
            "content": "dataset link :- https://archive.ics.uci.edu/ml/datasets/Iris . Import all libraries . import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt . Creating DataFrame . df = pd.read_csv(&quot;https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv&quot;) . df.head() . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . df.shape . (150, 5) . Univariate Analysis . df.loc[df[&#39;species&#39;]==&#39;setosa&#39;] . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . 5 5.4 | 3.9 | 1.7 | 0.4 | setosa | . 6 4.6 | 3.4 | 1.4 | 0.3 | setosa | . 7 5.0 | 3.4 | 1.5 | 0.2 | setosa | . 8 4.4 | 2.9 | 1.4 | 0.2 | setosa | . 9 4.9 | 3.1 | 1.5 | 0.1 | setosa | . 10 5.4 | 3.7 | 1.5 | 0.2 | setosa | . 11 4.8 | 3.4 | 1.6 | 0.2 | setosa | . 12 4.8 | 3.0 | 1.4 | 0.1 | setosa | . 13 4.3 | 3.0 | 1.1 | 0.1 | setosa | . 14 5.8 | 4.0 | 1.2 | 0.2 | setosa | . 15 5.7 | 4.4 | 1.5 | 0.4 | setosa | . 16 5.4 | 3.9 | 1.3 | 0.4 | setosa | . 17 5.1 | 3.5 | 1.4 | 0.3 | setosa | . 18 5.7 | 3.8 | 1.7 | 0.3 | setosa | . 19 5.1 | 3.8 | 1.5 | 0.3 | setosa | . 20 5.4 | 3.4 | 1.7 | 0.2 | setosa | . 21 5.1 | 3.7 | 1.5 | 0.4 | setosa | . 22 4.6 | 3.6 | 1.0 | 0.2 | setosa | . 23 5.1 | 3.3 | 1.7 | 0.5 | setosa | . 24 4.8 | 3.4 | 1.9 | 0.2 | setosa | . 25 5.0 | 3.0 | 1.6 | 0.2 | setosa | . 26 5.0 | 3.4 | 1.6 | 0.4 | setosa | . 27 5.2 | 3.5 | 1.5 | 0.2 | setosa | . 28 5.2 | 3.4 | 1.4 | 0.2 | setosa | . 29 4.7 | 3.2 | 1.6 | 0.2 | setosa | . 30 4.8 | 3.1 | 1.6 | 0.2 | setosa | . 31 5.4 | 3.4 | 1.5 | 0.4 | setosa | . 32 5.2 | 4.1 | 1.5 | 0.1 | setosa | . 33 5.5 | 4.2 | 1.4 | 0.2 | setosa | . 34 4.9 | 3.1 | 1.5 | 0.1 | setosa | . 35 5.0 | 3.2 | 1.2 | 0.2 | setosa | . 36 5.5 | 3.5 | 1.3 | 0.2 | setosa | . 37 4.9 | 3.1 | 1.5 | 0.1 | setosa | . 38 4.4 | 3.0 | 1.3 | 0.2 | setosa | . 39 5.1 | 3.4 | 1.5 | 0.2 | setosa | . 40 5.0 | 3.5 | 1.3 | 0.3 | setosa | . 41 4.5 | 2.3 | 1.3 | 0.3 | setosa | . 42 4.4 | 3.2 | 1.3 | 0.2 | setosa | . 43 5.0 | 3.5 | 1.6 | 0.6 | setosa | . 44 5.1 | 3.8 | 1.9 | 0.4 | setosa | . 45 4.8 | 3.0 | 1.4 | 0.3 | setosa | . 46 5.1 | 3.8 | 1.6 | 0.2 | setosa | . 47 4.6 | 3.2 | 1.4 | 0.2 | setosa | . 48 5.3 | 3.7 | 1.5 | 0.2 | setosa | . 49 5.0 | 3.3 | 1.4 | 0.2 | setosa | . df_setosa=df.loc[df[&#39;species&#39;]==&#39;setosa&#39;] df_virginica=df.loc[df[&#39;species&#39;]==&#39;virginica&#39;] df_versicolor=df.loc[df[&#39;species&#39;]==&#39;versicolor&#39;] . plt.plot(df_setosa[&#39;sepal_length&#39;],np.zeros_like(df_setosa[&#39;sepal_length&#39;]),&#39;o&#39;) plt.plot(df_virginica[&#39;sepal_length&#39;],np.zeros_like(df_virginica[&#39;sepal_length&#39;]),&#39;o&#39;) plt.plot(df_versicolor[&#39;sepal_length&#39;],np.zeros_like(df_versicolor[&#39;sepal_length&#39;]),&#39;o&#39;) plt.xlabel(&quot;sepal_length&quot;) plt.show() . Bivariate Analysis . sns.FacetGrid(df,hue=&#39;species&#39;,size=5).map(plt.scatter,&quot;petal_length&quot;,&quot;sepal_width&quot;).add_legend(); plt.show() . Multivariate Analysis . sns.pairplot(df,hue=&#39;species&#39;,size=3) . C: Users mrsid anaconda3 lib site-packages seaborn axisgrid.py:2079: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) . &lt;seaborn.axisgrid.PairGrid at 0x13024488808&gt; .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/24/EDA-IRIS.html",
            "relUrl": "/2021/04/24/EDA-IRIS.html",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Seaborn || sns",
            "content": "Distribution plots . distplot, joinplot, pairplot . import seaborn as sns import numpy as np . df = sns.load_dataset(&quot;tips&quot;) . df.head() # tip is dependent feature # others are independent features . total_bill tip sex smoker day time size . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | . df.dtypes . total_bill float64 tip float64 sex category smoker category day category time category size int64 dtype: object . Correlation with Heatmap . df.corr() # correlarion can only be found out if values are floating point or integers # corr values range b/w -1 to +1 . total_bill tip size . total_bill 1.000000 | 0.675734 | 0.598315 | . tip 0.675734 | 1.000000 | 0.489299 | . size 0.598315 | 0.489299 | 1.000000 | . Observations :- 1) +ve corr -&gt;&gt; Total bill inc then tip will also inc . sns.heatmap(df.corr()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d218d27108&gt; . JointPlot . Univariate analysis . sns.jointplot(x=&#39;tip&#39;,y=&#39;total_bill&#39;,data=df,kind=&#39;hex&#39;) # hex=hexagonal shape . &lt;seaborn.axisgrid.JointGrid at 0x2d219541988&gt; . sns.jointplot(x=&#39;tip&#39;,y=&#39;total_bill&#39;,data=df,kind=&#39;reg&#39;) # reg gives probablity density line(on graph) and regression line (inside plot) . &lt;seaborn.axisgrid.JointGrid at 0x2d2197b6d88&gt; . Pair plot . same data row is matched with another variable&#39;s value . sns.pairplot(df, hue=&#39;sex&#39;) . &lt;seaborn.axisgrid.PairGrid at 0x2d219f05bc8&gt; . Dist Plot . sns.distplot(df[&#39;tip&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21a51bcc8&gt; . sns.distplot(df[&#39;tip&#39;],kde =False,bins=10) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21a657dc8&gt; . Categorical Plots . Count Plot . sns.countplot(&#39;sex&#39;,data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21a6d2688&gt; . Bar Plot . sns.barplot(x=&#39;total_bill&#39;,y=&#39;sex&#39;,data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21b8f2788&gt; . Box Plot . sns.boxplot(&#39;sex&#39;, &#39;total_bill&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21b950348&gt; . sns.boxplot(x=&#39;day&#39;, y=&#39;total_bill&#39;, data=df, palette=&#39;rainbow&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21b9b9a48&gt; . sns.boxplot(data=df, orient=&#39;v&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21baf1708&gt; . sns.boxplot(x=&#39;total_bill&#39;, y=&#39;day&#39;, hue=&#39;smoker&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21bb73288&gt; . Violin Plot . sns.violinplot(x=&quot;total_bill&quot;,y=&#39;day&#39;,data=df, palette=&#39;rainbow&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21bc73488&gt; .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/23/Seaborn.html",
            "relUrl": "/2021/04/23/Seaborn.html",
            "date": " • Apr 23, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "Matplotlib || plt",
            "content": "import matplotlib.pyplot as plt %matplotlib inline . import numpy as np . x = np.arange(0,10) y=np.arange(11,21) . a=np.arange(40,50) b=np.arange(50,60) . Scatter plot . plt.scatter(x,y,c=&#39;g&#39;) # c= color plt.xlabel(&#39;X axis&#39;) plt.ylabel(&#39;Y axis&#39;) plt.title(&#39;Graph in 2D&#39;) plt.savefig(&#39;g1.png&#39;) plt.show() . plt plot . plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x20b5b3bef48&gt;] . y=x*x plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x20b5b4228c8&gt;] . plt.plot(x,y,&#39;r&#39;) . [&lt;matplotlib.lines.Line2D at 0x20b5b447d08&gt;] . plt.plot(x,y,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x20b5b4ef748&gt;] . plt.plot(x,y,&#39;r*-&#39;) . [&lt;matplotlib.lines.Line2D at 0x20b5b562fc8&gt;] . Subplots . plt.subplot(2,2,1) # 2 rows 2 cols 1 position plt.plot(x,y,&#39;r&#39;) plt.subplot(2,2,2) plt.plot(x,y,&#39;g&#39;) plt.subplot(2,2,3) plt.plot(x,y,&#39;b&#39;) . [&lt;matplotlib.lines.Line2D at 0x20b5b61f088&gt;] . . np.pi . 3.141592653589793 . x = np.arange(0,4*np.pi,0.1) y=np.sin(x) plt.title(&quot;sine wave form&quot;) plt.plot(x,y) plt.show() . x=np.arange(0,5*np.pi,0.1) y_sin = np.sin(x) y_cos = np.cos(x) plt.subplot(2,1,1) plt.plot(x,y_sin,&#39;r--&#39;) plt.title(&quot;sine graph&quot;) plt.subplot(2,1,2) plt.plot(x,y_cos,&#39;g--&#39;) plt.title(&quot;cosine graph&quot;) plt.show() . Bar plot . x= [2,8,10] y = [11,16,18] x2 = [3,9,11] y2 = [4,7,9] plt.bar(x,y) plt.bar(x2,y2,color =&#39;g&#39;) plt.title(&#39;Bar graph&#39;) plt.ylabel( &#39;Yaxis&#39;) plt.xlabel( &#39;Xaxis&#39;) plt.show() . Histograms . a = np.array([1,2,3,4,5,5,6,67,7,8,8,9]) # y axis == bins - desity or count plt.hist(a) plt.title(&#39;histogram&#39;) plt.show() . Box plot . data = [np.random.normal(0,std,100) for std in range(1,4)] # selecting a normal distribution b/w low=0, to std, step=100 # rectangular box plot plt.boxplot(data, vert=True, patch_artist= True) . {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x20b5bb46688&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb46f08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb55d08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb55e88&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb6b188&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb6ba08&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x20b5bb4b8c8&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb4bf48&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb5bdc8&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb5bec8&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb6bb88&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb71a08&gt;], &#39;boxes&#39;: [&lt;matplotlib.patches.PathPatch at 0x20b5bb46048&gt;, &lt;matplotlib.patches.PathPatch at 0x20b5bb50fc8&gt;, &lt;matplotlib.patches.PathPatch at 0x20b5bb65ac8&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x20b5bb4bfc8&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb5fd08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb71b88&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x20b5bb50f08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb5fe08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb76a08&gt;], &#39;means&#39;: []} . data . [array([-0.7784494 , -0.30130908, 0.54002525, -0.51800759, 0.01819769, -0.83990426, -0.28781469, 0.04318482, 1.23528389, 2.1785494 , -2.0737086 , 1.0928547 , -0.0187436 , 1.26047616, -0.22879622, 0.7987299 , -1.32200805, 1.5095032 , -0.90634209, -0.88452427, 0.21450132, -0.33105648, -0.89893418, 0.2640048 , 0.18846496, -0.13365763, -0.56769452, 1.70685974, 2.50448167, -0.71739823, -2.15135456, -0.79866835, 0.01126657, 0.03509671, 0.70977944, -0.48825295, -0.51388798, 0.03850738, -0.11959896, -1.44425172, -0.48869629, 1.99891486, -0.79457436, 0.82734671, -0.21331385, -1.01447424, -1.62881497, 1.55287689, -0.76185124, -1.33031956, -0.24552639, 0.07408732, -2.05106282, 1.08293709, -0.39720809, -0.37170031, -0.78308727, 0.94345425, -1.61168896, 0.75191668, -0.19178661, 0.35292808, -0.32761845, -0.12057788, -0.90665516, 0.61673275, 0.3552815 , -0.75085115, 0.95438335, -0.4752099 , -1.22754795, 0.90739187, 0.98549253, 1.17860435, -0.47033725, -1.11863367, -2.1007785 , -1.28848407, -0.97587155, -1.50746364, 0.15689869, -1.29434923, 0.95408283, 0.38562582, 1.09328084, -0.83567472, 1.46300781, 0.21707649, 1.04889211, 0.13129867, 0.78442675, 0.21995366, 1.63712729, 1.50326651, 0.28453443, -0.2031552 , -0.28490282, 1.33678566, 2.37008989, 0.79503051]), array([ 1.23556973, -0.02072204, -1.12229404, -2.96722053, -1.30085601, -2.60421508, 1.24700109, -0.31148209, -2.52475577, -3.79873713, -0.5184776 , -1.40388223, -0.76082764, 1.21536502, -0.98142646, 0.43235375, 2.01282379, -0.21453285, 3.61200475, 1.8287454 , -2.37699005, -4.43876649, -1.5534308 , 0.19087839, 0.63776082, -3.89796591, -0.77253082, 0.15942456, 1.50682854, -2.13153439, -0.03070496, -0.87138476, -3.60486968, -3.73673651, 1.36459964, -0.57526159, 1.74855 , -1.59916748, -2.53317411, 0.34688596, -0.39179164, 3.50326963, -2.16398775, 1.6853139 , 0.93583756, -3.19704488, 2.29302575, 0.1907704 , 1.65541487, -1.30203682, 2.56856035, 0.0327959 , 4.19304044, -1.00926479, -2.24279789, -0.69572595, -1.76483291, 3.0767504 , -2.20523853, 3.85941305, 0.02224512, 0.51100795, -0.64877433, -0.97541769, -0.55332363, 0.68110681, 1.04656981, -1.66401884, -2.22326276, 2.5260883 , 1.23117647, -0.60578903, 0.08622414, 1.41381078, -2.7653705 , -0.97335699, 2.92662744, -0.83610816, 2.29915347, 0.01851729, -1.31768037, -1.48470864, 1.02320517, 0.44434635, -3.43562133, -0.4494547 , 0.08147359, 3.30459418, 1.80139721, -1.308831 , -0.99884576, -1.46526386, -0.54199541, 1.12811024, 2.97529432, 1.64583481, -0.78990555, -0.74874302, -4.4103771 , -2.48981923]), array([ 3.95391703e+00, -1.07121577e+00, -3.84668853e+00, 6.77840007e+00, -2.19381045e+00, 7.10352670e-01, 6.73618307e-01, 1.37069922e+00, -3.81843396e+00, 1.26967121e+00, -2.22084017e+00, -3.53653835e+00, 9.12261523e-01, 3.46900445e+00, 5.60861189e-01, 1.81888792e+00, 7.13406114e-01, -3.34833646e+00, 1.39887349e+00, -1.53083906e+00, 3.99241572e+00, -1.95620365e+00, -1.32736259e+00, 1.45314767e+00, 1.86896524e+00, 1.41268309e+00, -2.04054499e+00, -3.22104097e+00, -3.38356292e+00, -1.07288730e+00, -2.13342416e+00, -1.17784314e+00, -5.50678185e-01, -2.93018741e+00, 6.09593785e+00, 3.56688350e-01, -2.74400006e+00, 1.41395686e+00, -1.06679209e+00, 3.99608167e+00, -1.63810367e+00, 3.26794993e+00, -2.17703756e+00, 5.76026096e+00, -3.16019468e+00, -2.04739358e+00, -9.21248072e-01, -1.17306562e+00, -1.40941302e+00, -3.39076210e+00, 8.42848917e+00, -2.23424984e+00, 1.51486619e+00, 3.39342705e+00, -3.71272706e+00, 9.32418444e+00, -2.89173783e+00, -7.17807468e-01, 6.45628003e+00, 2.46759215e+00, -5.40677123e-01, 1.03397626e+00, -4.61687260e-01, 2.28964222e+00, -1.45379187e+00, 1.09286059e+00, 1.66547924e+00, 2.60394771e+00, 3.59662329e-02, -1.58705864e+00, -2.26368232e+00, 2.50848563e+00, -1.72671381e+00, -3.19559078e+00, -9.92987939e-01, 8.91871959e-01, 1.03963870e+00, -4.01271402e-01, 3.12010149e+00, -1.35404888e+00, 2.93841033e+00, -9.41879808e-02, 5.56786269e-01, -9.35989605e-01, 1.10483247e+00, -1.21961918e+00, -4.03470597e-01, -1.41275722e-01, 2.15839643e-01, -2.90275833e+00, 6.03367683e+00, 4.09121350e+00, 3.09437534e+00, -2.16658125e-03, 2.75046954e+00, 8.71768377e-01, -2.33004375e+00, -8.64465990e-03, 2.06668848e+00, 5.57575505e-01])] . Pi chart . labels = &#39;python&#39;,&#39;c++&#39;, &#39;ruby&#39;, &#39;java&#39; sizes = [215,130,245,210] colors = [&#39;gold&#39;, &#39;yellowgreen&#39;,&#39;lightcoral&#39;, &#39;lightskyblue&#39;] explode = (0.1,0,0,0) #explode 1st slice #plot plt.pie(sizes,explode=explode,labels=labels,colors=colors, autopct=&#39;%1.1f%%&#39;,shadow=True) plt.axis(&#39;equal&#39;) plt.show() .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/23/Matplotlib.html",
            "relUrl": "/2021/04/23/Matplotlib.html",
            "date": " • Apr 23, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "Pyforest - Import all Python Data Science Libraries",
            "content": "pip install Pyforest . Collecting Pyforest Downloading pyforest-1.0.3.tar.gz (14 kB) Building wheels for collected packages: Pyforest Building wheel for Pyforest (setup.py): started Building wheel for Pyforest (setup.py): finished with status &#39;done&#39; Created wheel for Pyforest: filename=pyforest-1.0.3-py2.py3-none-any.whl size=13720 sha256=18e08121d5ee96f79c928bf37f744f0c937b1253580c5189dc7f68057ca46f2c Stored in directory: c: users mrsid appdata local pip cache wheels 72 b6 6c b593d021f7e83f481c5208bc23df0084bcfbeb5b141352b882 Successfully built Pyforest Installing collected packages: Pyforest Successfully installed Pyforest-1.0.3 Note: you may need to restart the kernel to use updated packages. . df = pd.read_csv(&#39;http://winterolympicsmedals.com/medals.csv&#39;) . df.head() . Year City Sport Discipline NOC Event Event gender Medal . 0 1924 | Chamonix | Skating | Figure skating | AUT | individual | M | Silver | . 1 1924 | Chamonix | Skating | Figure skating | AUT | individual | W | Gold | . 2 1924 | Chamonix | Skating | Figure skating | AUT | pairs | X | Gold | . 3 1924 | Chamonix | Bobsleigh | Bobsleigh | BEL | four-man | M | Bronze | . 4 1924 | Chamonix | Ice Hockey | Ice Hockey | CAN | ice hockey | M | Gold | . active_imports() # It imports only those libraries that are in use . import pandas as pd . [&#39;import pandas as pd&#39;] . lst1 = [1,2,3,4,5] lst2 = [6,7,8,9,10] plt.plot(lst1,lst2) plt.xlabel(&quot;X-axis&quot;) plt.ylabel(&quot;Y-axis&quot;) plt.show() . np.array([1,2,3,4,5]) . array([1, 2, 3, 4, 5]) . active_imports() . import matplotlib.pyplot as plt import pandas as pd import numpy as np . [&#39;import matplotlib.pyplot as plt&#39;, &#39;import pandas as pd&#39;, &#39;import numpy as np&#39;] . df1= pd.read_csv(&quot;C: Users mrsid Desktop 30 days of ML challenge NumPy and Pandas mercedesbenz.csv&quot;) . df1.head() . ID y X0 X1 X2 X3 X4 X5 X6 X8 ... X375 X376 X377 X378 X379 X380 X382 X383 X384 X385 . 0 0 | 130.81 | k | v | at | a | d | u | j | o | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 6 | 88.53 | k | t | av | e | d | y | l | o | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 7 | 76.26 | az | w | n | c | d | x | j | x | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 3 9 | 80.62 | az | t | n | f | d | x | l | e | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 13 | 78.02 | az | v | n | f | d | h | d | n | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 rows × 378 columns . sns.distplot(df1[&#39;y&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x27bb681d3c8&gt; . active_imports() . import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np . [&#39;import matplotlib.pyplot as plt&#39;, &#39;import seaborn as sns&#39;, &#39;import pandas as pd&#39;, &#39;import numpy as np&#39;] .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/22/Pyforest.html",
            "relUrl": "/2021/04/22/Pyforest.html",
            "date": " • Apr 22, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "NumPy || np",
            "content": "import numpy as np . lst1 = [1,2,3,4] arr = np.array(lst1) . type(arr) . numpy.ndarray . arr . array([1, 2, 3, 4]) . arr.shape . (4,) . list1=[1,2,3,4] list2=[6,7,8,9] list3=[3,4,5,6] arr = np.array([list1,list2,list3]) . arr . array([[1, 2, 3, 4], [6, 7, 8, 9], [3, 4, 5, 6]]) . arr.shape . (3, 4) . arr.reshape(4,3) . array([[1, 2, 3], [4, 6, 7], [8, 9, 3], [4, 5, 6]]) . arr.reshape(1,12) . array([[1, 2, 3, 4, 6, 7, 8, 9, 3, 4, 5, 6]]) . arr.shape . (3, 4) . Indexing . arr . array([[1, 2, 3, 4], [6, 7, 8, 9], [3, 4, 5, 6]]) . arr[0][1] . 2 . arr[1:,3:] . array([[9], [6]]) . arr[1:,2:] . array([[8, 9], [5, 6]]) . arr[:,2:] . array([[3, 4], [8, 9], [5, 6]]) . arr[0:2,0:2] # always remember left:right is left exact and right is one value greater than actual one . array([[1, 2], [6, 7]]) . Inbuilt functions . arr = np.arange(0,10) . arr . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . arr = np.arange(0,10,step=2) arr . array([0, 2, 4, 6, 8]) . # shift+tab for elaborate the function np.linspace(1,10,50) . array([ 1. , 1.18367347, 1.36734694, 1.55102041, 1.73469388, 1.91836735, 2.10204082, 2.28571429, 2.46938776, 2.65306122, 2.83673469, 3.02040816, 3.20408163, 3.3877551 , 3.57142857, 3.75510204, 3.93877551, 4.12244898, 4.30612245, 4.48979592, 4.67346939, 4.85714286, 5.04081633, 5.2244898 , 5.40816327, 5.59183673, 5.7755102 , 5.95918367, 6.14285714, 6.32653061, 6.51020408, 6.69387755, 6.87755102, 7.06122449, 7.24489796, 7.42857143, 7.6122449 , 7.79591837, 7.97959184, 8.16326531, 8.34693878, 8.53061224, 8.71428571, 8.89795918, 9.08163265, 9.26530612, 9.44897959, 9.63265306, 9.81632653, 10. ]) . print(arr) arr[3:] =100 # replace all indexes starting from 3rd to all by 100 print(arr) . [0 2 4 6 8] [ 0 2 4 100 100] . arr1=arr . arr1[3:]=500 arr1 . array([ 0, 2, 4, 500, 500]) . arr # array is actually a reference type hence change is reflected to actual array also . array([ 0, 2, 4, 500, 500]) . arr1 = arr.copy() arr1 . array([ 0, 2, 4, 500, 500]) . arr1[3:] = 800 print(arr1) print(arr) . [ 0 2 4 800 800] [ 0 2 4 500 500] . Some useful conditions for Exploratorty data analysis . arr = np.array([1,2,3,4,5]) val = 2 . arr . array([1, 2, 3, 4, 5]) . print(arr&lt;2) print(arr*2) print(arr%2) . [ True False False False False] [ 2 4 6 8 10] [1 0 1 0 1] . arr[arr&lt;2] . array([1]) . np.ones((2,5),dtype=int) . array([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]) . np.ones(4) . array([1., 1., 1., 1.]) . np.random.rand(3,3) . array([[0.97016302, 0.13230666, 0.31222633], [0.85189366, 0.07856671, 0.57296934], [0.71915461, 0.48997742, 0.24332137]]) . arr_ex = np.random.randn(4,4) # selects from random distribution arr_ex . array([[-0.60890655, -0.67170484, -0.28552398, 1.14748824], [-1.27784825, -0.60587355, -0.87103948, -0.75084882], [ 0.1356478 , 0.67908955, -0.18930585, -1.23064491], [ 0.0557476 , 0.96733176, -0.0119645 , 0.94036578]]) . arr_ex.reshape(16,1) . array([[-0.60890655], [-0.67170484], [-0.28552398], [ 1.14748824], [-1.27784825], [-0.60587355], [-0.87103948], [-0.75084882], [ 0.1356478 ], [ 0.67908955], [-0.18930585], [-1.23064491], [ 0.0557476 ], [ 0.96733176], [-0.0119645 ], [ 0.94036578]]) . import seaborn as sns import pandas as pd . sns.distplot(pd.DataFrame(arr_ex.reshape(16,1))) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b903b859c8&gt; . np.random.randint(0,100,8).reshape(4,2) . array([[17, 74], [67, 13], [91, 60], [92, 75]]) . np.random.random_sample((1,5)) . array([[0.02247093, 0.32708592, 0.95730227, 0.40039247, 0.43461314]]) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/22/NumPy.html",
            "relUrl": "/2021/04/22/NumPy.html",
            "date": " • Apr 22, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Python-Intermediate",
            "content": "def even_odd(num): if num%2==0: print(&quot;number is even&quot;) else: print(&quot;number is odd&quot;) . even_odd(24) . number is even . def hello(name, age=29): # name is positional argument and age is keyword argument print(&quot;my name is {} and age is {}&quot;.format(name, age)) . hello(&#39;sid&#39;) . my name is sid and age is 29 . hello(&#39;sid&#39;,20) . my name is sid and age is 20 . def hello(*args, **kwargs): print(args) print(kwargs) . hello(&#39;sid&#39;,&#39;saxena&#39;,age=29,dob=2000) . (&#39;sid&#39;, &#39;saxena&#39;) {&#39;age&#39;: 29, &#39;dob&#39;: 2000} . lst=[&#39;sid&#39;,&#39;saxena&#39;] dict_args={&#39;age&#39;: 20 ,&#39;dob&#39;:2000} . hello(*lst,**dict_args) . (&#39;sid&#39;, &#39;saxena&#39;) {&#39;age&#39;: 20, &#39;dob&#39;: 2000} . lst = [1,2,3,4,5,6,7] def evenoddsum(lst): even_sum=0 odd_sum=0 for i in lst: if i%2==0: even_sum += i else: odd_sum += i return even_sum,odd_sum . evenoddsum(lst) . (12, 16) . Lambda functions . def addition(a,b): return a+b # Single expression can only be converted . addition(4,5) . 9 . addition = lambda a,b:a+b . addition(5,6) . 11 . def even(num): if num%2==0: return True . even(24) . True . even1 = lambda num:num%2==0 . even1(12) . True . def add(x,y,z): return x+y+z . add(1,2,3) . 6 . three_add = lambda x,y,z:x+y+z . three_add(1,2,3) . 6 . Map Function . def even_odd(num): if num%2==0: return True else: return False . even_odd(23) . False . lst=[1,2,3,4,5,6,7,8] # apply same function on multiple values . map(even_odd,lst) # in order to instantiate convert it into a list ## memory is not intialised yet . &lt;map at 0x2a398c7e248&gt; . list(map(even_odd,lst)) . [False, True, False, True, False, True, False, True] . list(map(lambda num:num%2==0,lst)) . [False, True, False, True, False, True, False] . Filter function . def even(num): if num%2==0: return True . lst=[1,2,3,4,5,6,7] . list(filter(even,lst)) # return elements which satisfy the condition . [2, 4, 6] . list(filter(lambda num:num%2==0,lst)) . [2, 4, 6] . List Comprehension . provide a concise way to create lists, It consists of braces containing an expression followed by for clause, then zero or more for or if clauses . lst1=[] def lst_square(lst): for i in lst: lst1.append(i*i) return lst1 . lst_square([1,2,3,4,5,6,7]) . [1, 4, 9, 16, 25, 36, 49] . lst=[1,2,3,4,5,6,7] #list comprehension [i*i for i in lst ] . [1, 4, 9, 16, 25, 36, 49] . list1=[i*i for i in lst ] print(list1) . [1, 4, 9, 16, 25, 36, 49] . [i*i for i in lst if i%2==0] . [4, 16, 36] . String Formatting . print(&quot;hello&quot;) . hello . str=&quot;hello&quot; print(str) . hello . def greetings(name): return &quot;hello {}&quot;.format(name) . greetings(&#39;sid&#39;) . &#39;hello sid&#39; . def welcome_email(firstname,lastname): return &quot;welcome {}. is your last name is {}&quot;.format(firstname,lastname) # order can not be altered . welcome_email(&#39;sid&#39;,&#39;saxena&#39;) . &#39;welcome sid. is your last name is saxena&#39; . def welcome_email(name,age): return &quot;welcome {name}. is your age is {age}&quot;.format(age=age,name=name) # now ordering can alter . welcome_email(&#39;sid&#39;,20) . &#39;welcome sid. is your age is 20&#39; . def welcome_email(name,age): return &quot;welcome {name1}. is your age is {age1}&quot;.format(age1=age,name1=name) . welcome_email(&#39;sid&#39;,20) . &#39;welcome sid. is your age is 20&#39; . List Iterables vs Iterators . lst = [1,2,3,4,5,6,7] # this whole list is getting initialised in the memory for i in lst: print(i) . 1 2 3 4 5 6 7 . list1=iter(lst) #but in case of iterators whole listdose not get stored in memory it will get accessed only via next . list1 . &lt;list_iterator at 0x2a3996beb08&gt; . next(list1) . 1 . next(list1) . 2 . for i in lst1: print(i) . 1 4 9 16 25 36 49 . .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/21/Python-Intermediate.html",
            "relUrl": "/2021/04/21/Python-Intermediate.html",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Python-Basics",
            "content": "bool() . False . type(True) . bool . my_str = &quot;siddy&quot; . print(my_str.isalnum()) print(my_str.isupper()) . True False . my_str=&#39;sid123&#39; my_str.isalnum() . True . Lists . type([]) . list . list = [&#39;mathematics&#39;,100,10,20,122,230] print(list) . [&#39;mathematics&#39;, 100, 10, 20, 122, 230] . list[:] # 100 to 230 list[1:] . [100, 10, 20, 122, 230] . list[1:5] # left is the same index and right is index we want+1 . [100, 10, 20, 122] . print(list) . [&#39;mathematics&#39;, 100, 10, 20, 122, 230, &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;], &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;], &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;]] . list.insert(1,&quot;saxena&quot;) print(list) . [&#39;mathematics&#39;, &#39;saxena&#39;, &#39;saxena&#39;, 100, 10, 20, 122, 230, &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;], &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;], &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;]] . list2 = [1,2,3,4,5] print(list2) . [1, 2, 3, 4, 5] . list2.extend([6,7]) list2 . [1, 2, 3, 4, 5, 6, 7] . operations on list . sum(list2) . 28 . list2.pop() . 7 . list2.count(2) . 1 . list2.index(2,1,4) . 1 . print(min(list2)) print(max(list2)) . 1 6 . list2*2 . [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6] .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/21/Python-Basics.html",
            "relUrl": "/2021/04/21/Python-Basics.html",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Python-Advanced",
            "content": "correct way to initialise a Class . class Car: def __init__(self,window,door,enginetype): self.windows=window self.doors=door self.enginetype=enginetype def self_driving(self): return &quot;This is a {} car&quot;.format(self.enginetype) . car1=Car(4,5,&#39;petrol&#39;) # by default this init method is called . dir(car1) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;doors&#39;, &#39;enginetype&#39;, &#39;self_driving&#39;, &#39;windows&#39;] . car2=Car(3,4,&quot;diesel&quot;) . print(car1.windows) . 4 . print(car2.enginetype) . diesel . car1.self_driving() . &#39;This is a petrol car&#39; . print(car2.doors) . 4 . car2.enginetype=&quot;diesel&quot; . print(car2.enginetype) . diesel . Python Exception Handling . try: # code block where exception can occur a=b except: print(&quot;some problem may have occured&quot;) . some problem may have occured . try: # code block where exception can occur a=b except Exception as ex: print(ex) . name &#39;b&#39; is not defined . try: # code block where exception can occur a=b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . the user have not defined the error . try: # code block where exception can occur a=1 b=&#39;s&#39; c=a+b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39; . a=1 b=&#39;s&#39; c=a+b . TypeError Traceback (most recent call last) &lt;ipython-input-15-b5351790d4cc&gt; in &lt;module&gt; 1 a=1 2 b=&#39;s&#39; -&gt; 3 c=a+b TypeError: unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39; . try: # code block where exception can occur a=1 b=&#39;s&#39; c=a+b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except TypeError as ex2: print(&quot;the user has given unsupported data types for addition&quot;) print(&quot;try to make the data type similar&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . the user has given unsupported data types for addition try to make the data type similar . try: a=int(input(&quot;enter number 1 = &quot;)) b=int(input(&quot;enter number 2 = &quot;)) c=a/b d=a+b e=a*b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except TypeError as ex2: print(&quot;the user has given unsupported data types for addition&quot;) print(&quot;try to make the data type similar&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . enter number 1 = 12 enter number 2 = 12 . print(c) print(d) print(e) . 1.0 24 144 . try: a=int(input(&quot;enter number 1 = &quot;)) b=int(input(&quot;enter number 2 = &quot;)) c=a/b d=a+b e=a*b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except TypeError as ex2: print(&quot;the user has given unsupported data types for addition&quot;) print(&quot;try to make the data type similar&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . enter number 1 = 1 enter number 2 = 2 . 12/0 . ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-28-898e9759c56e&gt; in &lt;module&gt; -&gt; 1 12/0 ZeroDivisionError: division by zero . try: a=int(input(&quot;enter number 1 = &quot;)) b=int(input(&quot;enter number 2 = &quot;)) c=a/b d=a+b e=a*b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except TypeError as ex2: print(&quot;the user has given unsupported data types for addition&quot;) print(&quot;try to make the data type similar&quot;) except ZeroDivisionError as ex3: print(&quot;12/0 is not defined&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) else: print(c) print(d) print(e) finally: print(&quot;The execution is complete&quot;) . enter number 1 = 12 enter number 2 = 0 12/0 is not defined The execution is complete . Custom Exception . class Error(Exception): # inheriting the exception class pass class dobException(Error): pass . year = int(input(&quot;Enter the year of birth&quot;)) age = 2021-year try: if age&lt;=30 &amp; age&gt;20: print(&quot;Valid age&quot;) else: raise dobException except dobException: print(&quot;year range is not valid&quot;) . Enter the year of birth1555 year range is not valid . Public Private and Protected Access modifiers . class Car(): def __init__(self,windows, doors, enginetypes): self.windows=windows self.doors=doors self.enginetypes=enginetypes . audi = Car(4,5,&quot;Diesel&quot;) . audi . &lt;__main__.Car at 0x1ed3a2b5ac8&gt; . audi.windows . 4 . audi.windows=5 . audi.windows . 5 . dir(audi) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;doors&#39;, &#39;enginetypes&#39;, &#39;windows&#39;] . class Car(): def __init__(self,windows, doors, enginetypes): self._windows=windows self._doors=doors self._enginetypes=enginetypes . class Truck(Car): def __init__(self,windows, doors, enginetypes, hp): super().__init__(windows,doors,enginetypes) self.hp=hp . truck=Truck(4,2,&quot;Petrol&quot;,720) . dir(truck) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_doors&#39;, &#39;_enginetypes&#39;, &#39;_windows&#39;, &#39;hp&#39;] . truck._doors=4 . truck._doors . 4 . audi._windows . AttributeError Traceback (most recent call last) &lt;ipython-input-45-63cafd091289&gt; in &lt;module&gt; -&gt; 1 audi._windows AttributeError: &#39;Car&#39; object has no attribute &#39;_windows&#39; . dir(audi) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;doors&#39;, &#39;enginetypes&#39;, &#39;windows&#39;] . class Car(): def __init__(self,windows, doors, enginetypes): self.__windows=windows self.__doors=doors self.__enginetypes=enginetypes . audi=Car(4,2,&quot;petrol&quot;) . dir(audi) . [&#39;_Car__doors&#39;, &#39;_Car__enginetypes&#39;, &#39;_Car__windows&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;] . Inheritance . ## Car Blueprint class Car(): def __init__(self, windows, doors, enginetype): self.windows=windows self.doors=doors self.enginetype=enginetype def drive(self): print(&quot;the person drive a car&quot;) . car = Car(4,5,&quot;diesel&quot;) . car.windows . 4 . car.drive() . the person drive a car . class audi(Car): def __init__(self,windows,doors,enginetype,enableai): super().__init__(windows,doors,enginetype) self.enableai=enableai def selfdriving(self): print(&quot;Audi Supports Self driving&quot;) . audiQ7=audi(5,5,&#39;diesel&#39;,True) . audiQ7.enableai . True .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/21/Python-Advanced.html",
            "relUrl": "/2021/04/21/Python-Advanced.html",
            "date": " • Apr 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey 👋🏽, I’m Siddhant! . . . Hi, I’m Siddhant saxena, a Machine Learning novice and am very passionate about Artificial Intelligence 🚀 . I’m from India and am currently looking to collaborate on ML/DL projects and would love to work with like-minded individuals. . Talking about Me: . 💻 I’m currently working on ML Projects and am looking for collaborators; | 🌱 I’m currently learning Computer Vision; | 💬 Ask me about anything, I’d try my best to help; | 📫 How to reach me: mrsiddy.py@gmail.com; | . . . ⭐️ From Siddhant .",
          "url": "https://mr-siddy.github.io/ML-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mr-siddy.github.io/ML-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}