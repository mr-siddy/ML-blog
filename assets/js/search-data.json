{
  
    
        "post0": {
            "title": "Image Classification using Convolutional Neural Networks - Pytorch",
            "content": "Exploring the CIFAR10 Dataset . CIFAR-10 is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. . Link:- https://www.kaggle.com/c/cifar-10 . Dataset-credentials:- https://www.cs.toronto.edu/~kriz/cifar.html . . Importing Libraries . import os import torch import torchvision import tarfile from torchvision.datasets.utils import download_url from torch.utils.data import random_split . project_name = &#39;cifar10-cnn&#39; . dataset_url = &quot;https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz&quot; download_url(dataset_url, &#39;.&#39;) . with tarfile.open(&#39;./cifar10.tgz&#39;, &#39;r:gz&#39;) as tar: # r:gz --&gt; read mode in g zip format tar.extractall(path=&#39;./data&#39;) . data_dir = &#39;./data/cifar10&#39; print(os.listdir(data_dir)) classes = os.listdir(data_dir + &quot;/train&quot;) print(classes) . [&#39;test&#39;, &#39;train&#39;] [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;] . airplane_files = os.listdir(data_dir + &quot;/train/airplane&quot;) print(&quot;No. of test examples for airplanes: &quot;, len(airplane_files)) print(airplane_files[:5]) . No. of test examples for airplanes: 5000 [&#39;0001.png&#39;, &#39;0002.png&#39;, &#39;0003.png&#39;, &#39;0004.png&#39;, &#39;0005.png&#39;] . ship_test_files = os.listdir(data_dir + &quot;/test/ship&quot;) print(&quot;No of test examples for ship:&quot;, len(ship_test_files)) print(ship_test_files[10:50]) . No of test examples for ship: 1000 [&#39;0011.png&#39;, &#39;0012.png&#39;, &#39;0013.png&#39;, &#39;0014.png&#39;, &#39;0015.png&#39;, &#39;0016.png&#39;, &#39;0017.png&#39;, &#39;0018.png&#39;, &#39;0019.png&#39;, &#39;0020.png&#39;, &#39;0021.png&#39;, &#39;0022.png&#39;, &#39;0023.png&#39;, &#39;0024.png&#39;, &#39;0025.png&#39;, &#39;0026.png&#39;, &#39;0027.png&#39;, &#39;0028.png&#39;, &#39;0029.png&#39;, &#39;0030.png&#39;, &#39;0031.png&#39;, &#39;0032.png&#39;, &#39;0033.png&#39;, &#39;0034.png&#39;, &#39;0035.png&#39;, &#39;0036.png&#39;, &#39;0037.png&#39;, &#39;0038.png&#39;, &#39;0039.png&#39;, &#39;0040.png&#39;, &#39;0041.png&#39;, &#39;0042.png&#39;, &#39;0043.png&#39;, &#39;0044.png&#39;, &#39;0045.png&#39;, &#39;0046.png&#39;, &#39;0047.png&#39;, &#39;0048.png&#39;, &#39;0049.png&#39;, &#39;0050.png&#39;] . from torchvision.datasets import ImageFolder from torchvision.transforms import ToTensor . dataset = ImageFolder(data_dir + &#39;/train&#39;, transform = ToTensor()) . img, label = dataset[0] print(img.shape, label) img[2] # img tensor . torch.Size([3, 32, 32]) 0 . tensor([[0.7804, 0.7804, 0.7882, ..., 0.7843, 0.7804, 0.7765], [0.7961, 0.7961, 0.8000, ..., 0.8039, 0.7961, 0.7882], [0.8118, 0.8157, 0.8235, ..., 0.8235, 0.8157, 0.8078], ..., [0.8706, 0.8392, 0.7765, ..., 0.9686, 0.9686, 0.9686], [0.8745, 0.8667, 0.8627, ..., 0.9608, 0.9608, 0.9608], [0.8667, 0.8627, 0.8667, ..., 0.9529, 0.9529, 0.9529]]) . print(dataset.classes) . [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;] . import matplotlib import matplotlib.pyplot as plt %matplotlib inline matplotlib.rcParams[&#39;figure.facecolor&#39;] = &#39;#ffffff&#39; . def show_example(img, label): print(&#39;Label: &#39;, dataset.classes[label], &quot;(&quot;+str(label)+&quot;)&quot;) plt.imshow(img.permute(1, 2, 0)) # matplot lib expects channels in final dimension . img, label = dataset[5] show_example(img, label) . Label: airplane (0) . show_example(*dataset[0]) . Label: airplane (0) . show_example(*dataset[1099]) . Label: airplane (0) . Training and Validation Datasets . random_seed = 42 torch.manual_seed(random_seed); # It helps to standardise validation set . val_size = 5000 train_size = len(dataset) - val_size train_ds, val_ds = random_split(dataset, [train_size, val_size]) len(train_ds), len(val_ds) . (45000, 5000) . Creating DataLoaders . from torch.utils.data.dataloader import DataLoader batch_size = 128 . train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory = True) val_dl = DataLoader(val_ds, batch_size*2, num_workers = 4, pin_memory = True) . from torchvision.utils import make_grid def show_batch(dl): for images, labels in dl: fig, ax = plt.subplots(figsize = (12, 6)) ax.set_xticks([]); ax.set_yticks([]) ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0)) break . show_batch(train_dl) . Defining the Conv Model . nn.Linear gives full connected layer architecture nn.Conv2d gives convolutional neural network . Basic working of CNN can be described as follows : . . Working of kernel can be describes as : . . Implementation of a convolution operation on a 1 channel image with a 3x3 kernel . def apply_kernel(image, kernel): ri, ci = image.shape #image dimension rk, ck = kernel.shape #kernel dimension ro, co = ri-rk+1, ci-ck+1 #output dimension output = torch.zeros([ro, co]) for i in range(ro): for j in range(co): output[i, j] = torch.sum(image[i:i+rk, j:j+ck] * kernel) return output . sample_image = torch.tensor([ [0, 0, 75, 80, 80], [0, 75, 80, 80, 80], [0, 75, 80, 80, 80], [0, 70, 75, 80, 80], [0, 0, 0, 0, 0] ], dtype = torch.float32) sample_kernel = torch.tensor([ [-1, -2, -1], [0, 0, 0], [1, 2, 1] ], dtype = torch.float32) apply_kernel(sample_image, sample_kernel) . tensor([[ 155., 85., 5.], [ -15., -15., -5.], [-230., -315., -320.]]) . For Multiple channels kernel will have same function over all channels . Refer this blog post for more in-depth intution of CNN :- Convolution in depth . Observation : 5x5 image got reduced to a 3x3 output, while kernel in running over internal values multiple times, but still values of corner are covered only once, so we&#39;ll use padding and it will return output as the same size as input . Padding can be understood using following diagram: . Now we have moved kernel by 1 position each time, we can move kernel by 2 positons too, this is call Stride . Stride can be understood using folloeing diagram: . For multi-channel images, a different kernel is applied to each channels, and the outputs are added together pixel-wise. . There are certain advantages offered by convolutional layers when working with image data: . Fewer parameters: A small set of parameters (the kernel) is used to calculate outputs of the entire image, so the model has much fewer parameters compared to a fully connected layer. | Sparsity of connections: In each layer, each output element only depends on a small number of input elements, which makes the forward and backward passes more efficient. | Parameter sharing and spatial invariance: The features learned by a kernel in one part of the image can be used to detect similar pattern in a different part of another image. | . We will also use a max-pooling layers to progressively decrease the height &amp; width of the output tensors from each convolutional layer. . . Applying Single Convolutional Layer followed by max pooling . import torch.nn as nn import torch.nn.functional as F . conv = nn.Conv2d(3, 8, kernel_size = 3, stride = 1, padding = 1) # 8 is no of kernels which also descide no of output channels ie feature map . pool = nn.MaxPool2d(2, 2) . for images, labels in train_dl: print(&#39;images.shape:&#39;, images.shape) out = conv(images) print(&#39;output.shape:&#39;, out.shape) out = pool(out) print(&#39;max-pool output.shape:&#39;, out.shape) # max pool will reduce size break . images.shape: torch.Size([128, 3, 32, 32]) output.shape: torch.Size([128, 8, 32, 32]) max-pool output.shape: torch.Size([128, 8, 16, 16]) . conv.weight.shape # we have 8 kernel and each kernel contain 3 matrices for 3 input channel and each of 3 matrix have 3x3 matrix that is gonna slide . torch.Size([8, 3, 3, 3]) . conv.weight[0, 0] . tensor([[-0.1398, 0.1772, 0.0425], [-0.0123, 0.1491, -0.0260], [ 0.0454, 0.1039, 0.0843]], grad_fn=&lt;SelectBackward&gt;) . conv.weight[0] . tensor([[[-0.1398, 0.1772, 0.0425], [-0.0123, 0.1491, -0.0260], [ 0.0454, 0.1039, 0.0843]], [[-0.1612, -0.0831, -0.1083], [ 0.1363, -0.0058, -0.0148], [ 0.0635, 0.0091, 0.0112]], [[-0.0657, -0.0093, 0.1648], [ 0.1584, -0.0931, 0.0160], [ 0.1131, -0.1040, 0.0193]]], grad_fn=&lt;SelectBackward&gt;) . simple_model = nn.Sequential( nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1), nn.MaxPool2d(2,2) ) . for images, labels, in train_dl: print(&#39;images.shape:&#39;, images.shape) out = simple_model(images) print(&#39;out.shape:&#39;, out.shape) break . images.shape: torch.Size([128, 3, 32, 32]) out.shape: torch.Size([128, 8, 16, 16]) . Now lets define CNN model . class ImageClassificationBase(nn.Module): def training_step(self, batch): images, labels = batch out = self(images) loss = F.cross_entropy(out, labels) return loss def validation_step(self, batch): images, labels = batch out = self(images) loss = F.cross_entropy(out, labels) acc = accuracy(out, labels) return {&#39;val_loss&#39;: loss.detach(), &#39;val_acc&#39;: acc} def validation_epoch_end(self, outputs): batch_losses = [x[&#39;val_loss&#39;] for x in outputs] epoch_loss = torch.stack(batch_losses).mean() batch_accs = [x[&#39;val_acc&#39;] for x in outputs] epoch_acc = torch.stack(batch_accs).mean() return {&#39;val_loss&#39;: epoch_loss.item(), &#39;val_acc&#39;: epoch_acc.item()} def epoch_end(self, epoch, result): print(&quot;Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}&quot;.format(epoch, result[&#39;train_loss&#39;], result[&#39;val_loss&#39;], result[&#39;val_acc&#39;])) def accuracy(outputs, labels): _, preds = torch.max(outputs, dim=1) return torch.tensor(torch.sum(preds == labels).item() / len(preds)) . class Cifar10CnnModel(ImageClassificationBase): def __init__(self): super().__init__() self.network = nn.Sequential( # input: 3 x 32 x 32 nn.Conv2d(3, 32, kernel_size=3, padding=1), # i/p 3 channels, applies 32 kernels to create o/p: 32 x 32 # output: 32 x 32 x 32 nn.ReLU(), nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # output: 64 x 32 x 32 nn.ReLU(), nn.MaxPool2d(2, 2), # 32 x 32 --&gt; 16 x 16 #output 64 x 16 x 16 nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2), # output 128 x 8 x 8 nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2), # output 256 x 4 x 4 nn.Flatten(), # take 256x4x4 o/p feature map and flatten it out into a vector nn.Linear(256*4*4, 1024), nn.ReLU(), nn.Linear(1024, 512), nn.ReLU(), nn.Linear(512, 10)) def forward(self, xb): return self.network(xb) . model = Cifar10CnnModel() model . Cifar10CnnModel( (network): Sequential( (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU() (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU() (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU() (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU() (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU() (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (15): Flatten(start_dim=1, end_dim=-1) (16): Linear(in_features=4096, out_features=1024, bias=True) (17): ReLU() (18): Linear(in_features=1024, out_features=512, bias=True) (19): ReLU() (20): Linear(in_features=512, out_features=10, bias=True) ) ) . Look at out model : . for images, labels in train_dl: print(&#39;images.shape:&#39;, images.shape) out = model(images) print(&#39;out.shape:&#39;, out.shape) print(&#39;out[0]&#39;, out[0]) # out will have prob of each classes break . images.shape: torch.Size([128, 3, 32, 32]) out.shape: torch.Size([128, 10]) out[0] tensor([ 0.0391, -0.0104, 0.0005, 0.0281, -0.0311, -0.0069, 0.0147, -0.0170, 0.0353, 0.0030], grad_fn=&lt;SelectBackward&gt;) . Training our model using GPU . def get_default_device(): if torch.cuda.is_available(): return torch.device(&#39;cuda&#39;) else: return torch.device(&#39;cpu&#39;) . device = get_default_device() device . device(type=&#39;cuda&#39;) . def to_device(data, device): &quot;&quot;&quot;Move tensors to chosen device&quot;&quot;&quot; if isinstance(data, (list,tuple)): return [to_device(x, device) for x in data] return data.to(device, non_blocking = True) # to method . class DeviceDataLoader(): def __init__(self, dl, device): self.dl = dl self.device = device def __iter__(self): for b in self.dl: yield to_device(b, self.device) def __len__(self): return len(self.dl) . train_dl = DeviceDataLoader(train_dl, device) val_dl = DeviceDataLoader(val_dl, device) to_device(model, device); . @torch.no_grad() # tells when evaluate is being executed we dont want to compute any gradient def evaluate(model, val_loader): model.eval() # tells pytorch that these layers should be put into validation mode outputs = [model.validation_step(batch) for batch in val_loader] return model.validation_epoch_end(outputs) def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD): history = [] optimizer = opt_func(model.parameters(), lr) for epoch in range(epochs): # Training Phase model.train() # tells pytorch that these layers should be put into training mode train_losses = [] for batch in train_loader: loss = model.training_step(batch) train_losses.append(loss) loss.backward() optimizer.step() optimizer.zero_grad() # Validation Phase result = evaluate(model, val_loader) result[&#39;train_loss&#39;] = torch.stack(train_losses).mean().item() model.epoch_end(epoch, result) history.append(result) return history . model = to_device(Cifar10CnnModel(), device) . evaluate(model, val_dl) # with initial set of parameters --&gt; random result . {&#39;val_loss&#39;: 2.3025383949279785, &#39;val_acc&#39;: 0.10006892681121826} . num_epochs = 10 opt_func = torch.optim.Adam lr = 0.001 . history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func) . Epoch [0], train_loss: 1.7970, val_loss: 1.5241, val_acc: 0.4321 Epoch [1], train_loss: 1.2961, val_loss: 1.1054, val_acc: 0.5995 Epoch [2], train_loss: 1.0374, val_loss: 0.9509, val_acc: 0.6613 Epoch [3], train_loss: 0.8626, val_loss: 0.9137, val_acc: 0.6765 Epoch [4], train_loss: 0.7339, val_loss: 0.7842, val_acc: 0.7312 Epoch [5], train_loss: 0.6354, val_loss: 0.7274, val_acc: 0.7429 Epoch [6], train_loss: 0.5439, val_loss: 0.7468, val_acc: 0.7462 Epoch [7], train_loss: 0.4626, val_loss: 0.7059, val_acc: 0.7701 Epoch [8], train_loss: 0.3961, val_loss: 0.7297, val_acc: 0.7634 Epoch [9], train_loss: 0.3301, val_loss: 0.7733, val_acc: 0.7660 . def plot_accuracies(history): accuracies = [x[&#39;val_acc&#39;] for x in history] plt.plot(accuracies, &#39;-x&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;accuracy&#39;) plt.title(&#39;Accuracy vs. No. of epochs&#39;); . plot_accuracies(history) . def plot_losses(history): train_losses = [x.get(&#39;train_loss&#39;) for x in history] val_losses = [x[&#39;val_loss&#39;] for x in history] plt.plot(train_losses, &#39;-x&#39;) plt.plot(val_losses, &#39;-rx&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.legend([&#39;Training&#39;, &#39;Validation&#39;]) plt.title(&#39;Loss vs No of epochs&#39;) . plot_losses(history) . Training error goes down and val error getting up after some time : overfitting . example: . Testing with Individual Images . test_dataset = ImageFolder(data_dir+&#39;/test&#39;, transform=ToTensor()) . def predict_image(img, model): xb = to_device(img.unsqueeze(0), device) # convert to batch of 1 yb = model(xb) # get predictions from model _, preds = torch.max(yb, dim=1) # pick max probab return dataset.classes[preds[0].item()] # retrive label . img, label = test_dataset[1] plt.imshow(img.permute(1, 2, 0)) print(&#39;Label:&#39;, dataset.classes[label], &#39;, Predicted:&#39;, predict_image(img, model)) . Label: airplane , Predicted: airplane . img, label = test_dataset[1002] plt.imshow(img.permute(1, 2, 0)) print(&#39;Label:&#39;, dataset.classes[label], &#39;, Predicted:&#39;, predict_image(img, model)) . Label: automobile , Predicted: automobile . img, label = test_dataset[0] plt.imshow(img.permute(1, 2, 0)) print(&#39;Label:&#39;, dataset.classes[label], &#39;, Predicted:&#39;, predict_image(img, model)) . Label: airplane , Predicted: automobile . img, label = test_dataset[6153] plt.imshow(img.permute(1, 2, 0)) print(&#39;Label:&#39;, dataset.classes[label], &#39;, Predicted:&#39;, predict_image(img, model)) . Label: frog , Predicted: frog . test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device) result = evaluate(model, test_loader) result . {&#39;val_loss&#39;: 0.7635641694068909, &#39;val_acc&#39;: 0.765625} . torch.save(model.state_dict(), &#39;cifar10-cnn.pth&#39;) .",
            "url": "https://mr-siddy.github.io/ML-blog/deep-learning/2021/06/09/Image-Classification-CNN-pytorch.html",
            "relUrl": "/deep-learning/2021/06/09/Image-Classification-CNN-pytorch.html",
            "date": " • Jun 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Training Deep Neural Networks on a GPU",
            "content": "Importing Libraries . import torch import torchvision import numpy as np import matplotlib import matplotlib.pyplot as plt import torch.nn as nn import torch.nn.functional as F from torchvision.datasets import MNIST from torchvision.transforms import ToTensor from torchvision.utils import make_grid from torch.utils.data.dataloader import DataLoader from torch.utils.data import random_split %matplotlib inline matplotlib.rcParams[&#39;figure.facecolor&#39;] = &#39;#ffffff&#39; . dataset = MNIST(root=&#39;data/&#39;, download=False, transform=ToTensor()) . image, label = dataset[0] image.permute(1, 2, 0).shape . torch.Size([28, 28, 1]) . image, label = dataset[0] print(&#39;image.shape:&#39;, image.shape) plt.imshow(image.permute(1, 2, 0), cmap=&#39;gray&#39;) # plt.imshow expects channels to be last dimension in an image tensor, so we use permute to reorder print(&#39;label:&#39;, label) . image.shape: torch.Size([1, 28, 28]) label: 5 . len(dataset) . 60000 . dataset[0] . image, label = dataset[143] print(&#39;image.shape:&#39;, image.shape) plt.imshow(image.permute(1, 2, 0), cmap=&#39;gray&#39;) # plt.imshow expects channels to be last dimension in an image tensor, so we use permute to reorder print(&#39;label:&#39;, label) . image.shape: torch.Size([1, 28, 28]) label: 2 . validation-set using random_split . val_size = 10000 train_size = len(dataset) - val_size train_ds, val_ds = random_split(dataset, [train_size, val_size]) len(train_ds), len(val_ds) . (50000, 10000) . PyTorch data loaders . batch_size = 128 . train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True) val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True) . num_workers attribute tells the data loader instance how many sub-processes to use for data loading. By default, the num_workers value is set to zero, and a value of zero tells the loader to load the data inside the main process. . pin_memory (bool, optional) – If True, the data loader will copy tensors into CUDA pinned memory before returning them. . for images, _ in train_loader: print(&#39;images.shape&#39;, images.shape) print(&#39;grid.shape&#39;, make_grid(images, nrow=16).shape) break . images.shape torch.Size([128, 1, 28, 28]) grid.shape torch.Size([3, 242, 482]) . for images, _ in train_loader: print(&#39;image.shape:&#39;, image.shape) plt.figure(figsize=(16,8)) plt.axis(&#39;off&#39;) plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0))) break . image.shape: torch.Size([1, 28, 28]) . Creating NN . Hidden Layers, Activation function and Non-Linearity . for images, labels in train_loader: print(&#39;images.shape&#39;, images.shape) inputs = images.reshape(-1, 784) print(&#39;inputs.shape&#39;, inputs.shape) break . images.shape torch.Size([128, 1, 28, 28]) inputs.shape torch.Size([128, 784]) . input_size = inputs.shape[-1] #size of output from hidden layer is 32, can be inc or dec to change the learning capacity of model hidden_size = 32 . layer1 = nn.Linear(input_size, hidden_size ) # it will convert 784 to 32 . inputs.shape . torch.Size([128, 784]) . layer1_outputs = layer1(inputs) print(&#39;layer1_outputs&#39;, layer1_outputs.shape) . layer1_outputs torch.Size([128, 32]) . layer1_outputs_direct = inputs @ layer1.weight.t() + layer1.bias layer1_outputs_direct.shape . torch.Size([128, 32]) . torch.allclose(layer1_outputs, layer1_outputs_direct, 1e-3) . True . Thus, layer1_outputs and inputs have a linear relationship, i.e., each element of layer_outputs is a weighted sum of elements from inputs. Thus, even as we train the model and modify the weights, layer1 can only capture linear relationships between inputs and outputs. . . Next, we&#39;ll use the Rectified Linear Unit (ReLU) function as the activation function for the outputs. It has the formula relu(x) = max(0,x) i.e. it simply replaces negative values in a given tensor with the value 0. ReLU is a non-linear function . We can use the F.relu method to apply ReLU to the elements of a tensor. . . . F.relu(torch.tensor([[1, -1, 0], [-0.1, .2, 3]])) . tensor([[1.0000, 0.0000, 0.0000], [0.0000, 0.2000, 3.0000]]) . layer1_outputs.shape . torch.Size([128, 32]) . relu_outputs = F.relu(layer1_outputs) print(&#39;relu_outputs.shape:&#39;, relu_outputs.shape) print(&#39;min(layer1_outputs):&#39;, torch.min(layer1_outputs).item()) print(&#39;min(relu_outputs):&#39;, torch.min(relu_outputs).item()) . relu_outputs.shape: torch.Size([128, 32]) min(layer1_outputs): -0.6917587518692017 min(relu_outputs): 0.0 . output_size = 10 layer2 = nn.Linear(hidden_size, output_size) . layer2_outputs = layer2(relu_outputs) print(&#39;relu_outputs.shape:&#39;, relu_outputs.shape) print(&#39;layer2_outputs.shape:&#39;, layer2_outputs.shape) . relu_outputs.shape: torch.Size([128, 32]) layer2_outputs.shape: torch.Size([128, 10]) . inputs.shape . torch.Size([128, 784]) . F.cross_entropy(layer2_outputs, labels) . tensor(2.2991, grad_fn=&lt;NllLossBackward&gt;) . outputs = (F.relu(inputs @ layer1.weight.t() +layer1.bias)) @ layer2.weight.t() + layer2.bias . torch.allclose(outputs, layer2_outputs, 1e-3) . True . if we hadn&#39;t included a non-linear activation between the two linear layers, the final relationship b/w inputs and outputs would be Linear . outputs2 = (inputs @ layer1.weight.t() + layer1.bias) @ layer2.weight.t() + layer2.bias . combined_layer = nn.Linear(input_size, output_size) combined_layer.weight.data = layer2.weight @ layer1.weight combined_layer.bias.data = layer1.bias @ layer2.weight.t() + layer2.bias . outputs3 = inputs @ combined_layer.weight.t() + combined_layer.bias . torch.allclose(outputs2, outputs3, 1e-3) . False . Model . We are now ready to define our model. As discussed above, we&#39;ll create a neural network with one hidden layer. Here&#39;s what that means: . Instead of using a single nn.Linear object to transform a batch of inputs (pixel intensities) into outputs (class probabilities), we&#39;ll use two nn.Linear objects. Each of these is called a layer in the network. . | The first layer (also known as the hidden layer) will transform the input matrix of shape batch_size x 784 into an intermediate output matrix of shape batch_size x hidden_size. The parameter hidden_size can be configured manually (e.g., 32 or 64). . | We&#39;ll then apply a non-linear activation function to the intermediate outputs. The activation function transforms individual elements of the matrix. . | The result of the activation function, which is also of size batch_size x hidden_size, is passed into the second layer (also known as the output layer). The second layer transforms it into a matrix of size batch_size x 10. We can use this output to compute the loss and adjust weights using gradient descent. . | . As discussed above, our model will contain one hidden layer. Here&#39;s what it looks like visually: . . Let&#39;s define the model by extending the nn.Module class from PyTorch. . class MnistModel(nn.Module): def __init__(self, in_size, hidden_size, out_size): super().__init__() self.Linear1 = nn.Linear(in_size, hidden_size) # hidden layer self.Linear2 = nn.Linear(hidden_size, out_size) # output layer def forward(self, xb): xb = xb.view(xb.size(0),-1) # flatten image tensor out = self.Linear1(xb) # intermediate outputs using hidden layer out = F.relu(out) # applying activation function out = self.Linear2(out) # predictions using o/p layer return out def training_step(self, batch): images, labels = batch out = self(images) loss = F.cross_entropy(out, labels) return loss def validation_step(self, batch): images, labels = batch out = self(images) loss = F.cross_entropy(out, labels) acc = accuracy(out, labels) return {&#39;val_loss&#39;: loss, &#39;val_acc&#39;: acc} def validation_epoch_end(self, outputs): batch_loss = [x[&#39;val_loss&#39;] for x in outputs] epoch_loss = torch.stack(batch_loss).mean() # Combine losses batch_accs = [x[&#39;val_acc&#39;] for x in outputs] epoch_acc = torch.stack(batch_accs).mean() # combine accuracies return {&#39;val_loss&#39;: epoch_loss.item(), &#39;val_acc&#39;: epoch_acc.item()} def epoch_end(self, epoch, result): print(&quot;Epoch [{}], val_loss: {:4f}, val_acc: {:4f}&quot;.format(epoch, result[&#39;val_loss&#39;], result[&#39;val_acc&#39;])) . def accuracy(outputs, labels): _, preds = torch.max(outputs, dim=1) return torch.tensor(torch.sum(preds == labels).item() / len(preds)) . input_size = 784 hidden_size = 32 num_classes = 10 . model = MnistModel(input_size, hidden_size, out_size = num_classes) . for t in model.parameters(): # weights and bias for linear layer and hidden layer print(t.shape) . torch.Size([32, 784]) torch.Size([32]) torch.Size([10, 32]) torch.Size([10]) . for images, labels in train_loader: outputs = model(images) break loss = F.cross_entropy(outputs, labels) print(&#39;Loss:&#39;, loss.item()) print(&#39;outputs.shape: &#39;, outputs.shape) print(&#39;Sample outputs :&#39;, outputs[:2].data) . Loss: 2.3032402992248535 outputs.shape: torch.Size([128, 10]) Sample outputs : tensor([[ 0.0308, 0.1783, -0.0268, -0.1578, -0.1123, -0.0013, 0.1285, -0.0645, -0.0745, -0.0817], [-0.0028, 0.2039, -0.0196, -0.1190, -0.1853, 0.0436, 0.0258, 0.0813, -0.1292, -0.0431]]) . Training the Model using GPU . torch.cuda.is_available() . True . def get_default_device(): if torch.cuda.is_available(): return torch.device(&#39;cuda&#39;) else: return torch.device(&#39;cpu&#39;) . device = get_default_device() device . device(type=&#39;cuda&#39;) . def to_device(data, device): &quot;&quot;&quot;MOve tensors to chosen device&quot;&quot;&quot; if isinstance(data, (list,tuple)): return [to_device(x, device) for x in data] return data.to(device, non_blocking = True) # to method . for images, labels in train_loader: print(images.shape) print(images.device) images = to_device(images, device) print(images.device) break . torch.Size([128, 1, 28, 28]) cpu cuda:0 . DeviceDataLeader class to wrap our existing data loaders and move batches of data to the selected device, iter method to retrieve batches of data and an len to get number of batches . class DeviceDataLoader(): # wrap a dataloader to move data to device def __init__(self, dl , device): self.dl = dl self.device = device # yield a batch of data after moving it to device def __iter__(self): for b in self.dl: yield to_device(b, self.device) # number of batches def __len__(self): return len(self.dl) . # example def some_numbers(): yield 10 yield 20 yield 30 for value in some_numbers(): print(value) . 10 20 30 . train_loader = DeviceDataLoader(train_loader, device) val_loader = DeviceDataLoader(val_loader, device) . for xb, yb in val_loader: print(&#39;xb.device:&#39;, xb.device) print(&#39;yb:&#39;, yb) break . xb.device: cuda:0 yb: tensor([1, 4, 3, 7, 8, 2, 2, 2, 0, 1, 3, 7, 1, 2, 7, 6, 5, 2, 7, 6, 4, 6, 3, 0, 3, 7, 2, 1, 5, 1, 5, 0, 6, 0, 7, 1, 9, 3, 5, 6, 6, 6, 3, 1, 2, 7, 0, 1, 7, 4, 3, 9, 7, 2, 1, 4, 3, 4, 8, 3, 1, 0, 9, 6, 4, 0, 2, 5, 2, 6, 7, 4, 4, 6, 7, 3, 4, 0, 0, 2, 5, 5, 5, 5, 0, 9, 4, 3, 9, 6, 0, 4, 8, 6, 2, 8, 1, 7, 8, 2, 4, 4, 6, 6, 7, 0, 7, 4, 0, 1, 1, 9, 4, 0, 9, 2, 4, 2, 8, 6, 7, 1, 5, 3, 7, 8, 4, 2, 6, 8, 1, 7, 3, 8, 4, 4, 7, 9, 7, 2, 9, 6, 8, 7, 9, 4, 3, 5, 1, 9, 8, 9, 1, 3, 9, 6, 9, 9, 9, 9, 7, 4, 3, 3, 1, 2, 7, 8, 5, 8, 0, 8, 3, 1, 6, 3, 1, 0, 6, 6, 1, 5, 4, 7, 9, 4, 5, 4, 2, 3, 3, 2, 9, 6, 6, 3, 8, 4, 4, 2, 1, 7, 7, 3, 4, 5, 8, 2, 9, 9, 6, 8, 7, 6, 0, 6, 0, 0, 1, 1, 1, 4, 0, 9, 5, 2, 0, 3, 0, 0, 0, 4, 7, 9, 6, 0, 3, 4, 5, 6, 0, 0, 9, 7, 8, 6, 3, 0, 7, 8, 7, 7, 4, 0, 8, 0], device=&#39;cuda:0&#39;) . Training part . def evaluate(model, val_loader): outputs = [model.validation_step(batch) for batch in val_loader] return model.validation_epoch_end(outputs) def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD): history = [] optimizer = opt_func(model.parameters(), lr) for epoch in range(epochs): #training phase for batch in train_loader: loss = model.training_step(batch) loss.backward() optimizer.step() optimizer.zero_grad() #validation phase result = evaluate(model, val_loader) model.epoch_end(epoch, result) history.append(result) return history . model = MnistModel(input_size, hidden_size= hidden_size, out_size=num_classes) to_device(model, device) . MnistModel( (Linear1): Linear(in_features=784, out_features=32, bias=True) (Linear2): Linear(in_features=32, out_features=10, bias=True) ) . history = [evaluate(model, val_loader)] history . [{&#39;val_loss&#39;: 2.309469223022461, &#39;val_acc&#39;: 0.12490234524011612}] . history += fit(5, 0.5, model, train_loader, val_loader) . Epoch [0], val_loss: 0.202114, val_acc: 0.941016 Epoch [1], val_loss: 0.156000, val_acc: 0.953223 Epoch [2], val_loss: 0.156326, val_acc: 0.953516 Epoch [3], val_loss: 0.127720, val_acc: 0.962598 Epoch [4], val_loss: 0.119708, val_acc: 0.962793 . try with more less lr . history += fit(5, 0.1, model, train_loader, val_loader) . Epoch [0], val_loss: 0.109468, val_acc: 0.967578 Epoch [1], val_loss: 0.105522, val_acc: 0.968164 Epoch [2], val_loss: 0.105320, val_acc: 0.969824 Epoch [3], val_loss: 0.104688, val_acc: 0.969629 Epoch [4], val_loss: 0.104357, val_acc: 0.968555 . Lmao acc ~ 96% . losses = [x[&#39;val_loss&#39;] for x in history] plt.plot(losses, &#39;-x&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.title(&#39;Loss v/s Epochs&#39;) . Text(0.5, 1.0, &#39;Loss v/s Epochs&#39;) . accuracies = [x[&#39;val_acc&#39;] for x in history] plt.plot(accuracies, &#39;-x&#39;) plt.xlabel(&#39;epochs&#39;) plt.ylabel(&#39;accuracies&#39;) plt.title(&#39;Accuracies v/s Epochs&#39;) . Text(0.5, 1.0, &#39;Accuracies v/s Epochs&#39;) . Testing with Individual Images . test_dataset = MNIST(root=&#39;data/&#39;, train=False, transform=ToTensor()) . def predict_image(img, model): xb = to_device(img.unsqueeze(0), device) print(xb.device) yb = model(xb) _, preds = torch.max(yb, dim=1) return preds[0].item() . img, label = test_dataset[0] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;Label:&#39;, label, &#39;Prediction:&#39;, predict_image(img, model)) . cuda:0 Label: 7 Prediction: 7 . img, label = test_dataset[123] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;Label:&#39;, label, &#39;Prediction:&#39;, predict_image(img, model)) . cuda:0 Label: 6 Prediction: 6 . img, label = test_dataset[183] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;Label:&#39;, label, &#39;Prediction:&#39;, predict_image(img, model)) . cuda:0 Label: 0 Prediction: 0 . test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size=256), device) result = evaluate(model, test_loader) result . {&#39;val_loss&#39;: 0.10134970396757126, &#39;val_acc&#39;: 0.9697265625} . torch.save(model.state_dict(), &#39;mnist-feedforward.pth&#39;) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/06/03/NN-on-GPU.html",
            "relUrl": "/2021/06/03/NN-on-GPU.html",
            "date": " • Jun 3, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Logistic Regression on MNIST Database using Pytorch",
            "content": "import torch import torchvision from torchvision.datasets import MNIST . Working with Image . We&#39;ll use the famous MNIST Handwritten Digits Database as our training dataset. It consists of 28px by 28px grayscale images of handwritten digits (0 to 9) and labels for each image indicating which digit it represents. Here are some sample images from the dataset: . . Data Preprocessing . dataset = MNIST(root=&#39;data/&#39;, download=True) . len(dataset) . 60000 . test_dataset = MNIST(root=&#39;data/&#39;, train=False) len(test_dataset) . 10000 . dataset[0] # it gives :- image(part of pillow), 5 (label digit) . (&lt;PIL.Image.Image image mode=L size=28x28 at 0x25EA9AE4748&gt;, 5) . import matplotlib.pyplot as plt #indicates to jupyter we want to plot graphs within the notebook, without this it will show in popup %matplotlib inline . image, label = dataset[0] plt.imshow(image, cmap=&#39;gray&#39;) print(&#39;Label:&#39;, label) . Label: 5 . image, label = dataset[10] plt.imshow(image, cmap=&#39;gray&#39;) print(&#39;Label:&#39;, label) . Label: 3 . Pytorch dosen&#39;t know how to work with images, we need to convert images to tensors by specifying transform while creating our dataset . import torchvision.transforms as transform . dataset = MNIST(root=&#39;data/&#39;, train=True, transform=transform.ToTensor()) . img_tensor, label = dataset[0] print(img_tensor.shape,label) . torch.Size([1, 28, 28]) 5 . img_tensor . tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706, 0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922, 0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137, 0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000, 0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275, 0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922, 0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294, 0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098, 0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922, 0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922, 0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706, 0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922, 0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922, 0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]) . print(img_tensor[:,10:15,10:15]) print(torch.max(img_tensor),torch.min(img_tensor)) . tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000], [0.0000, 0.5451, 0.9922, 0.7451, 0.0078], [0.0000, 0.0431, 0.7451, 0.9922, 0.2745], [0.0000, 0.0000, 0.1373, 0.9451, 0.8824], [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]]) tensor(1.) tensor(0.) . plt.imshow(img_tensor[0,10:15,10:15], cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x25eadd15c08&gt; . Training and Validation Datasets . Training set - used to train the model, i.e., compute the loss and adjust the model&#39;s weights using gradient descent. | Validation set - used to evaluate the model during training, adjust hyperparameters (learning rate, etc.), and pick the best version of the model. | Test set - used to compare different models or approaches and report the model&#39;s final accuracy. | In the MNIST dataset, there are 60,000 training images and 10,000 test images. The test set is standardized so that different researchers can report their models&#39; results against the same collection of images. . Since there&#39;s no predefined validation set, we must manually split the 60,000 images into training and validation datasets. Let&#39;s set aside 10,000 randomly chosen images for validation. We can do this using the random_spilt method from PyTorch. . # using random_split from torch.utils.data import random_split train_ds, val_ds = random_split(dataset, [50000, 10000]) len(train_ds), len(val_ds) . (50000, 10000) . from torch.utils.data import DataLoader batch_size = 128 train_loader = DataLoader(train_ds, batch_size, shuffle=True) # shuffle=True for the training data loader to ensure batches generated in each epoch are different # randomization helps generalize and speed=up the training process val_loader = DataLoader(val_ds, batch_size ) . Model . A logistic regression model is almost identical to a linear regression model. It contains weights and bias matrices, and the output is obtained using simple matrix operations (pred = x @ w.t() + b). . | As we did with linear regression, we can use nn.Linear to create the model instead of manually creating and initializing the matrices. . | Since nn.Linear expects each training example to be a vector, each 1x28x28 image tensor is flattened into a vector of size 784 (28*28) before being passed into the model. . | The output for each image is a vector of size 10, with each element signifying the probability of a particular target label (i.e., 0 to 9). The predicted label for an image is simply the one with the highest probability. . | . import torch.nn as nn input_size = 28*28 num_classes = 10 #Logistic regression model model = nn.Linear(input_size, num_classes) . print(model.weight.shape) model.weight . torch.Size([10, 784]) . Parameter containing: tensor([[ 0.0097, -0.0244, -0.0155, ..., -0.0295, 0.0300, -0.0210], [ 0.0036, 0.0262, -0.0189, ..., 0.0118, 0.0162, -0.0324], [ 0.0041, -0.0117, -0.0011, ..., 0.0128, 0.0156, -0.0292], ..., [-0.0163, -0.0195, -0.0283, ..., -0.0236, 0.0138, 0.0091], [ 0.0336, -0.0344, 0.0232, ..., -0.0277, -0.0007, -0.0115], [ 0.0130, 0.0136, 0.0206, ..., 0.0128, -0.0171, -0.0017]], requires_grad=True) . print(model.bias.shape) model.bias . torch.Size([10]) . Parameter containing: tensor([ 0.0144, -0.0100, 0.0354, -0.0225, -0.0262, 0.0286, 0.0230, -0.0162, -0.0203, 0.0171], requires_grad=True) . so total parameters = 7850 . for images, labels in train_loader: print(labels) print(images.shape) outputs = model(images) print(outputs) break . tensor([7, 2, 1, 5, 4, 8, 6, 0, 0, 5, 3, 7, 9, 0, 8, 4, 5, 2, 5, 9, 3, 0, 6, 6, 4, 9, 2, 1, 5, 9, 3, 3, 4, 1, 0, 4, 2, 0, 0, 5, 6, 8, 9, 7, 6, 0, 4, 1, 2, 1, 9, 2, 6, 8, 5, 2, 8, 7, 7, 9, 7, 6, 7, 5, 9, 5, 5, 9, 1, 7, 5, 0, 7, 7, 2, 5, 0, 8, 1, 1, 7, 8, 7, 4, 7, 3, 1, 6, 4, 9, 9, 3, 9, 3, 5, 6, 6, 5, 6, 4, 2, 4, 5, 1, 8, 7, 0, 3, 9, 9, 5, 2, 7, 6, 9, 8, 5, 1, 3, 2, 3, 6, 4, 1, 3, 9, 7, 3]) torch.Size([128, 1, 28, 28]) . RuntimeError Traceback (most recent call last) &lt;ipython-input-21-170d6f073b97&gt; in &lt;module&gt; 4 print(labels) 5 print(images.shape) -&gt; 6 outputs = model(images) 7 print(outputs) 8 break ~ anaconda3 envs torch lib site-packages torch nn modules module.py in _call_impl(self, *input, **kwargs) 887 result = self._slow_forward(*input, **kwargs) 888 else: --&gt; 889 result = self.forward(*input, **kwargs) 890 for hook in itertools.chain( 891 _global_forward_hooks.values(), ~ anaconda3 envs torch lib site-packages torch nn modules linear.py in forward(self, input) 92 93 def forward(self, input: Tensor) -&gt; Tensor: &gt; 94 return F.linear(input, self.weight, self.bias) 95 96 def extra_repr(self) -&gt; str: ~ anaconda3 envs torch lib site-packages torch nn functional.py in linear(input, weight, bias) 1751 if has_torch_function_variadic(input, weight): 1752 return handle_torch_function(linear, (input, weight), input, weight, bias=bias) -&gt; 1753 return torch._C._nn.linear(input, weight, bias) 1754 1755 RuntimeError: mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10) . error occurred bcoz, input data dosen&#39;t have right shape. images are of the shape 1x28x28, but we need them to be vectors of size 784 ie we need to flatten them --&gt; using .reshape . images.shape . torch.Size([128, 1, 28, 28]) . images.reshape(128,784).shape . torch.Size([128, 784]) . To include this additional function within out model, we need to define a custom model by extending the nn.Module class . Inside the __init__ constructor method, we instantiate the weights and biases using nn.Linear. And inside the forward method, which is invoked when we pass a batch of inputs to the model, we flatten the input tensor and pass it into self.linear. . xb.reshape(-1, 28*28) indicates to PyTorch that we want a view of the xb tensor with two dimensions. The length along the 2nd dimension is 28*28 (i.e., 784). One argument to .reshape can be set to -1 (in this case, the first dimension) to let PyTorch figure it out automatically based on the shape of the original tensor. . Note that the model no longer has .weight and .bias attributes (as they are now inside the .linear attribute), but it does have a .parameters method that returns a list containing the weights and bias. . class MnistModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(input_size, num_classes) def forward(self, xb): xb = xb.reshape(-1, 784) out = self.linear(xb) return out model = MnistModel() #object . model.linear . Linear(in_features=784, out_features=10, bias=True) . print(model.linear.weight.shape, model.linear.bias.shape) list(model.parameters()) # bundle all weights and biases . torch.Size([10, 784]) torch.Size([10]) . [Parameter containing: tensor([[ 3.4617e-02, 2.8410e-02, 2.5686e-02, ..., -1.5863e-02, -3.8656e-03, 1.5305e-02], [-1.8018e-02, 1.7917e-02, -3.0232e-02, ..., -5.3763e-03, -3.2701e-02, 1.7136e-02], [-3.4262e-02, 8.5667e-03, -2.0011e-02, ..., -1.8257e-02, 3.4308e-02, 1.8136e-02], ..., [ 9.5943e-03, 1.6986e-02, 2.2343e-02, ..., 4.5605e-03, 9.5017e-03, -2.5854e-04], [-1.4468e-02, -2.7529e-02, 2.9830e-02, ..., -9.3204e-03, 6.1549e-03, -2.5384e-02], [ 3.3574e-02, 2.1621e-03, -8.8337e-03, ..., -4.9651e-05, -2.1324e-02, 2.4071e-03]], requires_grad=True), Parameter containing: tensor([ 0.0054, -0.0311, 0.0240, 0.0014, -0.0334, 0.0090, -0.0151, -0.0068, 0.0042, -0.0279], requires_grad=True)] . for images, labels in train_loader: outputs = model(images) break print(&#39;output.shape :&#39;, outputs.shape) print(&#39;Sample outputs layer 0 :&#39;, outputs[0].data) print(&#39;Sample outputs layer 0 and 1 :&#39;, outputs[:2].data) . output.shape : torch.Size([128, 10]) Sample outputs layer 0 : tensor([-0.1530, -0.4023, 0.0156, -0.0506, 0.0551, -0.2542, 0.0123, -0.3361, -0.1795, 0.1995]) Sample outputs layer 0 and 1 : tensor([[-0.1530, -0.4023, 0.0156, -0.0506, 0.0551, -0.2542, 0.0123, -0.3361, -0.1795, 0.1995], [-0.2933, -0.3964, 0.2764, -0.0050, -0.0035, -0.0671, 0.2888, -0.2759, -0.1555, 0.0238]]) . For each of the 100 input images, we get 10 outputs, one for each class. As discussed earlier, we&#39;d like these outputs to represent probabilities. Each output row&#39;s elements must lie between 0 to 1 and add up to 1, which is not the case. . To convert the output rows into probabilities, we use the softmax function, which has the following formula: . . First, we replace each element yi in an output row by e^yi, making all the elements positive. . . Then, we divide them by their sum to ensure that they add up to 1. The resulting vector can thus be interpreted as probabilities. . we&#39;ll use the implementation that&#39;s provided within PyTorch because it works well with multidimensional tensors (a list of output rows in our case). . import torch.nn.functional as F . outputs[0:2] . tensor([[-0.1530, -0.4023, 0.0156, -0.0506, 0.0551, -0.2542, 0.0123, -0.3361, -0.1795, 0.1995], [-0.2933, -0.3964, 0.2764, -0.0050, -0.0035, -0.0671, 0.2888, -0.2759, -0.1555, 0.0238]], grad_fn=&lt;SliceBackward&gt;) . probs = F.softmax(outputs, dim=1) # apply softmax for each output row # see why dim=0 dosen&#39;t workout print(&quot;sample probabilities: n&quot;, probs[:2].data) #sample probs print(&quot;Sum: &quot;, torch.sum(probs[0]).item()) # addup probs of an output row . sample probabilities: tensor([[0.0942, 0.0734, 0.1115, 0.1044, 0.1160, 0.0851, 0.1111, 0.0784, 0.0917, 0.1340], [0.0774, 0.0698, 0.1368, 0.1032, 0.1034, 0.0970, 0.1385, 0.0787, 0.0888, 0.1063]]) Sum: 1.0 . max_probs, preds = torch.max(probs, dim=1) print(preds) print(max_probs) . tensor([9, 6, 2, 9, 2, 2, 9, 5, 3, 9, 3, 8, 6, 9, 2, 9, 9, 8, 9, 9, 9, 3, 6, 3, 6, 3, 5, 3, 2, 2, 3, 9, 9, 8, 2, 2, 9, 6, 6, 4, 2, 0, 9, 9, 2, 2, 9, 9, 2, 9, 9, 9, 9, 9, 9, 9, 6, 9, 3, 3, 6, 9, 9, 3, 2, 3, 9, 8, 4, 8, 8, 4, 9, 2, 5, 2, 9, 2, 8, 2, 2, 6, 3, 4, 9, 9, 9, 9, 6, 6, 9, 6, 9, 3, 9, 9, 0, 6, 3, 2, 3, 5, 5, 3, 8, 3, 8, 2, 4, 3, 0, 9, 9, 6, 6, 3, 5, 2, 3, 9, 3, 2, 7, 9, 9, 2, 9, 9]) tensor([0.1340, 0.1385, 0.1331, 0.1312, 0.1378, 0.1389, 0.1765, 0.1426, 0.1205, 0.1485, 0.1351, 0.1217, 0.1359, 0.1315, 0.1463, 0.1313, 0.1251, 0.1250, 0.1213, 0.1374, 0.1270, 0.1285, 0.1210, 0.1202, 0.1537, 0.1311, 0.1201, 0.1155, 0.1454, 0.1945, 0.1293, 0.1238, 0.1146, 0.1202, 0.1568, 0.1285, 0.1396, 0.1146, 0.1350, 0.1179, 0.1579, 0.1335, 0.1622, 0.1176, 0.1568, 0.1461, 0.1368, 0.1345, 0.1403, 0.1409, 0.1426, 0.1331, 0.1502, 0.1324, 0.1265, 0.1440, 0.1352, 0.1367, 0.1186, 0.1284, 0.1301, 0.1446, 0.1604, 0.1328, 0.1556, 0.1515, 0.1254, 0.1221, 0.1191, 0.1351, 0.1306, 0.1152, 0.1264, 0.1515, 0.1167, 0.1381, 0.1342, 0.1423, 0.1233, 0.1406, 0.1184, 0.1247, 0.1140, 0.1443, 0.1542, 0.1410, 0.1169, 0.1249, 0.1407, 0.1257, 0.1382, 0.1283, 0.1415, 0.1145, 0.1345, 0.1229, 0.1267, 0.1291, 0.1408, 0.1413, 0.1231, 0.1289, 0.1252, 0.1473, 0.1149, 0.1233, 0.1328, 0.1257, 0.1239, 0.1239, 0.1235, 0.1495, 0.1488, 0.1119, 0.1191, 0.1328, 0.1396, 0.1598, 0.1483, 0.1271, 0.1297, 0.1417, 0.1131, 0.1655, 0.1422, 0.1257, 0.1339, 0.1229], grad_fn=&lt;MaxBackward0&gt;) . labels . tensor([9, 8, 5, 3, 3, 3, 2, 1, 0, 7, 2, 5, 3, 4, 8, 7, 4, 1, 9, 4, 9, 4, 4, 7, 0, 9, 1, 7, 0, 0, 8, 5, 1, 8, 0, 4, 7, 8, 2, 5, 2, 1, 4, 6, 0, 8, 6, 9, 5, 4, 9, 4, 9, 3, 4, 6, 3, 8, 1, 2, 2, 4, 9, 9, 5, 6, 9, 1, 8, 5, 8, 2, 9, 0, 3, 0, 9, 7, 1, 1, 7, 8, 9, 7, 9, 4, 4, 6, 6, 3, 7, 5, 9, 8, 1, 9, 9, 0, 2, 0, 0, 5, 1, 3, 9, 8, 1, 6, 9, 4, 6, 2, 5, 6, 8, 2, 1, 0, 2, 5, 7, 6, 7, 7, 0, 1, 4, 6]) . Evaluation Metric and Loss Function . outputs[:2] . tensor([[-0.1530, -0.4023, 0.0156, -0.0506, 0.0551, -0.2542, 0.0123, -0.3361, -0.1795, 0.1995], [-0.2933, -0.3964, 0.2764, -0.0050, -0.0035, -0.0671, 0.2888, -0.2759, -0.1555, 0.0238]], grad_fn=&lt;SliceBackward&gt;) . preds == labels . tensor([ True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, True, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, True, False, True, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, True, False, False, True, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, False, False]) . torch.sum(preds == labels) # this is telling how many out of 128 were get predicted correctly . tensor(21) . Accuracy of predictions . def accuracy(outputs, labels): _, preds = torch.max(outputs, dim=1) return torch.tensor(torch.sum(preds == labels).item() / len(preds)) . accuracy(outputs,labels) . tensor(0.1641) . The == operator performs an element-wise comparison of two tensors with the same shape and returns a tensor of the same shape, containing True for unequal elements and False for equal elements. Passing the result to torch.sum returns the number of labels that were predicted correctly. Finally, we divide by the total number of images to get the accuracy. . Note that we don&#39;t need to apply softmax to the outputs since its results have the same relative order. This is because e^x is an increasing function, i.e., if y1 &gt; y2, then e^y1 &gt; e^y2. The same holds after averaging out the values to get the softmax. . Let&#39;s calculate the accuracy of the current model on the first batch of data. . Accuracy is an excellent way for us (humans) to evaluate the model. However, we can&#39;t use it as a loss function for optimizing our model using gradient descent for the following reasons: . It&#39;s not a differentiable function. torch.max and == are both non-continuous and non-differentiable operations, so we can&#39;t use the accuracy for computing gradients w.r.t the weights and biases. . | It doesn&#39;t take into account the actual probabilities predicted by the model, so it can&#39;t provide sufficient feedback for incremental improvements. . | For these reasons, accuracy is often used as an evaluation metric for classification, but not as a loss function. A commonly used loss function for classification problems is the cross-entropy, which has the following formula: . . While it looks complicated, it&#39;s actually quite simple: . For each output row, pick the predicted probability for the correct label. E.g., if the predicted probabilities for an image are [0.1, 0.3, 0.2, ...] and the correct label is 1, we pick the corresponding element 0.3 and ignore the rest. . | Then, take the logarithm of the picked probability. If the probability is high, i.e., close to 1, then its logarithm is a very small negative value, close to 0. And if the probability is low (close to 0), then the logarithm is a very large negative value. We also multiply the result by -1, which results is a large postive value of the loss for poor predictions. . | . . Finally, take the average of the cross entropy across all the output rows to get the overall loss for a batch of data. | . Unlike accuracy, cross-entropy is a continuous and differentiable function. It also provides useful feedback for incremental improvements in the model (a slightly higher probability for the correct label leads to a lower loss). These two factors make cross-entropy a better choice for the loss function. . As you might expect, PyTorch provides an efficient and tensor-friendly implementation of cross-entropy as part of the torch.nn.functional package. Moreover, it also performs softmax internally, so we can directly pass in the model&#39;s outputs without converting them into probabilities. . probs . tensor([[0.0942, 0.0734, 0.1115, ..., 0.0784, 0.0917, 0.1340], [0.0774, 0.0698, 0.1368, ..., 0.0787, 0.0888, 0.1063], [0.1088, 0.0800, 0.1331, ..., 0.0638, 0.0798, 0.1175], ..., [0.1022, 0.0864, 0.1257, ..., 0.0881, 0.1148, 0.1158], [0.0974, 0.0822, 0.0975, ..., 0.0675, 0.0971, 0.1339], [0.0905, 0.1213, 0.1075, ..., 0.0760, 0.1040, 0.1229]], grad_fn=&lt;SoftmaxBackward&gt;) . loss_fn = F.cross_entropy . loss = loss_fn(outputs, labels) . print(loss) . tensor(2.3150, grad_fn=&lt;NllLossBackward&gt;) . Training the model . def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD): optimizer = opt_func(model.parameters(), lr) history = [] # for reading epoch-wise results for epoch in range(epochs): #training phase for batch in train_loader: loss = model.training_step(batch) loss.backward() optimizer.step() # change the gradients using the learning rate optimizer.zero_grad() #validation phase result = evaluate(model, val_loader) model.epoch_end(epoch, result) history.append(result) return history . def evaluate(model, val_loader): outputs = [model.validation_step(batch) for batch in val_loader] #list comprehension return model.validation_epoch_end(outputs) . Gaps to fill-in are : training_step, validation_step, validation_epoch_end, epoch_end used by : fit, evaluate . class MnistModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(input_size, num_classes) def forward(self, xb): xb = xb.reshape(-1, 784) out = self.linear(xb) return out def training_step(self, batch): images, labels = batch out = self(images) # Generate predictions loss = F.cross_entropy(out, labels) # Calculate loss return loss def validation_step(self, batch): images, labels = batch out = self(images) # Generate predictions loss = F.cross_entropy(out, labels) #loss acc = accuracy(out, labels) # clac accuracy return {&#39;val_loss&#39;: loss, &#39;val_acc&#39;: acc} def validation_epoch_end(self, outputs): batch_losses = [x[&#39;val_loss&#39;] for x in outputs] epoch_loss = torch.stack(batch_losses).mean() #combine losses batch_accs = [x[&#39;val_acc&#39;] for x in outputs] epoch_accs = torch.stack(batch_accs).mean() #combine accuracies return {&#39;val_loss&#39;: epoch_loss.item(), &#39;val_acc&#39;: epoch_accs.item()} def epoch_end(self, epoch, result): print(&quot;Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}&quot;.format(epoch, result[&#39;val_loss&#39;], result[&#39;val_acc&#39;])) model = MnistModel() . result0 = evaluate(model, val_loader) result0 . {&#39;val_loss&#39;: 2.283280611038208, &#39;val_acc&#39;: 0.10749604552984238} . Train the model . history1 = fit(5, 0.001, model, train_loader, val_loader) . Epoch [0], val_loss: 1.9208, val_acc: 0.6665 Epoch [1], val_loss: 1.6565, val_acc: 0.7396 Epoch [2], val_loss: 1.4599, val_acc: 0.7663 Epoch [3], val_loss: 1.3116, val_acc: 0.7836 Epoch [4], val_loss: 1.1978, val_acc: 0.7975 . history2 = fit(5, 0.001, model, train_loader, val_loader) . Epoch [0], val_loss: 1.1083, val_acc: 0.8057 Epoch [1], val_loss: 1.0365, val_acc: 0.8137 Epoch [2], val_loss: 0.9777, val_acc: 0.8194 Epoch [3], val_loss: 0.9287, val_acc: 0.8240 Epoch [4], val_loss: 0.8873, val_acc: 0.8287 . history3 = fit(5, 0.001, model, train_loader, val_loader) . Epoch [0], val_loss: 0.8517, val_acc: 0.8326 Epoch [1], val_loss: 0.8210, val_acc: 0.8360 Epoch [2], val_loss: 0.7939, val_acc: 0.8392 Epoch [3], val_loss: 0.7700, val_acc: 0.8419 Epoch [4], val_loss: 0.7487, val_acc: 0.8445 . history4 = fit(5, 0.001, model, train_loader, val_loader) . Epoch [0], val_loss: 0.7295, val_acc: 0.8467 Epoch [1], val_loss: 0.7123, val_acc: 0.8480 Epoch [2], val_loss: 0.6966, val_acc: 0.8495 Epoch [3], val_loss: 0.6822, val_acc: 0.8513 Epoch [4], val_loss: 0.6691, val_acc: 0.8527 . Visualization . history = [result0] + history1 + history2 + history3 + history4 accuracies = [result[&#39;val_acc&#39;] for result in history] plt.plot(accuracies, &#39;-x&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;accuracy&#39;) plt.title(&#39;Accuarcy vs No of epochs&#39;); . Testing on Individual Images . from torchvision import transforms . test_dataset = MNIST(root=&#39;data/&#39;, train = False, transform = transforms.ToTensor()) . img, label = test_dataset[0] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;shape&#39;, img.shape) print(&#39;label&#39;,label) . shape torch.Size([1, 28, 28]) label 7 . def predict_image(img, model): xb = img.unsqueeze(0) yb = model(xb) _, preds = torch.max(yb, dim=1) return preds[0].item() . img.unsqueeze simply adds another dimension at the begining of the 1x28x28 tensor, making it a 1x1x28x28 tensor, which the model views as a batch containing a single image. . img, label = test_dataset[0] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;label:&#39;,label, &#39;, predicted:&#39;,predict_image(img, model)) . label: 7 , predicted: 7 . img, label = test_dataset[1] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;label:&#39;,label, &#39;, predicted:&#39;,predict_image(img, model)) . label: 2 , predicted: 2 . img, label = test_dataset[5] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;label:&#39;,label, &#39;, predicted:&#39;,predict_image(img, model)) . label: 1 , predicted: 1 . img, label = test_dataset[193] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;label:&#39;,label, &#39;, predicted:&#39;,predict_image(img, model)) . label: 9 , predicted: 9 . img, label = test_dataset[1839] plt.imshow(img[0], cmap=&#39;gray&#39;) print(&#39;label:&#39;,label, &#39;, predicted:&#39;,predict_image(img, model)) # Here Model in Breaking Up . label: 2 , predicted: 8 . test_loader = DataLoader(test_dataset, batch_size=256) result = evaluate(model, test_loader) result . {&#39;val_loss&#39;: 0.6401068568229675, &#39;val_acc&#39;: 0.8603515625} . Saving and Loading the Model . torch.save(model.state_dict(), &#39;mnist-logistic.pth&#39;) # .state_dict returns Ordered Dict containig all the weights and bias matrices mapped to right attributes of the model . model.state_dict() . OrderedDict([(&#39;linear.weight&#39;, tensor([[ 0.0064, -0.0261, 0.0092, ..., 0.0320, 0.0143, 0.0129], [-0.0179, 0.0100, 0.0158, ..., 0.0125, -0.0167, 0.0209], [ 0.0081, 0.0290, -0.0355, ..., 0.0334, -0.0284, -0.0074], ..., [-0.0097, -0.0077, 0.0133, ..., 0.0153, -0.0036, 0.0283], [ 0.0289, -0.0290, -0.0317, ..., -0.0190, 0.0308, -0.0353], [-0.0043, 0.0184, -0.0096, ..., 0.0319, -0.0038, -0.0067]])), (&#39;linear.bias&#39;, tensor([-0.0544, 0.1269, 0.0047, -0.0240, 0.0016, 0.0284, -0.0175, 0.0452, -0.0609, -0.0155]))]) . model_2 = MnistModel() model_2.load_state_dict(torch.load(&#39;mnist-logistic.pth&#39;)) model_2.state_dict() . OrderedDict([(&#39;linear.weight&#39;, tensor([[ 0.0064, -0.0261, 0.0092, ..., 0.0320, 0.0143, 0.0129], [-0.0179, 0.0100, 0.0158, ..., 0.0125, -0.0167, 0.0209], [ 0.0081, 0.0290, -0.0355, ..., 0.0334, -0.0284, -0.0074], ..., [-0.0097, -0.0077, 0.0133, ..., 0.0153, -0.0036, 0.0283], [ 0.0289, -0.0290, -0.0317, ..., -0.0190, 0.0308, -0.0353], [-0.0043, 0.0184, -0.0096, ..., 0.0319, -0.0038, -0.0067]])), (&#39;linear.bias&#39;, tensor([-0.0544, 0.1269, 0.0047, -0.0240, 0.0016, 0.0284, -0.0175, 0.0452, -0.0609, -0.0155]))]) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/06/01/Logistic-MNIST-Pytorch.html",
            "relUrl": "/2021/06/01/Logistic-MNIST-Pytorch.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Introduction to Pytorch",
            "content": "import torch . Tensors . Tenson is a number, vector, matrix, or any N- Dimensional array. . t1 = torch.tensor(4.) t1 . tensor(4.) . t1.dtype . torch.float32 . t2 = torch.tensor([1., 2, 3, 4]) print(t2) # ALl the tensor elements are of same data type # Matrix - 2D tensor t3 = torch.tensor([[5, 6], [7,8], [9,10]]) print(t3) . tensor([1., 2., 3., 4.]) tensor([[ 5, 6], [ 7, 8], [ 9, 10]]) . # most of the time we will use floating point numbers t4 = torch.tensor([ [[11,12,13], [13,14,15]], [[15,16,17], [17,18,19.]] ]) t4 . tensor([[[11., 12., 13.], [13., 14., 15.]], [[15., 16., 17.], [17., 18., 19.]]]) . Tensors can have any number of dimensions and different lengths along each dimension. we can inspect length using .shape property . print(t1) t1.shape . tensor(4.) . torch.Size([]) . print(t2) t2.shape . tensor([1., 2., 3., 4.]) . torch.Size([4]) . print(t3) t3.shape . tensor([[ 5, 6], [ 7, 8], [ 9, 10]]) . torch.Size([3, 2]) . print(t4) t4.shape # start from outer most bracket and count number of elements in that ie. here 2 elements both matrices than we go 1 bracket in and there is also 2 list elements and so on ..... . tensor([[[11., 12., 13.], [13., 14., 15.]], [[15., 16., 17.], [17., 18., 19.]]]) . torch.Size([2, 2, 3]) . we can not make a tensor with an improper shape . Tensor Operations and Gradients . x = torch.tensor(3.) w = torch.tensor(4., requires_grad=True) # b = torch.tensor(5., requires_grad=True) x,w,b . (tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True)) . y = w * x + b y . tensor(17., grad_fn=&lt;AddBackward0&gt;) . y.backward() # derivatives of y w.r.t each of input tensors are stored in .grad property of respective tensors . print(&#39;dy/dx&#39;, x.grad) print(&#39;dy/dw&#39;, w.grad) print(&#39;dy/db&#39;, b.grad) . dy/dx None dy/dw tensor(3.) dy/db tensor(1.) . we have not specified requires_grad=True in x, this tells pytorch that we are not intrested in drivatives of any future output w.r.t x but we are intrested for w and b .... so requires_grad property is important to save millions of usless coputations of derrivatives as per requirement . &quot;grad&quot; in w.grad is short for gradient, which is another term for derivative primarily used while dealing with vectors and matrices . Tensor Functions . t6 = torch.full((3,2), 42) t6 . tensor([[42, 42], [42, 42], [42, 42]]) . t7 = torch.cat((t3,t6)) t7 . tensor([[ 5, 6], [ 7, 8], [ 9, 10], [42, 42], [42, 42], [42, 42]]) . t8 = torch.sin(t7) t8 . tensor([[-0.9589, -0.2794], [ 0.6570, 0.9894], [ 0.4121, -0.5440], [-0.9165, -0.9165], [-0.9165, -0.9165], [-0.9165, -0.9165]]) . t9 = t8.reshape(3,2,2) t9 . tensor([[[-0.9589, -0.2794], [ 0.6570, 0.9894]], [[ 0.4121, -0.5440], [-0.9165, -0.9165]], [[-0.9165, -0.9165], [-0.9165, -0.9165]]]) . Inter-operability with Numpy . import numpy as np . x = np.array([[1,2],[3, 4.]]) x . array([[1., 2.], [3., 4.]]) . y = torch.from_numpy(x) y . tensor([[1., 2.], [3., 4.]], dtype=torch.float64) . x.dtype, y.dtype . (dtype(&#39;float64&#39;), torch.float64) . z = y.numpy() z . array([[1., 2.], [3., 4.]]) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/05/29/PyTorch-Basics.html",
            "relUrl": "/2021/05/29/PyTorch-Basics.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Implementation of Linear Regression and Gradient Descent using Pytorch",
            "content": "Linear regression. model that predicts crop yields for apples and oranges (target variables) by looking at the average temperature, rainfall, and humidity (input variables or features) in a region. Here&#39;s the training data: . . In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias : . yield_apple = w11 * temp + w12 * rainfall + w13 * humidity + b1 yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2 . Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity: . . import torch import numpy as np . Training Data . inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70]], dtype=&#39;float32&#39;) . targets = np.array([[56,70], [81, 101], [119, 133], [22, 37], [103, 119]], dtype=&#39;float32&#39;) . inputs = torch.from_numpy(inputs) targets = torch.from_numpy(targets) print(inputs) print(targets) . tensor([[ 73., 67., 43.], [ 91., 88., 64.], [ 87., 134., 58.], [102., 43., 37.], [ 69., 96., 70.]]) tensor([[ 56., 70.], [ 81., 101.], [119., 133.], [ 22., 37.], [103., 119.]]) . Linear Regression Model from Scratch . w = torch.randn(2, 3, requires_grad=True) # torch.randn : creates a tensor with givent shape with random elements picked with normal distribution b = torch.randn(2, requires_grad= True) print(w) print(b) . tensor([[ 0.0728, -2.0486, 0.2053], [ 1.4556, -1.4721, -1.4280]], requires_grad=True) tensor([-2.6483, -2.7893], requires_grad=True) . Our model is just X * W_transpose + Bias . def model(x): return x @ w.t() + b # @-&gt; matrix multiplication in pytorch, .t() returns the transpose of a tensor . inputs @ w.t() + b . tensor([[-125.7638, -56.5679], [-163.1629, -91.2699], [-258.9209, -156.2401], [ -75.7193, 29.5415], [-179.9207, -143.6369]], grad_fn=&lt;AddBackward0&gt;) . preds = model(inputs) preds . tensor([[-125.7638, -56.5679], [-163.1629, -91.2699], [-258.9209, -156.2401], [ -75.7193, 29.5415], [-179.9207, -143.6369]], grad_fn=&lt;AddBackward0&gt;) . print(targets) . tensor([[ 56., 70.], [ 81., 101.], [119., 133.], [ 22., 37.], [103., 119.]]) . diff = preds - targets # diff * diff # * means element wise multiplication not matrix multiplication torch.sum(diff*diff) / diff.numel() # numel -&gt; number of element in diff matrix . tensor(53075.1758, grad_fn=&lt;DivBackward0&gt;) . Loss function . MSE Loss :- On average, each element in prediction differs from the actual target by the square root of the loss . def mse(t1,t2): diff = t1 - t2 return torch.sum(diff * diff) / diff.numel() . loss = mse(preds, targets) print(loss) . tensor(53075.1758, grad_fn=&lt;DivBackward0&gt;) . loss.backward() . print(w) print(w.grad) # derivative of the loss w.r.t element in w . tensor([[ 0.0728, -2.0486, 0.2053], [ 1.4556, -1.4721, -1.4280]], requires_grad=True) tensor([[-19571.1211, -23133.6465, -13756.3496], [-14156.5244, -17938.3672, -10636.8340]]) . print(b) print(b.grad) . tensor([-2.6483, -2.7893], requires_grad=True) tensor([-236.8975, -175.6347]) . Grad of loss w.r.t each element in tensor indicates the rate of change of loss or slope of the loss function . we can substract from each weight element a small quantity proportional to the derivative of the loss w.r.t that element to reduce the loss slightly . print(w) w.grad . tensor([[ 0.0728, -2.0486, 0.2053], [ 1.4556, -1.4721, -1.4280]], requires_grad=True) . tensor([[-19571.1211, -23133.6465, -13756.3496], [-14156.5244, -17938.3672, -10636.8340]]) . print(w) w.grad * 1e-5 # new weights to near w . tensor([[ 0.0728, -2.0486, 0.2053], [ 1.4556, -1.4721, -1.4280]], requires_grad=True) . tensor([[-0.1957, -0.2313, -0.1376], [-0.1416, -0.1794, -0.1064]]) . with torch.no_grad(): w -= w.grad * 1e-5 # 1e-5 is the step ie small coz loss is large.....Learning Rate b -= b.grad * 1e-5 . torch.no_grad() to indicate to Pytorch that we shouldn&#39;t take track, calculate, or modify gradients while updating the weights and biases . w, b . (tensor([[ 0.2685, -1.8173, 0.3429], [ 1.5971, -1.2927, -1.3216]], requires_grad=True), tensor([-2.6459, -2.7876], requires_grad=True)) . preds = model(inputs) loss = mse(preds, targets) print(loss) . tensor(37202.4609, grad_fn=&lt;DivBackward0&gt;) . Now reset the gradients to 0 . w.grad.zero_() b.grad.zero_() print(w.grad) print(b.grad) . tensor([[0., 0., 0.], [0., 0., 0.]]) tensor([0., 0.]) . Train the Model using Gradient descent . preds = model(inputs) print(preds) . tensor([[ -90.0597, -29.6393], [-116.1892, -55.7924], [-202.9139, -113.7154], [ -40.7170, 55.6320], [-134.5765, -109.2006]], grad_fn=&lt;AddBackward0&gt;) . loss = mse(preds, targets) print(loss) . tensor(37202.4609, grad_fn=&lt;DivBackward0&gt;) . loss.backward() print(w.grad) print(b.grad) . tensor([[-15880.6006, -19155.8594, -11304.5137], [-11370.2803, -14927.9023, -8782.6719]]) tensor([-193.0913, -142.5432]) . update the weights and biases using gradientdescent . with torch.no_grad(): w -= w.grad * 1e-5 b -= b.grad * 1e-5 w.grad.zero_() b.grad.zero_() . print(w) print(b) . tensor([[ 0.4273, -1.6257, 0.4559], [ 1.7108, -1.1434, -1.2338]], requires_grad=True) tensor([-2.6440, -2.7862], requires_grad=True) . preds = model(inputs) loss = mse(preds, targets) print(loss) . tensor(26488.4434, grad_fn=&lt;DivBackward0&gt;) . Train on multiple Epochs . for i in range(100): preds = model(inputs) loss = mse(preds, targets) loss.backward() with torch.no_grad(): w -= w.grad * 1e-5 b -= b.grad * 1e-5 w.grad.zero_() b.grad.zero_() . preds = model(inputs) loss = mse(preds, targets) print(loss) . tensor(1333.2324, grad_fn=&lt;DivBackward0&gt;) . print(preds) print(targets) . tensor([[ 65.7250, 83.3404], [ 92.7282, 99.8337], [ 80.9948, 113.9022], [ 73.8770, 114.4924], [ 88.8938, 71.9206]], grad_fn=&lt;AddBackward0&gt;) tensor([[ 56., 70.], [ 81., 101.], [119., 133.], [ 22., 37.], [103., 119.]]) . Linear Regression using Pytorch built-ins . import torch.nn as nn . inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70], [74, 66, 43], [91, 87, 65], [88, 134, 59], [101, 44, 37], [68, 96, 71], [73, 66, 44], [92, 87, 64], [87, 135, 57], [103, 43 ,36], [68, 97, 70]], dtype=&#39;float32&#39;) targets = np.array([[56,70], [81, 101], [119, 133], [22, 37], [103, 119], [57,69], [80,102], [118, 132], [21, 38], [104, 118], [57, 69], [82, 100], [118, 134], [20, 38], [102, 120]], dtype=&#39;float32&#39;) inputs = torch.from_numpy(inputs) targets = torch.from_numpy(targets) . print(inputs) print(targets) . tensor([[ 73., 67., 43.], [ 91., 88., 64.], [ 87., 134., 58.], [102., 43., 37.], [ 69., 96., 70.], [ 74., 66., 43.], [ 91., 87., 65.], [ 88., 134., 59.], [101., 44., 37.], [ 68., 96., 71.], [ 73., 66., 44.], [ 92., 87., 64.], [ 87., 135., 57.], [103., 43., 36.], [ 68., 97., 70.]]) tensor([[ 56., 70.], [ 81., 101.], [119., 133.], [ 22., 37.], [103., 119.], [ 57., 69.], [ 80., 102.], [118., 132.], [ 21., 38.], [104., 118.], [ 57., 69.], [ 82., 100.], [118., 134.], [ 20., 38.], [102., 120.]]) . Dataset and DataLoader . creating a TensorDataset, which allows access to rows from inputs and targets as tuples and provide standard APIs for working many different types pf datasets in Pytorch . from torch.utils.data import TensorDataset . train_ds = TensorDataset(inputs, targets) train_ds[0:3] # 0 to 3-1 . (tensor([[ 73., 67., 43.], [ 91., 88., 64.], [ 87., 134., 58.]]), tensor([[ 56., 70.], [ 81., 101.], [119., 133.]])) . from torch.utils.data import DataLoader . batch_size = 5 train_dl = DataLoader(train_ds, batch_size, shuffle=True) . inputs . tensor([[ 73., 67., 43.], [ 91., 88., 64.], [ 87., 134., 58.], [102., 43., 37.], [ 69., 96., 70.], [ 74., 66., 43.], [ 91., 87., 65.], [ 88., 134., 59.], [101., 44., 37.], [ 68., 96., 71.], [ 73., 66., 44.], [ 92., 87., 64.], [ 87., 135., 57.], [103., 43., 36.], [ 68., 97., 70.]]) . for xb, yb in train_dl: print(xb) print(yb) break . tensor([[102., 43., 37.], [ 91., 87., 65.], [ 69., 96., 70.], [ 88., 134., 59.], [ 74., 66., 43.]]) tensor([[ 22., 37.], [ 80., 102.], [103., 119.], [118., 132.], [ 57., 69.]]) . nn.Linear . Instead of initialising the weights and biases manually, we can define the model using the nn.Linear . model = nn.Linear(3, 2) print(model.weight) print(model.bias) . Parameter containing: tensor([[-0.1637, 0.0519, -0.1459], [-0.2050, 0.2159, -0.0023]], requires_grad=True) Parameter containing: tensor([-0.1157, -0.1562], requires_grad=True) . list(model.parameters()) . [Parameter containing: tensor([[-0.1637, 0.0519, -0.1459], [-0.2050, 0.2159, -0.0023]], requires_grad=True), Parameter containing: tensor([-0.1157, -0.1562], requires_grad=True)] . preds = model(inputs) . preds . tensor([[-14.8669, -0.7540], [-19.7889, 0.0420], [-15.8733, 10.8090], [-19.9838, -11.8685], [-16.6477, 6.2663], [-15.0825, -1.1750], [-19.9867, -0.1762], [-16.1830, 10.6017], [-19.7683, -11.4475], [-16.6298, 6.4690], [-15.0646, -0.9722], [-20.0045, -0.3789], [-15.6756, 11.0272], [-20.0016, -12.0712], [-16.4321, 6.6873]], grad_fn=&lt;AddmmBackward&gt;) . Loss Function . import torch.nn.functional as F . loss_fn = F.mse_loss . loss = loss_fn(model(inputs), targets) print(loss) . tensor(9453.6309, grad_fn=&lt;MseLossBackward&gt;) . Optimizer . we will use stochastic gradient descent -&gt; optim.SGD . opt = torch.optim.SGD(model.parameters(), lr=1e-5) #lr is the learning rate . Train the Model . def fit(num_epochs, model, loss_fn, opt, train_dl): for epoch in range(num_epochs): for xb, xy in train_dl: pred = model(xb) # Generate Predictions loss = loss_fn(pred, yb) # calculate loss loss.backward() # compute gradient opt.step() # update parameters using gradient opt.zero_grad() # reset the gradient to zero if (epoch+1) % 10 == 0: print(&#39;Epoch [{}/{}], Loss: {:.4f}&#39;.format(epoch+1, num_epochs, loss.item())) . fit(100, model, loss_fn, opt, train_dl) . Epoch [10/100], Loss: 1473.9495 Epoch [20/100], Loss: 1101.0323 Epoch [30/100], Loss: 1247.5220 Epoch [40/100], Loss: 1066.2527 Epoch [50/100], Loss: 1192.7886 Epoch [60/100], Loss: 1239.0150 Epoch [70/100], Loss: 916.5994 Epoch [80/100], Loss: 986.2520 Epoch [90/100], Loss: 1190.9945 Epoch [100/100], Loss: 1572.8744 . preds = model(inputs) . preds . tensor([[ 62.2054, 75.3431], [ 81.8397, 99.3478], [ 75.8782, 91.9953], [ 78.3076, 94.4091], [ 70.4413, 85.9155], [ 62.8510, 76.1146], [ 82.2799, 99.9035], [ 76.9230, 93.2708], [ 77.6620, 93.6375], [ 70.2358, 85.6997], [ 62.6455, 75.8988], [ 82.4854, 100.1193], [ 75.4381, 91.4396], [ 78.5131, 94.6249], [ 69.7957, 85.1440]], grad_fn=&lt;AddmmBackward&gt;) . targets . tensor([[ 56., 70.], [ 81., 101.], [119., 133.], [ 22., 37.], [103., 119.], [ 57., 69.], [ 80., 102.], [118., 132.], [ 21., 38.], [104., 118.], [ 57., 69.], [ 82., 100.], [118., 134.], [ 20., 38.], [102., 120.]]) . Random input Batch . model(torch.tensor([[75, 63, 44.]])) # we&#39;ll get a batch of output . tensor([[63.9573, 77.4678]], grad_fn=&lt;AddmmBackward&gt;) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/05/28/Torch-LR-GD.html",
            "relUrl": "/2021/05/28/Torch-LR-GD.html",
            "date": " • May 28, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Biot Savard Law using Plotly",
            "content": "import plotly import numpy as np import matplotlib.pyplot as plt from scipy.integrate import quad import plotly.graph_objects as go from plotly.offline import plot from IPython.core.display import display,HTML import sympy as smp from sympy.vector import cross . phi = np.linspace(0, 2*np.pi, 100) def l(phi): return (1+3/4 * np.sin(3*phi)) * np.array([np.cos(phi), np.sin(phi), np.zeros(len(phi))]) . lx, ly, lz = l(phi) . plt.figure(figsize=(7,7)) plt.plot(lx,ly) plt.xlabel(&#39;$x/R$&#39;, fontsize=25) plt.ylabel(&#39;$y/R$&#39;, fontsize=25) plt.show() . solve integrand using sympy . t, x, y, z = smp.symbols(&#39;t, x, y, z&#39;) . l = (1+(3/4)*smp.sin(3*t))*smp.Matrix([smp.cos(t), smp.sin(t), 0]) r = smp.Matrix([x,y,z]) sep=r-l . l . $ displaystyle left[ begin{matrix} left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)} left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)} 0 end{matrix} right]$ r . $ displaystyle left[ begin{matrix}x y z end{matrix} right]$ sep . $ displaystyle left[ begin{matrix}x - left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)} y - left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)} z end{matrix} right]$ Define integrand . integrand = smp.diff(l,t).cross(sep) / sep.norm()**3 . Get x, y z components of the integrand . integrand . $ displaystyle left[ begin{matrix} frac{z left( left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)} + 2.25 sin{ left(t right)} cos{ left(3 t right)} right)}{ left( left|{z} right|^{2} + left|{x - left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)}} right|^{2} + left|{y - left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)}} right|^{2} right)^{ frac{3}{2}}} - frac{z left(- left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)} + 2.25 cos{ left(t right)} cos{ left(3 t right)} right)}{ left( left|{z} right|^{2} + left|{x - left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)}} right|^{2} + left|{y - left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)}} right|^{2} right)^{ frac{3}{2}}} frac{- left(x - left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)} right) left( left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)} + 2.25 sin{ left(t right)} cos{ left(3 t right)} right) + left(y - left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)} right) left(- left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)} + 2.25 cos{ left(t right)} cos{ left(3 t right)} right)}{ left( left|{z} right|^{2} + left|{x - left(0.75 sin{ left(3 t right)} + 1 right) cos{ left(t right)}} right|^{2} + left|{y - left(0.75 sin{ left(3 t right)} + 1 right) sin{ left(t right)}} right|^{2} right)^{ frac{3}{2}}} end{matrix} right]$ dBxdt = smp.lambdify([t,x,y,z], integrand[0]) dBydt = smp.lambdify([t,x,y,z], integrand[1]) dBzdt = smp.lambdify([t,x,y,z], integrand[2]) . dBxdt(np.pi, 1, 1, 1) . -0.0680413817439772 . quad(dBxdt, 0, 2*np.pi, args=(1,1,1)) # gives value and error . (0.367215052854198, 6.916483780662426e-09) . Get the magnetic field by performing the integral over each component . def B(x,y,z): return np.array([quad(dBxdt, 0, 2*np.pi, args=(x,y,z))[0], quad(dBydt, 0, 2*np.pi, args=(x,y,z))[0], quad(dBzdt, 0, 2*np.pi, args=(x,y,z))[0]]) . B(0.5,0.5,0) . array([ 0. , 0. , 10.87779227]) . B(0.5,0.5,1) . array([0.19069963, 0.52786431, 1.52524645]) . Set up a meshgrid to solve for the field in some 3D volume . x = np.linspace(-2,2,20) xv,yv,zv = np.meshgrid(x,x,x) . B_field = np.vectorize(B, signature=&#39;(),(),()-&gt;(n)&#39;)(xv,yv,zv) Bx = B_field[:,:,:,0] By = B_field[:,:,:,1] Bz = B_field[:,:,:,2] . use plotly for 3D intractive plot . xv.ravel() . array([-2., -2., -2., ..., 2., 2., 2.]) . data = go.Cone(x=xv.ravel(), y=yv.ravel(), z=zv.ravel(), u=Bx.ravel(), v=By.ravel(), w=Bz.ravel(), colorscale=&#39;Inferno&#39;, colorbar=dict(title=&#39;$x^2$&#39;), sizemode=&quot;absolute&quot;, sizeref=20) layout = go.Layout(title=r&#39;Biot Savard Law &#39;, scene=dict(xaxis_title=r&#39;x&#39;, yaxis_title=r&#39;y&#39;, zaxis_title=r&#39;z&#39;, aspectratio=dict(x=1, y=1, z=1), camera_eye=dict(x=1.2, y=1.2, z=1.2))) fig = go.Figure(data = data, layout=layout) fig.add_scatter3d(x=lx, y=ly, z=lz, mode=&#39;lines&#39;, line = dict(color=&#39;green&#39;, width=10)) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/05/14/Biot-Savart-Law.html",
            "relUrl": "/2021/05/14/Biot-Savart-Law.html",
            "date": " • May 14, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Car Price Prediction using Random Forest Regressor",
            "content": "Problem Statement: . A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts. Which variables are significant in predicting the price of a car . Dataset link :- . https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho . Data Preprocessing . import pandas as pd . df = pd.read_csv(&#39;car data.csv&#39;) . df.head() . Car_Name Year Selling_Price Present_Price Kms_Driven Fuel_Type Seller_Type Transmission Owner . 0 ritz | 2014 | 3.35 | 5.59 | 27000 | Petrol | Dealer | Manual | 0 | . 1 sx4 | 2013 | 4.75 | 9.54 | 43000 | Diesel | Dealer | Manual | 0 | . 2 ciaz | 2017 | 7.25 | 9.85 | 6900 | Petrol | Dealer | Manual | 0 | . 3 wagon r | 2011 | 2.85 | 4.15 | 5200 | Petrol | Dealer | Manual | 0 | . 4 swift | 2014 | 4.60 | 6.87 | 42450 | Diesel | Dealer | Manual | 0 | . df.shape . (301, 9) . print(df[&quot;Seller_Type&quot;].unique()) print(df[&quot;Transmission&quot;].unique()) print(df[&quot;Owner&quot;].unique()) . [&#39;Dealer&#39; &#39;Individual&#39;] [&#39;Manual&#39; &#39;Automatic&#39;] [0 1 3] . df.isnull().sum() . Car_Name 0 Year 0 Selling_Price 0 Present_Price 0 Kms_Driven 0 Fuel_Type 0 Seller_Type 0 Transmission 0 Owner 0 dtype: int64 . df.describe() . Year Selling_Price Present_Price Kms_Driven Owner . count 301.000000 | 301.000000 | 301.000000 | 301.000000 | 301.000000 | . mean 2013.627907 | 4.661296 | 7.628472 | 36947.205980 | 0.043189 | . std 2.891554 | 5.082812 | 8.644115 | 38886.883882 | 0.247915 | . min 2003.000000 | 0.100000 | 0.320000 | 500.000000 | 0.000000 | . 25% 2012.000000 | 0.900000 | 1.200000 | 15000.000000 | 0.000000 | . 50% 2014.000000 | 3.600000 | 6.400000 | 32000.000000 | 0.000000 | . 75% 2016.000000 | 6.000000 | 9.900000 | 48767.000000 | 0.000000 | . max 2018.000000 | 35.000000 | 92.600000 | 500000.000000 | 3.000000 | . final_dataset=df[[&#39;Year&#39;, &#39;Selling_Price&#39;, &#39;Present_Price&#39;, &#39;Kms_Driven&#39;, &#39;Fuel_Type&#39;, &#39;Seller_Type&#39;, &#39;Transmission&#39;, &#39;Owner&#39;]] . final_dataset[&#39;Current_Year&#39;]=2021 . final_dataset[&#39;no_year&#39;]=final_dataset[&#39;Current_Year&#39;]-final_dataset[&#39;Year&#39;] . final_dataset.head() . Year Selling_Price Present_Price Kms_Driven Fuel_Type Seller_Type Transmission Owner Current_Year no_year . 0 2014 | 3.35 | 5.59 | 27000 | Petrol | Dealer | Manual | 0 | 2021 | 7 | . 1 2013 | 4.75 | 9.54 | 43000 | Diesel | Dealer | Manual | 0 | 2021 | 8 | . 2 2017 | 7.25 | 9.85 | 6900 | Petrol | Dealer | Manual | 0 | 2021 | 4 | . 3 2011 | 2.85 | 4.15 | 5200 | Petrol | Dealer | Manual | 0 | 2021 | 10 | . 4 2014 | 4.60 | 6.87 | 42450 | Diesel | Dealer | Manual | 0 | 2021 | 7 | . final_dataset.drop([&#39;Year&#39;],axis=1,inplace=True) . final_dataset.drop([&#39;Current_Year&#39;],axis=1,inplace=True) . final_dataset=pd.get_dummies(final_dataset,drop_first=True) . final_dataset.head() . Selling_Price Present_Price Kms_Driven Owner no_year Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . 0 3.35 | 5.59 | 27000 | 0 | 7 | 0 | 1 | 0 | 1 | . 1 4.75 | 9.54 | 43000 | 0 | 8 | 1 | 0 | 0 | 1 | . 2 7.25 | 9.85 | 6900 | 0 | 4 | 0 | 1 | 0 | 1 | . 3 2.85 | 4.15 | 5200 | 0 | 10 | 0 | 1 | 0 | 1 | . 4 4.60 | 6.87 | 42450 | 0 | 7 | 1 | 0 | 0 | 1 | . final_dataset.corr() . Selling_Price Present_Price Kms_Driven Owner no_year Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . Selling_Price 1.000000 | 0.878983 | 0.029187 | -0.088344 | -0.236141 | 0.552339 | -0.540571 | -0.550724 | -0.367128 | . Present_Price 0.878983 | 1.000000 | 0.203647 | 0.008057 | 0.047584 | 0.473306 | -0.465244 | -0.512030 | -0.348715 | . Kms_Driven 0.029187 | 0.203647 | 1.000000 | 0.089216 | 0.524342 | 0.172515 | -0.172874 | -0.101419 | -0.162510 | . Owner -0.088344 | 0.008057 | 0.089216 | 1.000000 | 0.182104 | -0.053469 | 0.055687 | 0.124269 | -0.050316 | . no_year -0.236141 | 0.047584 | 0.524342 | 0.182104 | 1.000000 | -0.064315 | 0.059959 | 0.039896 | -0.000394 | . Fuel_Type_Diesel 0.552339 | 0.473306 | 0.172515 | -0.053469 | -0.064315 | 1.000000 | -0.979648 | -0.350467 | -0.098643 | . Fuel_Type_Petrol -0.540571 | -0.465244 | -0.172874 | 0.055687 | 0.059959 | -0.979648 | 1.000000 | 0.358321 | 0.091013 | . Seller_Type_Individual -0.550724 | -0.512030 | -0.101419 | 0.124269 | 0.039896 | -0.350467 | 0.358321 | 1.000000 | 0.063240 | . Transmission_Manual -0.367128 | -0.348715 | -0.162510 | -0.050316 | -0.000394 | -0.098643 | 0.091013 | 0.063240 | 1.000000 | . Data-Visualisation . import seaborn as sns . sns.pairplot(final_dataset) . &lt;seaborn.axisgrid.PairGrid at 0x21f6bf7bb48&gt; . import matplotlib.pyplot as plt %matplotlib inline . corrmat = final_dataset.corr() top_corr_features=corrmat.index plt.figure(figsize=(20,20)) #plot heat map g = sns.heatmap(final_dataset[top_corr_features].corr(),annot=True, cmap=&#39;RdYlGn&#39;) . final_dataset.head() . Selling_Price Present_Price Kms_Driven Owner no_year Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . 0 3.35 | 5.59 | 27000 | 0 | 7 | 0 | 1 | 0 | 1 | . 1 4.75 | 9.54 | 43000 | 0 | 8 | 1 | 0 | 0 | 1 | . 2 7.25 | 9.85 | 6900 | 0 | 4 | 0 | 1 | 0 | 1 | . 3 2.85 | 4.15 | 5200 | 0 | 10 | 0 | 1 | 0 | 1 | . 4 4.60 | 6.87 | 42450 | 0 | 7 | 1 | 0 | 0 | 1 | . X=final_dataset.iloc[:,1:] y=final_dataset.iloc[:,0] . X.head() . Present_Price Kms_Driven Owner no_year Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . 0 5.59 | 27000 | 0 | 7 | 0 | 1 | 0 | 1 | . 1 9.54 | 43000 | 0 | 8 | 1 | 0 | 0 | 1 | . 2 9.85 | 6900 | 0 | 4 | 0 | 1 | 0 | 1 | . 3 4.15 | 5200 | 0 | 10 | 0 | 1 | 0 | 1 | . 4 6.87 | 42450 | 0 | 7 | 1 | 0 | 0 | 1 | . y.head() . 0 3.35 1 4.75 2 7.25 3 2.85 4 4.60 Name: Selling_Price, dtype: float64 . from sklearn.ensemble import ExtraTreesRegressor model=ExtraTreesRegressor() model.fit(X,y) . ExtraTreesRegressor() . print(model.feature_importances_) . [3.83348531e-01 4.08560046e-02 3.73461245e-04 7.61985654e-02 2.28609393e-01 1.08920232e-02 1.22151396e-01 1.37570626e-01] . feat_importances = pd.Series(model.feature_importances_, index=X.columns) feat_importances.nlargest(5).plot(kind=&#39;barh&#39;) plt.show() . Train Test Split . from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2) . X_train . Present_Price Kms_Driven Owner no_year Fuel_Type_Diesel Fuel_Type_Petrol Seller_Type_Individual Transmission_Manual . 174 0.72 | 38600 | 0 | 6 | 0 | 1 | 1 | 1 | . 144 0.99 | 25000 | 0 | 7 | 0 | 1 | 1 | 1 | . 212 13.60 | 22671 | 0 | 5 | 0 | 1 | 0 | 1 | . 196 0.52 | 500000 | 0 | 13 | 0 | 1 | 1 | 0 | . 243 7.60 | 7000 | 0 | 5 | 0 | 1 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 181 0.48 | 50000 | 0 | 5 | 0 | 1 | 1 | 1 | . 191 0.57 | 25000 | 1 | 9 | 0 | 1 | 1 | 1 | . 23 3.46 | 45280 | 0 | 7 | 0 | 1 | 0 | 1 | . 159 0.51 | 4000 | 0 | 4 | 0 | 1 | 1 | 0 | . 70 6.76 | 71000 | 0 | 7 | 1 | 0 | 0 | 1 | . 240 rows × 8 columns . X_train.shape . (240, 8) . ML model preparation . from sklearn.ensemble import RandomForestRegressor rf_random = RandomForestRegressor() . import numpy as np n_estimators=[int(x) for x in np.linspace(start = 100, stop = 1200, num =12 )] print(n_estimators) . [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200] . #Randomized Search CV # Number of trees in random forest n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)] # Number of features to consider at every split max_features = [&#39;auto&#39;, &#39;sqrt&#39;] # Maximum number of levels in tree max_depth = [int(x) for x in np.linspace(5, 30, num = 6)] # max_depth.append(None) # Minimum number of samples required to split a node min_samples_split = [2, 5, 10, 15, 100] # Minimum number of samples required at each leaf node min_samples_leaf = [1, 2, 5, 10] . random_grid = {&#39;n_estimators&#39;: n_estimators, &#39;max_features&#39;: max_features, &#39;max_depth&#39;: max_depth, &#39;min_samples_split&#39;: min_samples_split, &#39;min_samples_leaf&#39;: min_samples_leaf} print(random_grid) . {&#39;n_estimators&#39;: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200], &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;], &#39;max_depth&#39;: [5, 10, 15, 20, 25, 30], &#39;min_samples_split&#39;: [2, 5, 10, 15, 100], &#39;min_samples_leaf&#39;: [1, 2, 5, 10]} . # First create the base model to tune rf = RandomForestRegressor() . from sklearn.model_selection import RandomizedSearchCV . # Random search of parameters, using 3 fold cross validation, # search across 100 different combinations rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring=&#39;neg_mean_squared_error&#39;, n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1) . rf_random.fit(X_train,y_train) . Fitting 5 folds for each of 10 candidates, totalling 50 fits [CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time= 0.7s [CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time= 0.8s [CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time= 0.8s [CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time= 0.7s [CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=5, n_estimators=900; total time= 0.8s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time= 1.0s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time= 1.0s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time= 1.0s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time= 0.9s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=1100; total time= 0.9s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=100, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time= 0.3s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time= 0.3s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time= 0.3s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time= 0.3s [CV] END max_depth=15, max_features=auto, min_samples_leaf=5, min_samples_split=5, n_estimators=400; total time= 0.3s [CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=10, min_samples_split=5, n_estimators=700; total time= 0.6s [CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 0.9s [CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 0.9s [CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 0.9s [CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 0.9s [CV] END max_depth=25, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time= 0.9s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time= 0.9s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time= 0.9s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time= 0.9s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time= 0.9s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=10, min_samples_split=15, n_estimators=1100; total time= 0.9s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time= 0.2s [CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=15, n_estimators=300; total time= 0.2s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time= 0.6s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time= 0.6s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time= 0.5s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time= 0.5s [CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=700; total time= 0.5s [CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time= 0.6s [CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=15, n_estimators=700; total time= 0.6s . RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=1, param_distributions={&#39;max_depth&#39;: [5, 10, 15, 20, 25, 30], &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;], &#39;min_samples_leaf&#39;: [1, 2, 5, 10], &#39;min_samples_split&#39;: [2, 5, 10, 15, 100], &#39;n_estimators&#39;: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200]}, random_state=42, scoring=&#39;neg_mean_squared_error&#39;, verbose=2) . Predictiong the Results . predictions = rf_random.predict(X_test) . predictions . array([ 0.42823578, 6.54336523, 4.51053508, 0.51499616, 5.25339526, 0.41187859, 3.59500812, 7.08705453, 1.1462021 , 21.3181618 , 0.24429459, 4.46844221, 10.77746653, 2.19624475, 2.87045466, 2.78251651, 5.56248702, 4.71332205, 0.64855331, 0.48048546, 1.16921385, 4.98697281, 0.86404259, 0.56195234, 0.26471753, 4.16548545, 4.66760447, 5.54659743, 6.3007795 , 0.54582494, 1.15449372, 7.65211645, 3.23066131, 4.9624319 , 0.54126424, 5.68049317, 3.02435804, 1.14775612, 8.57444514, 4.96809414, 0.63048064, 12.89946909, 13.48483701, 5.83942354, 0.57111473, 2.54267514, 7.08720208, 5.10635913, 5.07656358, 7.00626737, 5.68402798, 5.82973362, 3.03374272, 5.81018912, 21.1221698 , 2.4988902 , 2.56008469, 21.03405001, 5.37708202, 0.62723892, 0.4913718 ]) . sns.distplot(y_test-predictions) . C: Users mrsid anaconda3 envs carprediction lib site-packages seaborn distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;AxesSubplot:xlabel=&#39;Selling_Price&#39;, ylabel=&#39;Density&#39;&gt; . plt.scatter(y_test,predictions) . &lt;matplotlib.collections.PathCollection at 0x21f0eb52748&gt; . import pickle . file = open(&#39;random_forest_regression_model.pkl&#39;, &#39;wb&#39;) # dump information pickle.dump(rf_random, file) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/05/03/car-price-prediction.html",
            "relUrl": "/2021/05/03/car-price-prediction.html",
            "date": " • May 3, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Multicollinearity in Linear Regression",
            "content": "import pandas as pd . Using Advertising Dataset, link :- . https://www.kaggle.com/bumba5341/advertisingcsv . import statsmodels.api as sm df_adv = pd.read_csv(&#39;Advertising.csv&#39;, index_col=0) df_adv.head() . TV radio newspaper sales . 1 230.1 | 37.8 | 69.2 | 22.1 | . 2 44.5 | 39.3 | 45.1 | 10.4 | . 3 17.2 | 45.9 | 69.3 | 9.3 | . 4 151.5 | 41.3 | 58.5 | 18.5 | . 5 180.8 | 10.8 | 58.4 | 12.9 | . X = df_adv[[&#39;TV&#39;,&#39;radio&#39;,&#39;newspaper&#39;]] y = df_adv[&#39;sales&#39;] print(X,y) . TV radio newspaper 1 230.1 37.8 69.2 2 44.5 39.3 45.1 3 17.2 45.9 69.3 4 151.5 41.3 58.5 5 180.8 10.8 58.4 .. ... ... ... 196 38.2 3.7 13.8 197 94.2 4.9 8.1 198 177.0 9.3 6.4 199 283.6 42.0 66.2 200 232.1 8.6 8.7 [200 rows x 3 columns] 1 22.1 2 10.4 3 9.3 4 18.5 5 12.9 ... 196 7.6 197 9.7 198 12.8 199 25.5 200 13.4 Name: sales, Length: 200, dtype: float64 . fit a Oridinirary least square model with intercept on TV and Radio . X = sm.add_constant(X) . X . const TV radio newspaper . 1 1.0 | 230.1 | 37.8 | 69.2 | . 2 1.0 | 44.5 | 39.3 | 45.1 | . 3 1.0 | 17.2 | 45.9 | 69.3 | . 4 1.0 | 151.5 | 41.3 | 58.5 | . 5 1.0 | 180.8 | 10.8 | 58.4 | . ... ... | ... | ... | ... | . 196 1.0 | 38.2 | 3.7 | 13.8 | . 197 1.0 | 94.2 | 4.9 | 8.1 | . 198 1.0 | 177.0 | 9.3 | 6.4 | . 199 1.0 | 283.6 | 42.0 | 66.2 | . 200 1.0 | 232.1 | 8.6 | 8.7 | . 200 rows × 4 columns . model = sm.OLS(y, X).fit() . model.summary() # const indicates B0 value . OLS Regression Results Dep. Variable: sales | R-squared: 0.897 | . Model: OLS | Adj. R-squared: 0.896 | . Method: Least Squares | F-statistic: 570.3 | . Date: Tue, 27 Apr 2021 | Prob (F-statistic): 1.58e-96 | . Time: 19:40:31 | Log-Likelihood: -386.18 | . No. Observations: 200 | AIC: 780.4 | . Df Residuals: 196 | BIC: 793.6 | . Df Model: 3 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . const 2.9389 | 0.312 | 9.422 | 0.000 | 2.324 | 3.554 | . TV 0.0458 | 0.001 | 32.809 | 0.000 | 0.043 | 0.049 | . radio 0.1885 | 0.009 | 21.893 | 0.000 | 0.172 | 0.206 | . newspaper -0.0010 | 0.006 | -0.177 | 0.860 | -0.013 | 0.011 | . Omnibus: 60.414 | Durbin-Watson: 2.084 | . Prob(Omnibus): 0.000 | Jarque-Bera (JB): 151.241 | . Skew: -1.327 | Prob(JB): 1.44e-33 | . Kurtosis: 6.332 | Cond. No. 454. | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. import matplotlib.pyplot as plt import seaborn as sns X.iloc[:,1:].corr() . TV radio newspaper . TV 1.000000 | 0.054809 | 0.056648 | . radio 0.054809 | 1.000000 | 0.354104 | . newspaper 0.056648 | 0.354104 | 1.000000 | . plt.imshow(X,cmap=&#39;autumn&#39;) plt.show() . sns.heatmap(X,linewidth = 0.5 , cmap = &#39;coolwarm&#39;) plt.show() . Using Salary DataSet, link :- . https://github.com/mr-siddy/Machine-Learning/blob/master/Linear%20Regression/Salary_Data.csv . df_salary = pd.read_csv(&#39;Salary_Data.csv&#39;) df_salary.head() . YearsExperience Age Salary . 0 1.1 | 21.0 | 39343 | . 1 1.3 | 21.5 | 46205 | . 2 1.5 | 21.7 | 37731 | . 3 2.0 | 22.0 | 43525 | . 4 2.2 | 22.2 | 39891 | . X = df_salary[[&#39;YearsExperience&#39;,&#39;Age&#39;]] y = df_salary[&#39;Salary&#39;] . fit OLS model on y and X . X = sm.add_constant(X) model = sm.OLS(y,X).fit() . model.summary() # here observe R2, const, stderr and P&gt;|t| --&gt; high correlation . OLS Regression Results Dep. Variable: Salary | R-squared: 0.960 | . Model: OLS | Adj. R-squared: 0.957 | . Method: Least Squares | F-statistic: 323.9 | . Date: Tue, 27 Apr 2021 | Prob (F-statistic): 1.35e-19 | . Time: 19:58:08 | Log-Likelihood: -300.35 | . No. Observations: 30 | AIC: 606.7 | . Df Residuals: 27 | BIC: 610.9 | . Df Model: 2 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . const -6661.9872 | 2.28e+04 | -0.292 | 0.773 | -5.35e+04 | 4.02e+04 | . YearsExperience 6153.3533 | 2337.092 | 2.633 | 0.014 | 1358.037 | 1.09e+04 | . Age 1836.0136 | 1285.034 | 1.429 | 0.165 | -800.659 | 4472.686 | . Omnibus: 2.695 | Durbin-Watson: 1.711 | . Prob(Omnibus): 0.260 | Jarque-Bera (JB): 1.975 | . Skew: 0.456 | Prob(JB): 0.372 | . Kurtosis: 2.135 | Cond. No. 626. | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. X.iloc[:,1:].corr() . YearsExperience Age . YearsExperience 1.000000 | 0.987258 | . Age 0.987258 | 1.000000 | . sns.heatmap(X, cmap=&#39;summer&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x21443500808&gt; . How to Resolve . we check the P value and drop the feature which has higher p value . drop_age = X.drop(&#39;Age&#39;, axis=1) . model = sm.OLS(y,drop_age).fit() model.summary() . OLS Regression Results Dep. Variable: Salary | R-squared: 0.957 | . Model: OLS | Adj. R-squared: 0.955 | . Method: Least Squares | F-statistic: 622.5 | . Date: Tue, 27 Apr 2021 | Prob (F-statistic): 1.14e-20 | . Time: 20:07:01 | Log-Likelihood: -301.44 | . No. Observations: 30 | AIC: 606.9 | . Df Residuals: 28 | BIC: 609.7 | . Df Model: 1 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . const 2.579e+04 | 2273.053 | 11.347 | 0.000 | 2.11e+04 | 3.04e+04 | . YearsExperience 9449.9623 | 378.755 | 24.950 | 0.000 | 8674.119 | 1.02e+04 | . Omnibus: 2.140 | Durbin-Watson: 1.648 | . Prob(Omnibus): 0.343 | Jarque-Bera (JB): 1.569 | . Skew: 0.363 | Prob(JB): 0.456 | . Kurtosis: 2.147 | Cond. No. 13.2 | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/27/Multico-LR.html",
            "relUrl": "/2021/04/27/Multico-LR.html",
            "date": " • Apr 27, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Multiple Linear Regrassion using 50_Startups dataset",
            "content": "dataset link :- https://www.kaggle.com/farhanmd29/50-startups . Import Libraries and Creating Dataframe . import numpy as np import matplotlib.pyplot as plt import pandas as pd . dataset = pd.read_csv(&quot;50_Startups.csv&quot;,&quot;,&quot;) dataset.head() . R&amp;D Spend Administration Marketing Spend State Profit . 0 165349.20 | 136897.80 | 471784.10 | New York | 192261.83 | . 1 162597.70 | 151377.59 | 443898.53 | California | 191792.06 | . 2 153441.51 | 101145.55 | 407934.54 | Florida | 191050.39 | . 3 144372.41 | 118671.85 | 383199.62 | New York | 182901.99 | . 4 142107.34 | 91391.77 | 366168.42 | Florida | 166187.94 | . X = dataset.iloc[:, :-1] y = dataset.iloc[:,4] . print(X.head(),&quot; n&quot;) print(y.head()) . R&amp;D Spend Administration Marketing Spend State 0 165349.20 136897.80 471784.10 New York 1 162597.70 151377.59 443898.53 California 2 153441.51 101145.55 407934.54 Florida 3 144372.41 118671.85 383199.62 New York 4 142107.34 91391.77 366168.42 Florida 0 192261.83 1 191792.06 2 191050.39 3 182901.99 4 166187.94 Name: Profit, dtype: float64 . Data Preprocessing . states=pd.get_dummies(X[&#39;State&#39;],drop_first=True) #get_dummies helps to create dummy variables wrt no of categorial fratures # drop_first = True helps us to create dummy variable trap . X = X.drop(&#39;State&#39;,axis=1) . X . R&amp;D Spend Administration Marketing Spend . 0 165349.20 | 136897.80 | 471784.10 | . 1 162597.70 | 151377.59 | 443898.53 | . 2 153441.51 | 101145.55 | 407934.54 | . 3 144372.41 | 118671.85 | 383199.62 | . 4 142107.34 | 91391.77 | 366168.42 | . 5 131876.90 | 99814.71 | 362861.36 | . 6 134615.46 | 147198.87 | 127716.82 | . 7 130298.13 | 145530.06 | 323876.68 | . 8 120542.52 | 148718.95 | 311613.29 | . 9 123334.88 | 108679.17 | 304981.62 | . 10 101913.08 | 110594.11 | 229160.95 | . 11 100671.96 | 91790.61 | 249744.55 | . 12 93863.75 | 127320.38 | 249839.44 | . 13 91992.39 | 135495.07 | 252664.93 | . 14 119943.24 | 156547.42 | 256512.92 | . 15 114523.61 | 122616.84 | 261776.23 | . 16 78013.11 | 121597.55 | 264346.06 | . 17 94657.16 | 145077.58 | 282574.31 | . 18 91749.16 | 114175.79 | 294919.57 | . 19 86419.70 | 153514.11 | 0.00 | . 20 76253.86 | 113867.30 | 298664.47 | . 21 78389.47 | 153773.43 | 299737.29 | . 22 73994.56 | 122782.75 | 303319.26 | . 23 67532.53 | 105751.03 | 304768.73 | . 24 77044.01 | 99281.34 | 140574.81 | . 25 64664.71 | 139553.16 | 137962.62 | . 26 75328.87 | 144135.98 | 134050.07 | . 27 72107.60 | 127864.55 | 353183.81 | . 28 66051.52 | 182645.56 | 118148.20 | . 29 65605.48 | 153032.06 | 107138.38 | . 30 61994.48 | 115641.28 | 91131.24 | . 31 61136.38 | 152701.92 | 88218.23 | . 32 63408.86 | 129219.61 | 46085.25 | . 33 55493.95 | 103057.49 | 214634.81 | . 34 46426.07 | 157693.92 | 210797.67 | . 35 46014.02 | 85047.44 | 205517.64 | . 36 28663.76 | 127056.21 | 201126.82 | . 37 44069.95 | 51283.14 | 197029.42 | . 38 20229.59 | 65947.93 | 185265.10 | . 39 38558.51 | 82982.09 | 174999.30 | . 40 28754.33 | 118546.05 | 172795.67 | . 41 27892.92 | 84710.77 | 164470.71 | . 42 23640.93 | 96189.63 | 148001.11 | . 43 15505.73 | 127382.30 | 35534.17 | . 44 22177.74 | 154806.14 | 28334.72 | . 45 1000.23 | 124153.04 | 1903.93 | . 46 1315.46 | 115816.21 | 297114.46 | . 47 0.00 | 135426.92 | 0.00 | . 48 542.05 | 51743.15 | 0.00 | . 49 0.00 | 116983.80 | 45173.06 | . print(states.head()) . Florida New York 0 0 1 1 0 0 2 1 0 3 0 1 4 1 0 . X=pd.concat([X,states],axis=1) . print(X.head()) # Now we will apply y=b0+b1x1+b2x2+....... . R&amp;D Spend Administration Marketing Spend Florida New York 0 165349.20 136897.80 471784.10 0 1 1 162597.70 151377.59 443898.53 0 0 2 153441.51 101145.55 407934.54 1 0 3 144372.41 118671.85 383199.62 0 1 4 142107.34 91391.77 366168.42 1 0 . Train_test_split . from sklearn.model_selection import train_test_split . X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=0) . y_test . 28 103282.38 11 144259.40 10 146121.95 41 77798.83 2 191050.39 27 105008.31 38 81229.06 31 97483.56 22 110352.25 4 166187.94 Name: Profit, dtype: float64 . Applying Linear Regression . from sklearn.linear_model import LinearRegression . regressor = LinearRegression() regressor.fit(X_train, y_train) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . y_pred = regressor.predict(X_test) . y_pred . array([103015.20159796, 132582.27760816, 132447.73845174, 71976.09851258, 178537.48221055, 116161.24230165, 67851.69209676, 98791.73374687, 113969.43533012, 167921.0656955 ]) . from sklearn.metrics import r2_score # r2 = 1-(sum_of_residual/sum_of_mean) also model is good if r2 --&gt; 1 score= r2_score(y_test,y_pred) . score . 0.9347068473282423 .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/26/multiple-linear-regression.html",
            "relUrl": "/2021/04/26/multiple-linear-regression.html",
            "date": " • Apr 26, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Simple, Ridge and Lasso Linear Regression",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model import seaborn as sns . from sklearn.datasets import load_boston . df = load_boston() . df . {&#39;data&#39;: array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02, 4.9800e+00], [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02, 9.1400e+00], [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02, 4.0300e+00], ..., [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02, 5.6400e+00], [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02, 6.4800e+00], [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02, 7.8800e+00]]), &#39;target&#39;: array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. , 18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6, 15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2, 13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7, 21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9, 35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5, 19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. , 20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2, 23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8, 33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4, 21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. , 20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6, 23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4, 15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4, 17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7, 25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4, 23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. , 32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3, 34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4, 20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. , 26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3, 31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1, 22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6, 42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. , 36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4, 32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. , 20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1, 20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2, 22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1, 21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6, 19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7, 32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1, 18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8, 16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8, 13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3, 8.8, 7.2, 10.5, 7.4, 10.2, 11.5, 15.1, 23.2, 9.7, 13.8, 12.7, 13.1, 12.5, 8.5, 5. , 6.3, 5.6, 7.2, 12.1, 8.3, 8.5, 5. , 11.9, 27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3, 7. , 7.2, 7.5, 10.4, 8.8, 8.4, 16.7, 14.2, 20.8, 13.4, 11.7, 8.3, 10.2, 10.9, 11. , 9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4, 9.6, 8.7, 8.4, 12.8, 10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4, 15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7, 19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2, 29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8, 20.6, 21.2, 19.1, 20.6, 15.2, 7. , 8.1, 13.6, 20.1, 21.8, 24.5, 23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]), &#39;feature_names&#39;: array([&#39;CRIM&#39;, &#39;ZN&#39;, &#39;INDUS&#39;, &#39;CHAS&#39;, &#39;NOX&#39;, &#39;RM&#39;, &#39;AGE&#39;, &#39;DIS&#39;, &#39;RAD&#39;, &#39;TAX&#39;, &#39;PTRATIO&#39;, &#39;B&#39;, &#39;LSTAT&#39;], dtype=&#39;&lt;U7&#39;), &#39;DESCR&#39;: &#34;.. _boston_dataset: n nBoston house prices dataset n n n**Data Set Characteristics:** n n :Number of Instances: 506 n n :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. n n :Attribute Information (in order): n - CRIM per capita crime rate by town n - ZN proportion of residential land zoned for lots over 25,000 sq.ft. n - INDUS proportion of non-retail business acres per town n - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) n - NOX nitric oxides concentration (parts per 10 million) n - RM average number of rooms per dwelling n - AGE proportion of owner-occupied units built prior to 1940 n - DIS weighted distances to five Boston employment centres n - RAD index of accessibility to radial highways n - TAX full-value property-tax rate per $10,000 n - PTRATIO pupil-teacher ratio by town n - B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town n - LSTAT % lower status of the population n - MEDV Median value of owner-occupied homes in $1000&#39;s n n :Missing Attribute Values: None n n :Creator: Harrison, D. and Rubinfeld, D.L. n nThis is a copy of UCI ML housing dataset. nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/ n n nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. n nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. &#39;Hedonic nprices and the demand for clean air&#39;, J. Environ. Economics &amp; Management, nvol.5, 81-102, 1978. Used in Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics n...&#39;, Wiley, 1980. N.B. Various transformations are used in the table on npages 244-261 of the latter. n nThe Boston house-price data has been used in many machine learning papers that address regression nproblems. n n.. topic:: References n n - Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&#39;, Wiley, 1980. 244-261. n - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. n&#34;, &#39;filename&#39;: &#39;C: Users mrsid anaconda3 lib site-packages sklearn datasets data boston_house_prices.csv&#39;} . dataset= pd.DataFrame(df.data) print(dataset.head()) . 0 1 2 3 4 5 6 7 8 9 10 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 11 12 0 396.90 4.98 1 396.90 9.14 2 392.83 4.03 3 394.63 2.94 4 396.90 5.33 . dataset.head() . 0 1 2 3 4 5 6 7 8 9 10 11 12 . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | . dataset.columns=df.feature_names . dataset.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | . df.target.shape . (506,) . dataset[&quot;price&quot;]=df.target . dataset.head() . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT price . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . X=dataset.iloc[:,:-1] # independent fratures y=dataset.iloc[:,-1] # dependent frature . Linear Regression . from sklearn.model_selection import cross_val_score from sklearn.linear_model import LinearRegression lin_regressor = LinearRegression() mse = cross_val_score(lin_regressor,X,y,scoring=&#39;neg_mean_squared_error&#39;,cv=5) #cv=crossvalidation mean_mse = np.mean(mse) print(mean_mse) # this value should be nearer to zero . -37.13180746769922 . Ridge Regression . from sklearn.linear_model import Ridge from sklearn.model_selection import GridSearchCV ridge=Ridge() parameters = {&#39;alpha&#39;:[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]} ridge_regressor=GridSearchCV(ridge,parameters,scoring = &#39;neg_mean_squared_error&#39;,cv=5) ridge_regressor.fit(X,y) . GridSearchCV(cv=5, error_score=nan, estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver=&#39;auto&#39;, tol=0.001), iid=&#39;deprecated&#39;, n_jobs=None, param_grid={&#39;alpha&#39;: [1e-15, 1e-10, 1e-08, 0.001, 0.01, 1, 5, 10, 20, 30, 35, 40, 45, 50, 55, 100]}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=&#39;neg_mean_squared_error&#39;, verbose=0) . print(ridge_regressor.best_params_) print(ridge_regressor.best_score_) . {&#39;alpha&#39;: 100} -29.905701947540365 . Lasso Regression . from sklearn.linear_model import Lasso from sklearn.model_selection import GridSearchCV lasso=Lasso() parameters = {&#39;alpha&#39;:[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]} Lasso_regressor=GridSearchCV(lasso,parameters,scoring = &#39;neg_mean_squared_error&#39;,cv=5) Lasso_regressor.fit(X,y) print(Lasso_regressor.best_params_) print(Lasso_regressor.best_score_) . {&#39;alpha&#39;: 1} -35.531580220694856 . C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4430.746729651311, tolerance: 3.9191485420792076 positive) C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4397.459304778431, tolerance: 3.3071316790123455 positive) C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3796.653037433508, tolerance: 2.813643886419753 positive) C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2564.292735790545, tolerance: 3.3071762123456794 positive) C: Users mrsid anaconda3 lib site-packages sklearn linear_model _coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4294.252997826028, tolerance: 3.4809104444444445 positive) . from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0) . prediction_lasso=Lasso_regressor.predict(X_test) prediction_ridge=ridge_regressor.predict(X_test) . Plotting of results . import seaborn as sns sns.distplot(y_test-prediction_lasso) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x20e4370d588&gt; . sns.distplot(y_test-prediction_ridge) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x20e437ebf48&gt; . sns.distplot(y_test-mean_mse) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x20e438e2e48&gt; .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/26/Regression-on-Boston-Housing-Dataset.html",
            "relUrl": "/2021/04/26/Regression-on-Boston-Housing-Dataset.html",
            "date": " • Apr 26, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "EDA with Python and Applying Logistic Regression",
            "content": "Import Libraries . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline . The Data . train = pd.read_csv(&#39;titanic_train.csv&#39;) . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . Missing Data . train.isnull() # if True indicates a null value # but it is not a good way as data set can be vast . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 False | False | False | False | False | False | False | False | False | False | True | False | . 1 False | False | False | False | False | False | False | False | False | False | False | False | . 2 False | False | False | False | False | False | False | False | False | False | True | False | . 3 False | False | False | False | False | False | False | False | False | False | False | False | . 4 False | False | False | False | False | False | False | False | False | False | True | False | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 False | False | False | False | False | False | False | False | False | False | True | False | . 887 False | False | False | False | False | False | False | False | False | False | False | False | . 888 False | False | False | False | False | True | False | False | False | False | True | False | . 889 False | False | False | False | False | False | False | False | False | False | False | False | . 890 False | False | False | False | False | False | False | False | False | False | True | False | . 891 rows × 12 columns . sns.heatmap(train.isnull(),yticklabels = False, cbar =False, cmap=&#39;viridis&#39;) # so most of the null values are present in age and cabin . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a91cd84c8&gt; . sns.set_style(&#39;whitegrid&#39;) sns.countplot(x=&#39;Survived&#39;,data=train) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a91d397c8&gt; . sns.set_style(&#39;whitegrid&#39;) sns.countplot(x=&#39;Survived&#39;,hue=&#39;Sex&#39;,data=train, palette=&#39;RdBu_r&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92b67d88&gt; . sns.set_style(&#39;whitegrid&#39;) sns.countplot(x=&#39;Survived&#39;,hue=&#39;Pclass&#39;,data=train, palette=&#39;rainbow&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92c2ee88&gt; . sns.distplot(train[&#39;Age&#39;].dropna(), kde=False, color=&#39;darkred&#39;, bins=40) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92ca32c8&gt; . train[&#39;Age&#39;].hist(bins=30, color=&#39;blue&#39;,alpha=0.3) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92daeb88&gt; . sns.countplot(x=&#39;SibSp&#39;, data=train) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92e6f2c8&gt; . train[&#39;Fare&#39;].hist(bins=30,color=&#39;green&#39;,alpha=0.4) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92ed4e48&gt; . Data Cleaning . plt.figure(figsize=(12,7)) sns.boxplot(x=&#39;Pclass&#39;,y=&#39;Age&#39;,data=train,palette=&#39;winter&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a92c2e5c8&gt; . we can see the wealthrer passengers in the higher classes tend to older, which makes sense, we&#39;ll use these average age values to impute based on pcalss for age . def impute_age(cols): Age = cols[0] Pclass = cols[1] if pd.isnull(Age): if Pclass == 1: return 37 elif Pclass == 2: return 29 else: return 24 else: return Age . train[&#39;Age&#39;] = train [[&#39;Age&#39;,&#39;Pclass&#39;]].apply(impute_age,axis=1 ) # now check heatmap again . sns.heatmap(train.isnull(),yticklabels = False, cbar =False, cmap=&#39;viridis&#39;) # so most of the null values are present in age and cabin . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a931ec508&gt; . we have to apply a lot of feature engineering to handle Cabin coz of a lot of Nan values hence we&#39;ll drop it for now . train.drop(&#39;Cabin&#39;,axis=1,inplace=True) . sns.heatmap(train.isnull(),yticklabels = False, cbar =False, cmap=&#39;viridis&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a93031ac8&gt; . train.dropna(inplace=True) . sns.heatmap(train.isnull(),yticklabels = False, cbar =False, cmap=&#39;viridis&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19a930b6148&gt; . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | S | . Converting Categorical Features . we&#39;ll need to convert categorical features to dummy variables using pandas, otherwise our machine learning algorithm wont be able to directly take in those features as inputs . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 889 entries, 0 to 890 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 889 non-null int64 1 Survived 889 non-null int64 2 Pclass 889 non-null int64 3 Name 889 non-null object 4 Sex 889 non-null object 5 Age 889 non-null float64 6 SibSp 889 non-null int64 7 Parch 889 non-null int64 8 Ticket 889 non-null object 9 Fare 889 non-null float64 10 Embarked 889 non-null object dtypes: float64(2), int64(5), object(4) memory usage: 83.3+ KB . pd.get_dummies(train[&#39;Embarked&#39;],drop_first=True).head() . Q S . 0 0 | 1 | . 1 0 | 0 | . 2 0 | 1 | . 3 0 | 1 | . 4 0 | 1 | . sex = pd.get_dummies(train[&#39;Sex&#39;],drop_first=True) embark = pd.get_dummies(train[&#39;Embarked&#39;],drop_first=True) . train.drop([&#39;Sex&#39;,&#39;Embarked&#39;,&#39;Name&#39;,&#39;Ticket&#39;],axis=1,inplace=True) . train.head() . PassengerId Survived Pclass Age SibSp Parch Fare . 0 1 | 0 | 3 | 22.0 | 1 | 0 | 7.2500 | . 1 2 | 1 | 1 | 38.0 | 1 | 0 | 71.2833 | . 2 3 | 1 | 3 | 26.0 | 0 | 0 | 7.9250 | . 3 4 | 1 | 1 | 35.0 | 1 | 0 | 53.1000 | . 4 5 | 0 | 3 | 35.0 | 0 | 0 | 8.0500 | . train = pd.concat([train,sex,embark],axis=1) . train.head() . PassengerId Survived Pclass Age SibSp Parch Fare male Q S . 0 1 | 0 | 3 | 22.0 | 1 | 0 | 7.2500 | 1 | 0 | 1 | . 1 2 | 1 | 1 | 38.0 | 1 | 0 | 71.2833 | 0 | 0 | 0 | . 2 3 | 1 | 3 | 26.0 | 0 | 0 | 7.9250 | 0 | 0 | 1 | . 3 4 | 1 | 1 | 35.0 | 1 | 0 | 53.1000 | 0 | 0 | 1 | . 4 5 | 0 | 3 | 35.0 | 0 | 0 | 8.0500 | 1 | 0 | 1 | . Building a Logistic Regression Model . Train Test Split . train.drop(&#39;Survived&#39;,axis=1).head() . PassengerId Pclass Age SibSp Parch Fare male Q S . 0 1 | 3 | 22.0 | 1 | 0 | 7.2500 | 1 | 0 | 1 | . 1 2 | 1 | 38.0 | 1 | 0 | 71.2833 | 0 | 0 | 0 | . 2 3 | 3 | 26.0 | 0 | 0 | 7.9250 | 0 | 0 | 1 | . 3 4 | 1 | 35.0 | 1 | 0 | 53.1000 | 0 | 0 | 1 | . 4 5 | 3 | 35.0 | 0 | 0 | 8.0500 | 1 | 0 | 1 | . train[&#39;Survived&#39;].head() . 0 0 1 1 2 1 3 1 4 0 Name: Survived, dtype: int64 . from sklearn.model_selection import train_test_split . X_train,X_test,y_train,y_test = train_test_split(train.drop(&#39;Survived&#39;,axis=1),train[&#39;Survived&#39;],test_size=0.30,random_state=101) . Training and Predicting . from sklearn.linear_model import LogisticRegression . logmodel = LogisticRegression() logmodel.fit(X_train,y_train) . C: Users mrsid anaconda3 lib site-packages sklearn linear_model _logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . predictions = logmodel.predict(X_test) . from sklearn.metrics import confusion_matrix . accuracy = confusion_matrix(y_test,predictions) . accuracy . array([[149, 14], [ 39, 65]], dtype=int64) . from sklearn.metrics import accuracy_score . accuracy = accuracy_score(y_test,predictions) accuracy . 0.8014981273408239 . predictions . array([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1], dtype=int64) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/25/EDA.html",
            "relUrl": "/2021/04/25/EDA.html",
            "date": " • Apr 25, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Exploratory Data Analysis on Iris Dataset",
            "content": "dataset link :- https://archive.ics.uci.edu/ml/datasets/Iris . Import all libraries . import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt . Creating DataFrame . df = pd.read_csv(&quot;https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv&quot;) . df.head() . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . df.shape . (150, 5) . Univariate Analysis . df.loc[df[&#39;species&#39;]==&#39;setosa&#39;] . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . 5 5.4 | 3.9 | 1.7 | 0.4 | setosa | . 6 4.6 | 3.4 | 1.4 | 0.3 | setosa | . 7 5.0 | 3.4 | 1.5 | 0.2 | setosa | . 8 4.4 | 2.9 | 1.4 | 0.2 | setosa | . 9 4.9 | 3.1 | 1.5 | 0.1 | setosa | . 10 5.4 | 3.7 | 1.5 | 0.2 | setosa | . 11 4.8 | 3.4 | 1.6 | 0.2 | setosa | . 12 4.8 | 3.0 | 1.4 | 0.1 | setosa | . 13 4.3 | 3.0 | 1.1 | 0.1 | setosa | . 14 5.8 | 4.0 | 1.2 | 0.2 | setosa | . 15 5.7 | 4.4 | 1.5 | 0.4 | setosa | . 16 5.4 | 3.9 | 1.3 | 0.4 | setosa | . 17 5.1 | 3.5 | 1.4 | 0.3 | setosa | . 18 5.7 | 3.8 | 1.7 | 0.3 | setosa | . 19 5.1 | 3.8 | 1.5 | 0.3 | setosa | . 20 5.4 | 3.4 | 1.7 | 0.2 | setosa | . 21 5.1 | 3.7 | 1.5 | 0.4 | setosa | . 22 4.6 | 3.6 | 1.0 | 0.2 | setosa | . 23 5.1 | 3.3 | 1.7 | 0.5 | setosa | . 24 4.8 | 3.4 | 1.9 | 0.2 | setosa | . 25 5.0 | 3.0 | 1.6 | 0.2 | setosa | . 26 5.0 | 3.4 | 1.6 | 0.4 | setosa | . 27 5.2 | 3.5 | 1.5 | 0.2 | setosa | . 28 5.2 | 3.4 | 1.4 | 0.2 | setosa | . 29 4.7 | 3.2 | 1.6 | 0.2 | setosa | . 30 4.8 | 3.1 | 1.6 | 0.2 | setosa | . 31 5.4 | 3.4 | 1.5 | 0.4 | setosa | . 32 5.2 | 4.1 | 1.5 | 0.1 | setosa | . 33 5.5 | 4.2 | 1.4 | 0.2 | setosa | . 34 4.9 | 3.1 | 1.5 | 0.1 | setosa | . 35 5.0 | 3.2 | 1.2 | 0.2 | setosa | . 36 5.5 | 3.5 | 1.3 | 0.2 | setosa | . 37 4.9 | 3.1 | 1.5 | 0.1 | setosa | . 38 4.4 | 3.0 | 1.3 | 0.2 | setosa | . 39 5.1 | 3.4 | 1.5 | 0.2 | setosa | . 40 5.0 | 3.5 | 1.3 | 0.3 | setosa | . 41 4.5 | 2.3 | 1.3 | 0.3 | setosa | . 42 4.4 | 3.2 | 1.3 | 0.2 | setosa | . 43 5.0 | 3.5 | 1.6 | 0.6 | setosa | . 44 5.1 | 3.8 | 1.9 | 0.4 | setosa | . 45 4.8 | 3.0 | 1.4 | 0.3 | setosa | . 46 5.1 | 3.8 | 1.6 | 0.2 | setosa | . 47 4.6 | 3.2 | 1.4 | 0.2 | setosa | . 48 5.3 | 3.7 | 1.5 | 0.2 | setosa | . 49 5.0 | 3.3 | 1.4 | 0.2 | setosa | . df_setosa=df.loc[df[&#39;species&#39;]==&#39;setosa&#39;] df_virginica=df.loc[df[&#39;species&#39;]==&#39;virginica&#39;] df_versicolor=df.loc[df[&#39;species&#39;]==&#39;versicolor&#39;] . plt.plot(df_setosa[&#39;sepal_length&#39;],np.zeros_like(df_setosa[&#39;sepal_length&#39;]),&#39;o&#39;) plt.plot(df_virginica[&#39;sepal_length&#39;],np.zeros_like(df_virginica[&#39;sepal_length&#39;]),&#39;o&#39;) plt.plot(df_versicolor[&#39;sepal_length&#39;],np.zeros_like(df_versicolor[&#39;sepal_length&#39;]),&#39;o&#39;) plt.xlabel(&quot;sepal_length&quot;) plt.show() . Bivariate Analysis . sns.FacetGrid(df,hue=&#39;species&#39;,size=5).map(plt.scatter,&quot;petal_length&quot;,&quot;sepal_width&quot;).add_legend(); plt.show() . Multivariate Analysis . sns.pairplot(df,hue=&#39;species&#39;,size=3) . C: Users mrsid anaconda3 lib site-packages seaborn axisgrid.py:2079: UserWarning: The `size` parameter has been renamed to `height`; please update your code. warnings.warn(msg, UserWarning) . &lt;seaborn.axisgrid.PairGrid at 0x13024488808&gt; .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/24/EDA-IRIS.html",
            "relUrl": "/2021/04/24/EDA-IRIS.html",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Seaborn || sns",
            "content": "Distribution plots . distplot, joinplot, pairplot . import seaborn as sns import numpy as np . df = sns.load_dataset(&quot;tips&quot;) . df.head() # tip is dependent feature # others are independent features . total_bill tip sex smoker day time size . 0 16.99 | 1.01 | Female | No | Sun | Dinner | 2 | . 1 10.34 | 1.66 | Male | No | Sun | Dinner | 3 | . 2 21.01 | 3.50 | Male | No | Sun | Dinner | 3 | . 3 23.68 | 3.31 | Male | No | Sun | Dinner | 2 | . 4 24.59 | 3.61 | Female | No | Sun | Dinner | 4 | . df.dtypes . total_bill float64 tip float64 sex category smoker category day category time category size int64 dtype: object . Correlation with Heatmap . df.corr() # correlarion can only be found out if values are floating point or integers # corr values range b/w -1 to +1 . total_bill tip size . total_bill 1.000000 | 0.675734 | 0.598315 | . tip 0.675734 | 1.000000 | 0.489299 | . size 0.598315 | 0.489299 | 1.000000 | . Observations :- 1) +ve corr -&gt;&gt; Total bill inc then tip will also inc . sns.heatmap(df.corr()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d218d27108&gt; . JointPlot . Univariate analysis . sns.jointplot(x=&#39;tip&#39;,y=&#39;total_bill&#39;,data=df,kind=&#39;hex&#39;) # hex=hexagonal shape . &lt;seaborn.axisgrid.JointGrid at 0x2d219541988&gt; . sns.jointplot(x=&#39;tip&#39;,y=&#39;total_bill&#39;,data=df,kind=&#39;reg&#39;) # reg gives probablity density line(on graph) and regression line (inside plot) . &lt;seaborn.axisgrid.JointGrid at 0x2d2197b6d88&gt; . Pair plot . same data row is matched with another variable&#39;s value . sns.pairplot(df, hue=&#39;sex&#39;) . &lt;seaborn.axisgrid.PairGrid at 0x2d219f05bc8&gt; . Dist Plot . sns.distplot(df[&#39;tip&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21a51bcc8&gt; . sns.distplot(df[&#39;tip&#39;],kde =False,bins=10) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21a657dc8&gt; . Categorical Plots . Count Plot . sns.countplot(&#39;sex&#39;,data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21a6d2688&gt; . Bar Plot . sns.barplot(x=&#39;total_bill&#39;,y=&#39;sex&#39;,data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21b8f2788&gt; . Box Plot . sns.boxplot(&#39;sex&#39;, &#39;total_bill&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21b950348&gt; . sns.boxplot(x=&#39;day&#39;, y=&#39;total_bill&#39;, data=df, palette=&#39;rainbow&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21b9b9a48&gt; . sns.boxplot(data=df, orient=&#39;v&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21baf1708&gt; . sns.boxplot(x=&#39;total_bill&#39;, y=&#39;day&#39;, hue=&#39;smoker&#39;, data=df) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21bb73288&gt; . Violin Plot . sns.violinplot(x=&quot;total_bill&quot;,y=&#39;day&#39;,data=df, palette=&#39;rainbow&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x2d21bc73488&gt; .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/23/Seaborn.html",
            "relUrl": "/2021/04/23/Seaborn.html",
            "date": " • Apr 23, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Matplotlib || plt",
            "content": "import matplotlib.pyplot as plt %matplotlib inline . import numpy as np . x = np.arange(0,10) y=np.arange(11,21) . a=np.arange(40,50) b=np.arange(50,60) . Scatter plot . plt.scatter(x,y,c=&#39;g&#39;) # c= color plt.xlabel(&#39;X axis&#39;) plt.ylabel(&#39;Y axis&#39;) plt.title(&#39;Graph in 2D&#39;) plt.savefig(&#39;g1.png&#39;) plt.show() . plt plot . plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x20b5b3bef48&gt;] . y=x*x plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x20b5b4228c8&gt;] . plt.plot(x,y,&#39;r&#39;) . [&lt;matplotlib.lines.Line2D at 0x20b5b447d08&gt;] . plt.plot(x,y,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x20b5b4ef748&gt;] . plt.plot(x,y,&#39;r*-&#39;) . [&lt;matplotlib.lines.Line2D at 0x20b5b562fc8&gt;] . Subplots . plt.subplot(2,2,1) # 2 rows 2 cols 1 position plt.plot(x,y,&#39;r&#39;) plt.subplot(2,2,2) plt.plot(x,y,&#39;g&#39;) plt.subplot(2,2,3) plt.plot(x,y,&#39;b&#39;) . [&lt;matplotlib.lines.Line2D at 0x20b5b61f088&gt;] . . np.pi . 3.141592653589793 . x = np.arange(0,4*np.pi,0.1) y=np.sin(x) plt.title(&quot;sine wave form&quot;) plt.plot(x,y) plt.show() . x=np.arange(0,5*np.pi,0.1) y_sin = np.sin(x) y_cos = np.cos(x) plt.subplot(2,1,1) plt.plot(x,y_sin,&#39;r--&#39;) plt.title(&quot;sine graph&quot;) plt.subplot(2,1,2) plt.plot(x,y_cos,&#39;g--&#39;) plt.title(&quot;cosine graph&quot;) plt.show() . Bar plot . x= [2,8,10] y = [11,16,18] x2 = [3,9,11] y2 = [4,7,9] plt.bar(x,y) plt.bar(x2,y2,color =&#39;g&#39;) plt.title(&#39;Bar graph&#39;) plt.ylabel( &#39;Yaxis&#39;) plt.xlabel( &#39;Xaxis&#39;) plt.show() . Histograms . a = np.array([1,2,3,4,5,5,6,67,7,8,8,9]) # y axis == bins - desity or count plt.hist(a) plt.title(&#39;histogram&#39;) plt.show() . Box plot . data = [np.random.normal(0,std,100) for std in range(1,4)] # selecting a normal distribution b/w low=0, to std, step=100 # rectangular box plot plt.boxplot(data, vert=True, patch_artist= True) . {&#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x20b5bb46688&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb46f08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb55d08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb55e88&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb6b188&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb6ba08&gt;], &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x20b5bb4b8c8&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb4bf48&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb5bdc8&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb5bec8&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb6bb88&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb71a08&gt;], &#39;boxes&#39;: [&lt;matplotlib.patches.PathPatch at 0x20b5bb46048&gt;, &lt;matplotlib.patches.PathPatch at 0x20b5bb50fc8&gt;, &lt;matplotlib.patches.PathPatch at 0x20b5bb65ac8&gt;], &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x20b5bb4bfc8&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb5fd08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb71b88&gt;], &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x20b5bb50f08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb5fe08&gt;, &lt;matplotlib.lines.Line2D at 0x20b5bb76a08&gt;], &#39;means&#39;: []} . data . [array([-0.7784494 , -0.30130908, 0.54002525, -0.51800759, 0.01819769, -0.83990426, -0.28781469, 0.04318482, 1.23528389, 2.1785494 , -2.0737086 , 1.0928547 , -0.0187436 , 1.26047616, -0.22879622, 0.7987299 , -1.32200805, 1.5095032 , -0.90634209, -0.88452427, 0.21450132, -0.33105648, -0.89893418, 0.2640048 , 0.18846496, -0.13365763, -0.56769452, 1.70685974, 2.50448167, -0.71739823, -2.15135456, -0.79866835, 0.01126657, 0.03509671, 0.70977944, -0.48825295, -0.51388798, 0.03850738, -0.11959896, -1.44425172, -0.48869629, 1.99891486, -0.79457436, 0.82734671, -0.21331385, -1.01447424, -1.62881497, 1.55287689, -0.76185124, -1.33031956, -0.24552639, 0.07408732, -2.05106282, 1.08293709, -0.39720809, -0.37170031, -0.78308727, 0.94345425, -1.61168896, 0.75191668, -0.19178661, 0.35292808, -0.32761845, -0.12057788, -0.90665516, 0.61673275, 0.3552815 , -0.75085115, 0.95438335, -0.4752099 , -1.22754795, 0.90739187, 0.98549253, 1.17860435, -0.47033725, -1.11863367, -2.1007785 , -1.28848407, -0.97587155, -1.50746364, 0.15689869, -1.29434923, 0.95408283, 0.38562582, 1.09328084, -0.83567472, 1.46300781, 0.21707649, 1.04889211, 0.13129867, 0.78442675, 0.21995366, 1.63712729, 1.50326651, 0.28453443, -0.2031552 , -0.28490282, 1.33678566, 2.37008989, 0.79503051]), array([ 1.23556973, -0.02072204, -1.12229404, -2.96722053, -1.30085601, -2.60421508, 1.24700109, -0.31148209, -2.52475577, -3.79873713, -0.5184776 , -1.40388223, -0.76082764, 1.21536502, -0.98142646, 0.43235375, 2.01282379, -0.21453285, 3.61200475, 1.8287454 , -2.37699005, -4.43876649, -1.5534308 , 0.19087839, 0.63776082, -3.89796591, -0.77253082, 0.15942456, 1.50682854, -2.13153439, -0.03070496, -0.87138476, -3.60486968, -3.73673651, 1.36459964, -0.57526159, 1.74855 , -1.59916748, -2.53317411, 0.34688596, -0.39179164, 3.50326963, -2.16398775, 1.6853139 , 0.93583756, -3.19704488, 2.29302575, 0.1907704 , 1.65541487, -1.30203682, 2.56856035, 0.0327959 , 4.19304044, -1.00926479, -2.24279789, -0.69572595, -1.76483291, 3.0767504 , -2.20523853, 3.85941305, 0.02224512, 0.51100795, -0.64877433, -0.97541769, -0.55332363, 0.68110681, 1.04656981, -1.66401884, -2.22326276, 2.5260883 , 1.23117647, -0.60578903, 0.08622414, 1.41381078, -2.7653705 , -0.97335699, 2.92662744, -0.83610816, 2.29915347, 0.01851729, -1.31768037, -1.48470864, 1.02320517, 0.44434635, -3.43562133, -0.4494547 , 0.08147359, 3.30459418, 1.80139721, -1.308831 , -0.99884576, -1.46526386, -0.54199541, 1.12811024, 2.97529432, 1.64583481, -0.78990555, -0.74874302, -4.4103771 , -2.48981923]), array([ 3.95391703e+00, -1.07121577e+00, -3.84668853e+00, 6.77840007e+00, -2.19381045e+00, 7.10352670e-01, 6.73618307e-01, 1.37069922e+00, -3.81843396e+00, 1.26967121e+00, -2.22084017e+00, -3.53653835e+00, 9.12261523e-01, 3.46900445e+00, 5.60861189e-01, 1.81888792e+00, 7.13406114e-01, -3.34833646e+00, 1.39887349e+00, -1.53083906e+00, 3.99241572e+00, -1.95620365e+00, -1.32736259e+00, 1.45314767e+00, 1.86896524e+00, 1.41268309e+00, -2.04054499e+00, -3.22104097e+00, -3.38356292e+00, -1.07288730e+00, -2.13342416e+00, -1.17784314e+00, -5.50678185e-01, -2.93018741e+00, 6.09593785e+00, 3.56688350e-01, -2.74400006e+00, 1.41395686e+00, -1.06679209e+00, 3.99608167e+00, -1.63810367e+00, 3.26794993e+00, -2.17703756e+00, 5.76026096e+00, -3.16019468e+00, -2.04739358e+00, -9.21248072e-01, -1.17306562e+00, -1.40941302e+00, -3.39076210e+00, 8.42848917e+00, -2.23424984e+00, 1.51486619e+00, 3.39342705e+00, -3.71272706e+00, 9.32418444e+00, -2.89173783e+00, -7.17807468e-01, 6.45628003e+00, 2.46759215e+00, -5.40677123e-01, 1.03397626e+00, -4.61687260e-01, 2.28964222e+00, -1.45379187e+00, 1.09286059e+00, 1.66547924e+00, 2.60394771e+00, 3.59662329e-02, -1.58705864e+00, -2.26368232e+00, 2.50848563e+00, -1.72671381e+00, -3.19559078e+00, -9.92987939e-01, 8.91871959e-01, 1.03963870e+00, -4.01271402e-01, 3.12010149e+00, -1.35404888e+00, 2.93841033e+00, -9.41879808e-02, 5.56786269e-01, -9.35989605e-01, 1.10483247e+00, -1.21961918e+00, -4.03470597e-01, -1.41275722e-01, 2.15839643e-01, -2.90275833e+00, 6.03367683e+00, 4.09121350e+00, 3.09437534e+00, -2.16658125e-03, 2.75046954e+00, 8.71768377e-01, -2.33004375e+00, -8.64465990e-03, 2.06668848e+00, 5.57575505e-01])] . Pi chart . labels = &#39;python&#39;,&#39;c++&#39;, &#39;ruby&#39;, &#39;java&#39; sizes = [215,130,245,210] colors = [&#39;gold&#39;, &#39;yellowgreen&#39;,&#39;lightcoral&#39;, &#39;lightskyblue&#39;] explode = (0.1,0,0,0) #explode 1st slice #plot plt.pie(sizes,explode=explode,labels=labels,colors=colors, autopct=&#39;%1.1f%%&#39;,shadow=True) plt.axis(&#39;equal&#39;) plt.show() .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/23/Matplotlib.html",
            "relUrl": "/2021/04/23/Matplotlib.html",
            "date": " • Apr 23, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Pyforest - Import all Python Data Science Libraries",
            "content": "pip install Pyforest . Collecting Pyforest Downloading pyforest-1.0.3.tar.gz (14 kB) Building wheels for collected packages: Pyforest Building wheel for Pyforest (setup.py): started Building wheel for Pyforest (setup.py): finished with status &#39;done&#39; Created wheel for Pyforest: filename=pyforest-1.0.3-py2.py3-none-any.whl size=13720 sha256=18e08121d5ee96f79c928bf37f744f0c937b1253580c5189dc7f68057ca46f2c Stored in directory: c: users mrsid appdata local pip cache wheels 72 b6 6c b593d021f7e83f481c5208bc23df0084bcfbeb5b141352b882 Successfully built Pyforest Installing collected packages: Pyforest Successfully installed Pyforest-1.0.3 Note: you may need to restart the kernel to use updated packages. . df = pd.read_csv(&#39;http://winterolympicsmedals.com/medals.csv&#39;) . df.head() . Year City Sport Discipline NOC Event Event gender Medal . 0 1924 | Chamonix | Skating | Figure skating | AUT | individual | M | Silver | . 1 1924 | Chamonix | Skating | Figure skating | AUT | individual | W | Gold | . 2 1924 | Chamonix | Skating | Figure skating | AUT | pairs | X | Gold | . 3 1924 | Chamonix | Bobsleigh | Bobsleigh | BEL | four-man | M | Bronze | . 4 1924 | Chamonix | Ice Hockey | Ice Hockey | CAN | ice hockey | M | Gold | . active_imports() # It imports only those libraries that are in use . import pandas as pd . [&#39;import pandas as pd&#39;] . lst1 = [1,2,3,4,5] lst2 = [6,7,8,9,10] plt.plot(lst1,lst2) plt.xlabel(&quot;X-axis&quot;) plt.ylabel(&quot;Y-axis&quot;) plt.show() . np.array([1,2,3,4,5]) . array([1, 2, 3, 4, 5]) . active_imports() . import matplotlib.pyplot as plt import pandas as pd import numpy as np . [&#39;import matplotlib.pyplot as plt&#39;, &#39;import pandas as pd&#39;, &#39;import numpy as np&#39;] . df1= pd.read_csv(&quot;C: Users mrsid Desktop 30 days of ML challenge NumPy and Pandas mercedesbenz.csv&quot;) . df1.head() . ID y X0 X1 X2 X3 X4 X5 X6 X8 ... X375 X376 X377 X378 X379 X380 X382 X383 X384 X385 . 0 0 | 130.81 | k | v | at | a | d | u | j | o | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 6 | 88.53 | k | t | av | e | d | y | l | o | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 7 | 76.26 | az | w | n | c | d | x | j | x | ... | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 3 9 | 80.62 | az | t | n | f | d | x | l | e | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 13 | 78.02 | az | v | n | f | d | h | d | n | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 rows × 378 columns . sns.distplot(df1[&#39;y&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x27bb681d3c8&gt; . active_imports() . import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np . [&#39;import matplotlib.pyplot as plt&#39;, &#39;import seaborn as sns&#39;, &#39;import pandas as pd&#39;, &#39;import numpy as np&#39;] .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/22/Pyforest.html",
            "relUrl": "/2021/04/22/Pyforest.html",
            "date": " • Apr 22, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "NumPy || np",
            "content": "import numpy as np . lst1 = [1,2,3,4] arr = np.array(lst1) . type(arr) . numpy.ndarray . arr . array([1, 2, 3, 4]) . arr.shape . (4,) . list1=[1,2,3,4] list2=[6,7,8,9] list3=[3,4,5,6] arr = np.array([list1,list2,list3]) . arr . array([[1, 2, 3, 4], [6, 7, 8, 9], [3, 4, 5, 6]]) . arr.shape . (3, 4) . arr.reshape(4,3) . array([[1, 2, 3], [4, 6, 7], [8, 9, 3], [4, 5, 6]]) . arr.reshape(1,12) . array([[1, 2, 3, 4, 6, 7, 8, 9, 3, 4, 5, 6]]) . arr.shape . (3, 4) . Indexing . arr . array([[1, 2, 3, 4], [6, 7, 8, 9], [3, 4, 5, 6]]) . arr[0][1] . 2 . arr[1:,3:] . array([[9], [6]]) . arr[1:,2:] . array([[8, 9], [5, 6]]) . arr[:,2:] . array([[3, 4], [8, 9], [5, 6]]) . arr[0:2,0:2] # always remember left:right is left exact and right is one value greater than actual one . array([[1, 2], [6, 7]]) . Inbuilt functions . arr = np.arange(0,10) . arr . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . arr = np.arange(0,10,step=2) arr . array([0, 2, 4, 6, 8]) . # shift+tab for elaborate the function np.linspace(1,10,50) . array([ 1. , 1.18367347, 1.36734694, 1.55102041, 1.73469388, 1.91836735, 2.10204082, 2.28571429, 2.46938776, 2.65306122, 2.83673469, 3.02040816, 3.20408163, 3.3877551 , 3.57142857, 3.75510204, 3.93877551, 4.12244898, 4.30612245, 4.48979592, 4.67346939, 4.85714286, 5.04081633, 5.2244898 , 5.40816327, 5.59183673, 5.7755102 , 5.95918367, 6.14285714, 6.32653061, 6.51020408, 6.69387755, 6.87755102, 7.06122449, 7.24489796, 7.42857143, 7.6122449 , 7.79591837, 7.97959184, 8.16326531, 8.34693878, 8.53061224, 8.71428571, 8.89795918, 9.08163265, 9.26530612, 9.44897959, 9.63265306, 9.81632653, 10. ]) . print(arr) arr[3:] =100 # replace all indexes starting from 3rd to all by 100 print(arr) . [0 2 4 6 8] [ 0 2 4 100 100] . arr1=arr . arr1[3:]=500 arr1 . array([ 0, 2, 4, 500, 500]) . arr # array is actually a reference type hence change is reflected to actual array also . array([ 0, 2, 4, 500, 500]) . arr1 = arr.copy() arr1 . array([ 0, 2, 4, 500, 500]) . arr1[3:] = 800 print(arr1) print(arr) . [ 0 2 4 800 800] [ 0 2 4 500 500] . Some useful conditions for Exploratorty data analysis . arr = np.array([1,2,3,4,5]) val = 2 . arr . array([1, 2, 3, 4, 5]) . print(arr&lt;2) print(arr*2) print(arr%2) . [ True False False False False] [ 2 4 6 8 10] [1 0 1 0 1] . arr[arr&lt;2] . array([1]) . np.ones((2,5),dtype=int) . array([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]) . np.ones(4) . array([1., 1., 1., 1.]) . np.random.rand(3,3) . array([[0.97016302, 0.13230666, 0.31222633], [0.85189366, 0.07856671, 0.57296934], [0.71915461, 0.48997742, 0.24332137]]) . arr_ex = np.random.randn(4,4) # selects from random distribution arr_ex . array([[-0.60890655, -0.67170484, -0.28552398, 1.14748824], [-1.27784825, -0.60587355, -0.87103948, -0.75084882], [ 0.1356478 , 0.67908955, -0.18930585, -1.23064491], [ 0.0557476 , 0.96733176, -0.0119645 , 0.94036578]]) . arr_ex.reshape(16,1) . array([[-0.60890655], [-0.67170484], [-0.28552398], [ 1.14748824], [-1.27784825], [-0.60587355], [-0.87103948], [-0.75084882], [ 0.1356478 ], [ 0.67908955], [-0.18930585], [-1.23064491], [ 0.0557476 ], [ 0.96733176], [-0.0119645 ], [ 0.94036578]]) . import seaborn as sns import pandas as pd . sns.distplot(pd.DataFrame(arr_ex.reshape(16,1))) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1b903b859c8&gt; . np.random.randint(0,100,8).reshape(4,2) . array([[17, 74], [67, 13], [91, 60], [92, 75]]) . np.random.random_sample((1,5)) . array([[0.02247093, 0.32708592, 0.95730227, 0.40039247, 0.43461314]]) .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/22/NumPy.html",
            "relUrl": "/2021/04/22/NumPy.html",
            "date": " • Apr 22, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Python-Intermediate",
            "content": "def even_odd(num): if num%2==0: print(&quot;number is even&quot;) else: print(&quot;number is odd&quot;) . even_odd(24) . number is even . def hello(name, age=29): # name is positional argument and age is keyword argument print(&quot;my name is {} and age is {}&quot;.format(name, age)) . hello(&#39;sid&#39;) . my name is sid and age is 29 . hello(&#39;sid&#39;,20) . my name is sid and age is 20 . def hello(*args, **kwargs): print(args) print(kwargs) . hello(&#39;sid&#39;,&#39;saxena&#39;,age=29,dob=2000) . (&#39;sid&#39;, &#39;saxena&#39;) {&#39;age&#39;: 29, &#39;dob&#39;: 2000} . lst=[&#39;sid&#39;,&#39;saxena&#39;] dict_args={&#39;age&#39;: 20 ,&#39;dob&#39;:2000} . hello(*lst,**dict_args) . (&#39;sid&#39;, &#39;saxena&#39;) {&#39;age&#39;: 20, &#39;dob&#39;: 2000} . lst = [1,2,3,4,5,6,7] def evenoddsum(lst): even_sum=0 odd_sum=0 for i in lst: if i%2==0: even_sum += i else: odd_sum += i return even_sum,odd_sum . evenoddsum(lst) . (12, 16) . Lambda functions . def addition(a,b): return a+b # Single expression can only be converted . addition(4,5) . 9 . addition = lambda a,b:a+b . addition(5,6) . 11 . def even(num): if num%2==0: return True . even(24) . True . even1 = lambda num:num%2==0 . even1(12) . True . def add(x,y,z): return x+y+z . add(1,2,3) . 6 . three_add = lambda x,y,z:x+y+z . three_add(1,2,3) . 6 . Map Function . def even_odd(num): if num%2==0: return True else: return False . even_odd(23) . False . lst=[1,2,3,4,5,6,7,8] # apply same function on multiple values . map(even_odd,lst) # in order to instantiate convert it into a list ## memory is not intialised yet . &lt;map at 0x2a398c7e248&gt; . list(map(even_odd,lst)) . [False, True, False, True, False, True, False, True] . list(map(lambda num:num%2==0,lst)) . [False, True, False, True, False, True, False] . Filter function . def even(num): if num%2==0: return True . lst=[1,2,3,4,5,6,7] . list(filter(even,lst)) # return elements which satisfy the condition . [2, 4, 6] . list(filter(lambda num:num%2==0,lst)) . [2, 4, 6] . List Comprehension . provide a concise way to create lists, It consists of braces containing an expression followed by for clause, then zero or more for or if clauses . lst1=[] def lst_square(lst): for i in lst: lst1.append(i*i) return lst1 . lst_square([1,2,3,4,5,6,7]) . [1, 4, 9, 16, 25, 36, 49] . lst=[1,2,3,4,5,6,7] #list comprehension [i*i for i in lst ] . [1, 4, 9, 16, 25, 36, 49] . list1=[i*i for i in lst ] print(list1) . [1, 4, 9, 16, 25, 36, 49] . [i*i for i in lst if i%2==0] . [4, 16, 36] . String Formatting . print(&quot;hello&quot;) . hello . str=&quot;hello&quot; print(str) . hello . def greetings(name): return &quot;hello {}&quot;.format(name) . greetings(&#39;sid&#39;) . &#39;hello sid&#39; . def welcome_email(firstname,lastname): return &quot;welcome {}. is your last name is {}&quot;.format(firstname,lastname) # order can not be altered . welcome_email(&#39;sid&#39;,&#39;saxena&#39;) . &#39;welcome sid. is your last name is saxena&#39; . def welcome_email(name,age): return &quot;welcome {name}. is your age is {age}&quot;.format(age=age,name=name) # now ordering can alter . welcome_email(&#39;sid&#39;,20) . &#39;welcome sid. is your age is 20&#39; . def welcome_email(name,age): return &quot;welcome {name1}. is your age is {age1}&quot;.format(age1=age,name1=name) . welcome_email(&#39;sid&#39;,20) . &#39;welcome sid. is your age is 20&#39; . List Iterables vs Iterators . lst = [1,2,3,4,5,6,7] # this whole list is getting initialised in the memory for i in lst: print(i) . 1 2 3 4 5 6 7 . list1=iter(lst) #but in case of iterators whole listdose not get stored in memory it will get accessed only via next . list1 . &lt;list_iterator at 0x2a3996beb08&gt; . next(list1) . 1 . next(list1) . 2 . for i in lst1: print(i) . 1 4 9 16 25 36 49 . .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/21/Python-Intermediate.html",
            "relUrl": "/2021/04/21/Python-Intermediate.html",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Python-Basics",
            "content": "bool() . False . type(True) . bool . my_str = &quot;siddy&quot; . print(my_str.isalnum()) print(my_str.isupper()) . True False . my_str=&#39;sid123&#39; my_str.isalnum() . True . Lists . type([]) . list . list = [&#39;mathematics&#39;,100,10,20,122,230] print(list) . [&#39;mathematics&#39;, 100, 10, 20, 122, 230] . list[:] # 100 to 230 list[1:] . [100, 10, 20, 122, 230] . list[1:5] # left is the same index and right is index we want+1 . [100, 10, 20, 122] . print(list) . [&#39;mathematics&#39;, 100, 10, 20, 122, 230, &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;], &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;], &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;]] . list.insert(1,&quot;saxena&quot;) print(list) . [&#39;mathematics&#39;, &#39;saxena&#39;, &#39;saxena&#39;, 100, 10, 20, 122, 230, &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;], &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;], &#39;sid&#39;, [&#39;sid1&#39;, &#39;sid2&#39;]] . list2 = [1,2,3,4,5] print(list2) . [1, 2, 3, 4, 5] . list2.extend([6,7]) list2 . [1, 2, 3, 4, 5, 6, 7] . operations on list . sum(list2) . 28 . list2.pop() . 7 . list2.count(2) . 1 . list2.index(2,1,4) . 1 . print(min(list2)) print(max(list2)) . 1 6 . list2*2 . [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6] .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/21/Python-Basics.html",
            "relUrl": "/2021/04/21/Python-Basics.html",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Python-Advanced",
            "content": "correct way to initialise a Class . class Car: def __init__(self,window,door,enginetype): self.windows=window self.doors=door self.enginetype=enginetype def self_driving(self): return &quot;This is a {} car&quot;.format(self.enginetype) . car1=Car(4,5,&#39;petrol&#39;) # by default this init method is called . dir(car1) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;doors&#39;, &#39;enginetype&#39;, &#39;self_driving&#39;, &#39;windows&#39;] . car2=Car(3,4,&quot;diesel&quot;) . print(car1.windows) . 4 . print(car2.enginetype) . diesel . car1.self_driving() . &#39;This is a petrol car&#39; . print(car2.doors) . 4 . car2.enginetype=&quot;diesel&quot; . print(car2.enginetype) . diesel . Python Exception Handling . try: # code block where exception can occur a=b except: print(&quot;some problem may have occured&quot;) . some problem may have occured . try: # code block where exception can occur a=b except Exception as ex: print(ex) . name &#39;b&#39; is not defined . try: # code block where exception can occur a=b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . the user have not defined the error . try: # code block where exception can occur a=1 b=&#39;s&#39; c=a+b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39; . a=1 b=&#39;s&#39; c=a+b . TypeError Traceback (most recent call last) &lt;ipython-input-15-b5351790d4cc&gt; in &lt;module&gt; 1 a=1 2 b=&#39;s&#39; -&gt; 3 c=a+b TypeError: unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39; . try: # code block where exception can occur a=1 b=&#39;s&#39; c=a+b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except TypeError as ex2: print(&quot;the user has given unsupported data types for addition&quot;) print(&quot;try to make the data type similar&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . the user has given unsupported data types for addition try to make the data type similar . try: a=int(input(&quot;enter number 1 = &quot;)) b=int(input(&quot;enter number 2 = &quot;)) c=a/b d=a+b e=a*b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except TypeError as ex2: print(&quot;the user has given unsupported data types for addition&quot;) print(&quot;try to make the data type similar&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . enter number 1 = 12 enter number 2 = 12 . print(c) print(d) print(e) . 1.0 24 144 . try: a=int(input(&quot;enter number 1 = &quot;)) b=int(input(&quot;enter number 2 = &quot;)) c=a/b d=a+b e=a*b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except TypeError as ex2: print(&quot;the user has given unsupported data types for addition&quot;) print(&quot;try to make the data type similar&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) . enter number 1 = 1 enter number 2 = 2 . 12/0 . ZeroDivisionError Traceback (most recent call last) &lt;ipython-input-28-898e9759c56e&gt; in &lt;module&gt; -&gt; 1 12/0 ZeroDivisionError: division by zero . try: a=int(input(&quot;enter number 1 = &quot;)) b=int(input(&quot;enter number 2 = &quot;)) c=a/b d=a+b e=a*b except NameError as ex1: print(&quot;the user have not defined the error&quot;) except TypeError as ex2: print(&quot;the user has given unsupported data types for addition&quot;) print(&quot;try to make the data type similar&quot;) except ZeroDivisionError as ex3: print(&quot;12/0 is not defined&quot;) except Exception as ex: # this always needs to be written on the lst print(ex) else: print(c) print(d) print(e) finally: print(&quot;The execution is complete&quot;) . enter number 1 = 12 enter number 2 = 0 12/0 is not defined The execution is complete . Custom Exception . class Error(Exception): # inheriting the exception class pass class dobException(Error): pass . year = int(input(&quot;Enter the year of birth&quot;)) age = 2021-year try: if age&lt;=30 &amp; age&gt;20: print(&quot;Valid age&quot;) else: raise dobException except dobException: print(&quot;year range is not valid&quot;) . Enter the year of birth1555 year range is not valid . Public Private and Protected Access modifiers . class Car(): def __init__(self,windows, doors, enginetypes): self.windows=windows self.doors=doors self.enginetypes=enginetypes . audi = Car(4,5,&quot;Diesel&quot;) . audi . &lt;__main__.Car at 0x1ed3a2b5ac8&gt; . audi.windows . 4 . audi.windows=5 . audi.windows . 5 . dir(audi) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;doors&#39;, &#39;enginetypes&#39;, &#39;windows&#39;] . class Car(): def __init__(self,windows, doors, enginetypes): self._windows=windows self._doors=doors self._enginetypes=enginetypes . class Truck(Car): def __init__(self,windows, doors, enginetypes, hp): super().__init__(windows,doors,enginetypes) self.hp=hp . truck=Truck(4,2,&quot;Petrol&quot;,720) . dir(truck) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_doors&#39;, &#39;_enginetypes&#39;, &#39;_windows&#39;, &#39;hp&#39;] . truck._doors=4 . truck._doors . 4 . audi._windows . AttributeError Traceback (most recent call last) &lt;ipython-input-45-63cafd091289&gt; in &lt;module&gt; -&gt; 1 audi._windows AttributeError: &#39;Car&#39; object has no attribute &#39;_windows&#39; . dir(audi) . [&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;doors&#39;, &#39;enginetypes&#39;, &#39;windows&#39;] . class Car(): def __init__(self,windows, doors, enginetypes): self.__windows=windows self.__doors=doors self.__enginetypes=enginetypes . audi=Car(4,2,&quot;petrol&quot;) . dir(audi) . [&#39;_Car__doors&#39;, &#39;_Car__enginetypes&#39;, &#39;_Car__windows&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;] . Inheritance . ## Car Blueprint class Car(): def __init__(self, windows, doors, enginetype): self.windows=windows self.doors=doors self.enginetype=enginetype def drive(self): print(&quot;the person drive a car&quot;) . car = Car(4,5,&quot;diesel&quot;) . car.windows . 4 . car.drive() . the person drive a car . class audi(Car): def __init__(self,windows,doors,enginetype,enableai): super().__init__(windows,doors,enginetype) self.enableai=enableai def selfdriving(self): print(&quot;Audi Supports Self driving&quot;) . audiQ7=audi(5,5,&#39;diesel&#39;,True) . audiQ7.enableai . True .",
            "url": "https://mr-siddy.github.io/ML-blog/2021/04/21/Python-Advanced.html",
            "relUrl": "/2021/04/21/Python-Advanced.html",
            "date": " • Apr 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey 👋🏽, I’m Siddhant! . . . Hi, I’m Siddhant saxena, a Machine Learning novice and am very passionate about Artificial Intelligence 🚀 . I’m from India and am currently looking to collaborate on ML/DL projects and would love to work with like-minded individuals. . Talking about Me: . 💻 I’m currently working on ML Projects and am looking for collaborators; | 🌱 I’m currently learning Computer Vision; | 💬 Ask me about anything, I’d try my best to help; | 📫 How to reach me: mrsiddy.py@gmail.com; | . . . ⭐️ From Siddhant .",
          "url": "https://mr-siddy.github.io/ML-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mr-siddy.github.io/ML-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}